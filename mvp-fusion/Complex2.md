---
conversion:
  engine: mvp-fusion-highspeed
  page_count: 13
  conversion_time_ms: 65.05758711136878
  source_file: Complex2.pdf
  format: PDF
content_analysis:
  has_pdf_content: true
  has_tables: false
  has_images: true
  has_formulas: true
  has_code: false
  has_links: true
  has_lists: true
  has_headers: true
  has_footnotes: true
  has_citations: true
  has_structured_data: false
processing: {stage: completed, content_length: 54821}
domain_classification:
  routing: {skip_entity_extraction: false, enable_deep_domain_extraction: false, domain_specialization_route: general}
  top_domains: [emerging_tech, data_science_ai, private_equity, software_development, database]
  top_document_types: [sec_filing, change_order, return_authorization, technical_research, exam]
  domains:
    emerging_tech: 14.0
    data_science_ai: 9.3
    private_equity: 9.0
    software_development: 6.4
    database: 4.2
    data_analytics: 4.1
    business_intelligence: 3.1
    frameworks: 3.0
    research: 2.3
    customer_success: 2.1
    due_diligence: 2.0
    policy_legislation: 1.8
    product_quality: 1.7
    conversion_optimization: 1.7
    architecture: 1.4
    learning_instruction: 1.3
    federal_agencies: 1.2
    payments: 1.2
    engagement: 1.2
    hr_tech: 1.2
    grants_funding: 1.2
    feature_comparison: 1.1
    student_management: 1.0
    market_dynamics: 1.0
    ai_innovation: 1.0
    investment_terms: 0.9
    contracts: 0.9
    legal_compliance: 0.9
    enforcement: 0.9
    process_management: 0.8
    mobile_development: 0.7
    customer_experience: 0.7
    lab_tech: 0.6
    innovation: 0.6
    strategic_rationale: 0.6
    product_development: 0.6
    agencies: 0.6
  document_types:
    sec_filing: 15.1
    change_order: 12.5
    return_authorization: 6.6
    technical_research: 4.4
    exam: 3.6
    tax_opinion: 3.2
    product_review: 3.1
    portfolio_statement: 2.3
    instructional: 2.2
    risk_assessment: 2.1
    term_sheet: 2.0
    influencer_content: 1.8
    forum_discussion: 1.8
    nda: 1.6
    survey_research: 1.5
    method_validation: 1.4
    standardized_test: 1.3
    standard_operating_procedure: 1.3
    trademark_application: 1.2
    stress_test: 1.1
    test_report: 1.1
    online_course: 1.1
    iso_document: 1.0
    lease_agreement: 1.0
    system_documentation: 1.0
    sales_receipt: 1.0
    design_specification: 0.9
    research_paper: 0.8
    tax_return: 0.8
    calibration_record: 0.8
    corrective_action: 0.8
    equipment_manual: 0.8
    training_material: 0.8
    attendance_record: 0.8
    twitter_thread: 0.6
    production_schedule: 0.6
raw_entities:
  person:
  - value: Bin Wang
    text: Bin Wang
    type: PERSON
    span: {start: 94, end: 102}
  - value: Guang Liang
    text: Guang Liang
    type: PERSON
    span: {start: 124, end: 135}
  - value: Avg Len
    text: Avg Len
    type: PERSON
    span: {start: 15422, end: 15429}
  - value: Max Len
    text: Max Len
    type: PERSON
    span: {start: 15664, end: 15671}
  - value: Avg Len
    text: Avg Len
    type: PERSON
    span: {start: 15678, end: 15685}
  - value: Shift Win
    text: Shift Win
    type: PERSON
    span: {start: 22285, end: 22294}
  - value: Asana Math
    text: Asana Math
    type: PERSON
    span: {start: 45003, end: 45013}
  - value: Cambria Math
    text: Cambria Math
    type: PERSON
    span: {start: 45015, end: 45027}
  org:
  - value: the U
    text: the U
    type: ORG
    span: {start: 241, end: 246}
  - value: The U
    text: The U
    type: ORG
    span: {start: 374, end: 379}
  - value: the U
    text: the U
    type: ORG
    span: {start: 807, end: 812}
  - value: the U
    text: the U
    type: ORG
    span: {start: 1179, end: 1184}
  - value: standing o
    text: standing o
    type: ORG
    span: {start: 2069, end: 2079}
  - value: Performance
    text: Performance
    type: ORG
    span: {start: 2542, end: 2553}
  - value: the U
    text: the U
    type: ORG
    span: {start: 4987, end: 4992}
  - value: the U
    text: the U
    type: ORG
    span: {start: 5512, end: 5517}
  - value: open
    text: open
    type: ORG
    span: {start: 7175, end: 7179}
  - value: Open
    text: Open
    type: ORG
    span: {start: 7191, end: 7195}
  - value: the u
    text: the u
    type: ORG
    span: {start: 9930, end: 9935}
  - value: the U
    text: the U
    type: ORG
    span: {start: 10270, end: 10275}
  - value: The U
    text: The U
    type: ORG
    span: {start: 10755, end: 10760}
  - value: on the other hand
    text: on the other hand
    type: ORG
    span: {start: 11248, end: 11265}
  - value: the base
    text: the base
    type: ORG
    span: {start: 12699, end: 12707}
  - value: StackExchange
    text: StackExchange
    type: ORG
    span: {start: 12880, end: 12893}
  - value: the pub
    text: the pub
    type: ORG
    span: {start: 14393, end: 14400}
  - value: the max
    text: the max
    type: ORG
    span: {start: 15692, end: 15699}
  - value: The U
    text: The U
    type: ORG
    span: {start: 15815, end: 15820}
  - value: The U
    text: The U
    type: ORG
    span: {start: 15982, end: 15987}
  - value: the U
    text: the U
    type: ORG
    span: {start: 17653, end: 17658}
  - value: The U
    text: The U
    type: ORG
    span: {start: 18429, end: 18434}
  - value: the U
    text: the U
    type: ORG
    span: {start: 18526, end: 18531}
  - value: Length
    text: Length
    type: ORG
    span: {start: 18989, end: 18995}
  - value: the U
    text: the U
    type: ORG
    span: {start: 19434, end: 19439}
  - value: l spa
    text: l spa
    type: ORG
    span: {start: 23053, end: 23058}
  - value: the U
    text: the U
    type: ORG
    span: {start: 23515, end: 23520}
  - value: Table 3
    text: Table 3
    type: ORG
    span: {start: 24799, end: 24806}
  - value: batch
    text: batch
    type: ORG
    span: {start: 25049, end: 25054}
  - value: NVIDIA
    text: NVIDIA
    type: ORG
    span: {start: 25705, end: 25711}
  - value: emory
    text: emory
    type: ORG
    span: {start: 25731, end: 25736}
  - value: The arch
    text: The arch
    type: ORG
    span: {start: 26082, end: 26090}
  - value: the U
    text: the U
    type: ORG
    span: {start: 26195, end: 26200}
  - value: Table 3
    text: Table 3
    type: ORG
    span: {start: 27573, end: 27580}
  - value: the base
    text: the base
    type: ORG
    span: {start: 30025, end: 30033}
  - value: the base
    text: the base
    type: ORG
    span: {start: 30698, end: 30706}
  - value: the base
    text: the base
    type: ORG
    span: {start: 31568, end: 31576}
  - value: the U
    text: the U
    type: ORG
    span: {start: 31671, end: 31676}
  - value: the U
    text: the U
    type: ORG
    span: {start: 32238, end: 32243}
  - value: superior detail
    text: superior detail
    type: ORG
    span: {start: 33031, end: 33046}
  - value: Association for Computing Machinery
    text: Association for Computing Machinery
    type: ORG
    span: {start: 33691, end: 33726}
  - value: applied mathematics
    text: applied mathematics
    type: ORG
    span: {start: 33651, end: 33670}
  - value: InternLM
    text: InternLM
    type: ORG
    span: {start: 35159, end: 35167}
  - value: Level
    text: Level
    type: ORG
    span: {start: 37367, end: 37372}
  - value: Suzuki
    text: Suzuki
    type: ORG
    span: {start: 39132, end: 39138}
  - value: Internlm
    text: Internlm
    type: ORG
    span: {start: 41174, end: 41182}
  - value: StackExchange
    text: StackExchange
    type: ORG
    span: {start: 42590, end: 42603}
  - value: unico
    text: unico
    type: ORG
    span: {start: 44478, end: 44483}
  - value: rent source
    text: rent source
    type: ORG
    span: {start: 45625, end: 45636}
  - value: The bar
    text: The bar
    type: ORG
    span: {start: 47378, end: 47385}
  - value: the U
    text: the U
    type: ORG
    span: {start: 48030, end: 48035}
  - value: the den
    text: the den
    type: ORG
    span: {start: 48397, end: 48404}
  - value: On the other hand
    text: On the other hand
    type: ORG
    span: {start: 48685, end: 48702}
  gpe:
  - value: Shanghai
    text: Shanghai
    type: GPE
    span: {start: 189, end: 197}
    metadata: {subcategory: major_cities}
  - value: European
    text: European
    type: GPE
    span: {start: 36100, end: 36108}
    metadata: {subcategory: regional_and_geopolitical_blocs}
  - value: izhevsk
    text: izhevsk
    type: GPE
    span: {start: 7988, end: 7995}
    metadata: {subcategory: major_cities}
  - value: Chinese
    text: Chinese
    type: GPE
    span: {start: 13778, end: 13785}
    metadata: {subcategory: demonyms_individuals}
  - value: English
    text: English
    type: GPE
    span: {start: 13790, end: 13797}
    metadata: {subcategory: demonyms_individuals}
  - value: Europe
    text: Europe
    type: GPE
    span: {start: 36874, end: 36880}
    metadata: {subcategory: regional_and_geopolitical_blocs}
  - value: essen
    text: essen
    type: GPE
    span: {start: 1843, end: 1848}
    metadata: {subcategory: major_cities}
  - value: paris
    text: paris
    type: GPE
    span: {start: 2557, end: 2562}
    metadata: {subcategory: major_cities}
  - value: Other
    text: Other
    type: GPE
    span: {start: 3102, end: 3107}
    metadata: {subcategory: us_government_agencies}
  - value: turin
    text: turin
    type: GPE
    span: {start: 7509, end: 7514}
    metadata: {subcategory: major_cities}
  - value: other
    text: other
    type: GPE
    span: {start: 11255, end: 11260}
    metadata: {subcategory: us_government_agencies}
  - value: Greek
    text: Greek
    type: GPE
    span: {start: 47510, end: 47515}
    metadata: {subcategory: language_linked_identities}
  - value: ROME
    text: ROME
    type: GPE
    span: {start: 15193, end: 15197}
    metadata: {subcategory: major_cities}
  - value: mali
    text: mali
    type: GPE
    span: {start: 20286, end: 20290}
    metadata: {subcategory: countries}
  - value: cali
    text: cali
    type: GPE
    span: {start: 39778, end: 39782}
    metadata: {subcategory: major_cities}
  - value: loc
    text: loc
    type: GPE
    span: {start: 1059, end: 1062}
    metadata: {subcategory: us_government_agencies}
  - value: doc
    text: doc
    type: GPE
    span: {start: 1697, end: 1700}
    metadata: {subcategory: us_government_agencies}
  - value: cia
    text: cia
    type: GPE
    span: {start: 2156, end: 2159}
    metadata: {subcategory: us_government_agencies}
  - value: arc
    text: arc
    type: GPE
    span: {start: 2183, end: 2186}
    metadata: {subcategory: us_government_agencies}
  - value: usa
    text: usa
    type: GPE
    span: {start: 3972, end: 3975}
    metadata: {subcategory: government_forms}
  - value: nsf
    text: nsf
    type: GPE
    span: {start: 5682, end: 5685}
    metadata: {subcategory: us_government_agencies}
  - value: opm
    text: opm
    type: GPE
    span: {start: 6453, end: 6456}
    metadata: {subcategory: us_government_agencies}
  - value: Doc
    text: Doc
    type: GPE
    span: {start: 9464, end: 9467}
    metadata: {subcategory: us_government_agencies}
  - value: eac
    text: eac
    type: GPE
    span: {start: 18187, end: 18190}
    metadata: {subcategory: us_government_agencies}
  - value: fec
    text: fec
    type: GPE
    span: {start: 18345, end: 18348}
    metadata: {subcategory: us_government_agencies}
  - value: epa
    text: epa
    type: GPE
    span: {start: 19865, end: 19868}
    metadata: {subcategory: us_government_agencies}
  - value: nea
    text: nea
    type: GPE
    span: {start: 25848, end: 25851}
    metadata: {subcategory: us_government_agencies}
  - value: Arc
    text: Arc
    type: GPE
    span: {start: 26912, end: 26915}
    metadata: {subcategory: us_government_agencies}
  - value: sec
    text: sec
    type: GPE
    span: {start: 29745, end: 29748}
    metadata: {subcategory: us_government_agencies}
  - value: ssa
    text: ssa
    type: GPE
    span: {start: 29805, end: 29808}
    metadata: {subcategory: us_government_agencies}
  - value: Gao
    text: Gao
    type: GPE
    span: {start: 34490, end: 34493}
    metadata: {subcategory: us_government_agencies}
  - value: nib
    text: nib
    type: GPE
    span: {start: 37661, end: 37664}
    metadata: {subcategory: us_government_agencies}
  - value: ibb
    text: ibb
    type: GPE
    span: {start: 38138, end: 38141}
    metadata: {subcategory: major_cities}
  - value: Sec
    text: Sec
    type: GPE
    span: {start: 42051, end: 42054}
    metadata: {subcategory: us_government_agencies}
  - value: Eac
    text: Eac
    type: GPE
    span: {start: 48047, end: 48050}
    metadata: {subcategory: us_government_agencies}
  - value: doe
    text: doe
    type: GPE
    span: {start: 49118, end: 49121}
    metadata: {subcategory: us_government_agencies}
  - value: Ero
    text: Ero
    type: GPE
    span: {start: 49501, end: 49504}
    metadata: {subcategory: us_government_agencies}
  location:
  - value: Shanghai
    text: Shanghai
    type: LOC
    span: {start: 189, end: 197}
    metadata: {subcategory: urban_settlements}
  - value: izhevsk
    text: izhevsk
    type: LOC
    span: {start: 7988, end: 7995}
    metadata: {subcategory: urban_settlements}
  - value: Europe
    text: Europe
    type: LOC
    span: {start: 36100, end: 36106}
    metadata: {subcategory: continents}
  - value: essen
    text: essen
    type: LOC
    span: {start: 1843, end: 1848}
    metadata: {subcategory: urban_settlements}
  - value: paris
    text: paris
    type: LOC
    span: {start: 2557, end: 2562}
    metadata: {subcategory: urban_settlements}
  - value: turin
    text: turin
    type: LOC
    span: {start: 7509, end: 7514}
    metadata: {subcategory: urban_settlements}
  - value: ROME
    text: ROME
    type: LOC
    span: {start: 15193, end: 15197}
    metadata: {subcategory: urban_settlements}
  - value: cali
    text: cali
    type: LOC
    span: {start: 39778, end: 39782}
    metadata: {subcategory: urban_settlements}
  - value: ibb
    text: ibb
    type: LOC
    span: {start: 37662, end: 37665}
    metadata: {subcategory: urban_settlements}
  date:
  - value: 2024-2-29
    text: 2024-2-29
    type: DATE
    span: {start: 0, end: 9}
  time: []
  money: []
  phone: []
  email:
  - value: heconghui@pjlab.org.cn
    text: heconghui@pjlab.org.cn
    type: EMAIL
    span: {start: 0, end: 22}
  url:
  - value: https://opendatalab.com/OpenDataLab/UniMER-Dataset
    text: https://opendatalab.com/OpenDataLab/UniMER-Dataset
    type: URL
    span: {start: 0, end: 50}
  - value: https://github.com/
    text: https://github.com/
    type: URL
    span: {start: 0, end: 19}
  regulation: []
  measurement:
  - value: 1M
    text: 1M
    type: MEASUREMENT
    span: {start: 0, end: 2}
  - value: 791 l
    text: 791 l
    type: MEASUREMENT
    span: {start: 0, end: 5}
  - value: 80G
    text: 80G
    type: MEASUREMENT
    span: {start: 0, end: 3}
  - value: 100M
    text: 100M
    type: MEASUREMENT
    span: {start: 0, end: 4}
  - value: 202M
    text: 202M
    type: MEASUREMENT
    span: {start: 0, end: 4}
  - value: 325M
    text: 325M
    type: MEASUREMENT
    span: {start: 0, end: 4}
  - value: '7


      M'
    text: '7


      M'
    type: MEASUREMENT
    span: {start: 0, end: 4}
  - value: 16 m
    text: 16 m
    type: MEASUREMENT
    span: {start: 0, end: 4}
  - value: '5

      G'
    text: '5

      G'
    type: MEASUREMENT
    span: {start: 0, end: 3}
  - value: '2

      L'
    text: '2

      L'
    type: MEASUREMENT
    span: {start: 0, end: 3}
  - value: '5

      L'
    text: '5

      L'
    type: MEASUREMENT
    span: {start: 0, end: 3}
  - value: '2

      M'
    text: '2

      M'
    type: MEASUREMENT
    span: {start: 0, end: 3}
  - value: '3

      M'
    text: '3

      M'
    type: MEASUREMENT
    span: {start: 0, end: 3}
  - value: '9


      M'
    text: '9


      M'
    type: MEASUREMENT
    span: {start: 0, end: 4}
  - value: 4 m
    text: 4 m
    type: MEASUREMENT
    span: {start: 0, end: 3}
  - value: 2 l
    text: 2 l
    type: MEASUREMENT
    span: {start: 0, end: 3}
entity_insights:
  has_financial_data: false
  has_contact_info: true
  has_temporal_data: true
  has_external_references: true
  total_entities_found: 127
normalization:
  normalization_method: mvp-fusion-canonicalization
  processing_time_ms: 2.2170559968799353
  statistics:
    processing_time_ms: 2.2170559968799353
    original_entity_count: 127
    normalized_entity_count: 123
    entity_reduction_percent: 3.149606299212598
    entity_types_processed: 123
    canonical_forms_created: 123
    total_mentions: 124
    performance_metrics: {entities_per_ms: 55.47897760503027, memory_efficient: true, edge_compatible: true}
  canonical_entities:
  - id: p001
    type: PERSON
    normalized: Bin Wang
    aliases: []
    count: 1
    mentions:
    - text: Bin Wang
      span: {start: 94, end: 102}
    metadata: null
  - id: p002
    type: PERSON
    normalized: Guang Liang
    aliases: []
    count: 1
    mentions:
    - text: Guang Liang
      span: {start: 124, end: 135}
    metadata: null
  - id: p003
    type: PERSON
    normalized: Avg Len
    aliases: []
    count: 2
    mentions:
    - text: Avg Len
      span: {start: 15422, end: 15429}
    - text: Avg Len
      span: {start: 15678, end: 15685}
    metadata: null
  - id: p004
    type: PERSON
    normalized: Max Len
    aliases: []
    count: 1
    mentions:
    - text: Max Len
      span: {start: 15664, end: 15671}
    metadata: null
  - id: p005
    type: PERSON
    normalized: Shift Win
    aliases: []
    count: 1
    mentions:
    - text: Shift Win
      span: {start: 22285, end: 22294}
    metadata: null
  - id: p006
    type: PERSON
    normalized: Asana Math
    aliases: []
    count: 1
    mentions:
    - text: Asana Math
      span: {start: 45003, end: 45013}
    metadata: null
  - id: p007
    type: PERSON
    normalized: Cambria Math
    aliases: []
    count: 1
    mentions:
    - text: Cambria Math
      span: {start: 45015, end: 45027}
    metadata: null
  - id: org006
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 241, end: 246}
    metadata:
      id: org006
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 241, end: 246}
      count: 1
  - id: org007
    type: ORG
    normalized: The U
    aliases: []
    count: 1
    mentions:
    - text: The U
      span: {start: 374, end: 379}
    metadata:
      id: org007
      canonical: The U
      aliases: []
      mentions:
      - text: The U
        span: {start: 374, end: 379}
      count: 1
  - id: org008
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 807, end: 812}
    metadata:
      id: org008
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 807, end: 812}
      count: 1
  - id: org009
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 1179, end: 1184}
    metadata:
      id: org009
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 1179, end: 1184}
      count: 1
  - id: org010
    type: ORG
    normalized: standing o
    aliases: []
    count: 1
    mentions:
    - text: standing o
      span: {start: 2069, end: 2079}
    metadata:
      id: org010
      canonical: standing o
      aliases: []
      mentions:
      - text: standing o
        span: {start: 2069, end: 2079}
      count: 1
  - id: org011
    type: ORG
    normalized: Performance
    aliases: []
    count: 1
    mentions:
    - text: Performance
      span: {start: 2542, end: 2553}
    metadata:
      id: org011
      canonical: Performance
      aliases: []
      mentions:
      - text: Performance
        span: {start: 2542, end: 2553}
      count: 1
  - id: org012
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 4987, end: 4992}
    metadata:
      id: org012
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 4987, end: 4992}
      count: 1
  - id: org013
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 5512, end: 5517}
    metadata:
      id: org013
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 5512, end: 5517}
      count: 1
  - id: org014
    type: ORG
    normalized: open
    aliases: []
    count: 1
    mentions:
    - text: open
      span: {start: 7175, end: 7179}
    metadata:
      id: org014
      canonical: open
      aliases: []
      mentions:
      - text: open
        span: {start: 7175, end: 7179}
      count: 1
  - id: org015
    type: ORG
    normalized: Open
    aliases: []
    count: 1
    mentions:
    - text: Open
      span: {start: 7191, end: 7195}
    metadata:
      id: org015
      canonical: Open
      aliases: []
      mentions:
      - text: Open
        span: {start: 7191, end: 7195}
      count: 1
  - id: org016
    type: ORG
    normalized: the u
    aliases: []
    count: 1
    mentions:
    - text: the u
      span: {start: 9930, end: 9935}
    metadata:
      id: org016
      canonical: the u
      aliases: []
      mentions:
      - text: the u
        span: {start: 9930, end: 9935}
      count: 1
  - id: org017
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 10270, end: 10275}
    metadata:
      id: org017
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 10270, end: 10275}
      count: 1
  - id: org018
    type: ORG
    normalized: The U
    aliases: []
    count: 1
    mentions:
    - text: The U
      span: {start: 10755, end: 10760}
    metadata:
      id: org018
      canonical: The U
      aliases: []
      mentions:
      - text: The U
        span: {start: 10755, end: 10760}
      count: 1
  - id: org019
    type: ORG
    normalized: on the other hand
    aliases: []
    count: 1
    mentions:
    - text: on the other hand
      span: {start: 11248, end: 11265}
    metadata:
      id: org019
      canonical: on the other hand
      aliases: []
      mentions:
      - text: on the other hand
        span: {start: 11248, end: 11265}
      count: 1
  - id: org020
    type: ORG
    normalized: the base
    aliases: []
    count: 1
    mentions:
    - text: the base
      span: {start: 12699, end: 12707}
    metadata:
      id: org020
      canonical: the base
      aliases: []
      mentions:
      - text: the base
        span: {start: 12699, end: 12707}
      count: 1
  - id: org021
    type: ORG
    normalized: StackExchange
    aliases: []
    count: 1
    mentions:
    - text: StackExchange
      span: {start: 12880, end: 12893}
    metadata:
      id: org021
      canonical: StackExchange
      aliases: []
      mentions:
      - text: StackExchange
        span: {start: 12880, end: 12893}
      count: 1
  - id: org022
    type: ORG
    normalized: the pub
    aliases: []
    count: 1
    mentions:
    - text: the pub
      span: {start: 14393, end: 14400}
    metadata:
      id: org022
      canonical: the pub
      aliases: []
      mentions:
      - text: the pub
        span: {start: 14393, end: 14400}
      count: 1
  - id: org023
    type: ORG
    normalized: the max
    aliases: []
    count: 1
    mentions:
    - text: the max
      span: {start: 15692, end: 15699}
    metadata:
      id: org023
      canonical: the max
      aliases: []
      mentions:
      - text: the max
        span: {start: 15692, end: 15699}
      count: 1
  - id: org024
    type: ORG
    normalized: The U
    aliases: []
    count: 1
    mentions:
    - text: The U
      span: {start: 15815, end: 15820}
    metadata:
      id: org024
      canonical: The U
      aliases: []
      mentions:
      - text: The U
        span: {start: 15815, end: 15820}
      count: 1
  - id: org025
    type: ORG
    normalized: The U
    aliases: []
    count: 1
    mentions:
    - text: The U
      span: {start: 15982, end: 15987}
    metadata:
      id: org025
      canonical: The U
      aliases: []
      mentions:
      - text: The U
        span: {start: 15982, end: 15987}
      count: 1
  - id: org026
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 17653, end: 17658}
    metadata:
      id: org026
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 17653, end: 17658}
      count: 1
  - id: org027
    type: ORG
    normalized: The U
    aliases: []
    count: 1
    mentions:
    - text: The U
      span: {start: 18429, end: 18434}
    metadata:
      id: org027
      canonical: The U
      aliases: []
      mentions:
      - text: The U
        span: {start: 18429, end: 18434}
      count: 1
  - id: org028
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 18526, end: 18531}
    metadata:
      id: org028
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 18526, end: 18531}
      count: 1
  - id: org029
    type: ORG
    normalized: Length
    aliases: []
    count: 1
    mentions:
    - text: Length
      span: {start: 18989, end: 18995}
    metadata:
      id: org029
      canonical: Length
      aliases: []
      mentions:
      - text: Length
        span: {start: 18989, end: 18995}
      count: 1
  - id: org030
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 19434, end: 19439}
    metadata:
      id: org030
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 19434, end: 19439}
      count: 1
  - id: org031
    type: ORG
    normalized: l spa
    aliases: []
    count: 1
    mentions:
    - text: l spa
      span: {start: 23053, end: 23058}
    metadata:
      id: org031
      canonical: l spa
      aliases: []
      mentions:
      - text: l spa
        span: {start: 23053, end: 23058}
      count: 1
  - id: org032
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 23515, end: 23520}
    metadata:
      id: org032
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 23515, end: 23520}
      count: 1
  - id: org033
    type: ORG
    normalized: Table 3
    aliases: []
    count: 1
    mentions:
    - text: Table 3
      span: {start: 24799, end: 24806}
    metadata:
      id: org033
      canonical: Table 3
      aliases: []
      mentions:
      - text: Table 3
        span: {start: 24799, end: 24806}
      count: 1
  - id: org034
    type: ORG
    normalized: batch
    aliases: []
    count: 1
    mentions:
    - text: batch
      span: {start: 25049, end: 25054}
    metadata:
      id: org034
      canonical: batch
      aliases: []
      mentions:
      - text: batch
        span: {start: 25049, end: 25054}
      count: 1
  - id: org035
    type: ORG
    normalized: NVIDIA
    aliases: []
    count: 1
    mentions:
    - text: NVIDIA
      span: {start: 25705, end: 25711}
    metadata:
      id: org035
      canonical: NVIDIA
      aliases: []
      mentions:
      - text: NVIDIA
        span: {start: 25705, end: 25711}
      count: 1
  - id: org036
    type: ORG
    normalized: emory
    aliases: []
    count: 1
    mentions:
    - text: emory
      span: {start: 25731, end: 25736}
    metadata:
      id: org036
      canonical: emory
      aliases: []
      mentions:
      - text: emory
        span: {start: 25731, end: 25736}
      count: 1
  - id: org037
    type: ORG
    normalized: The arch
    aliases: []
    count: 1
    mentions:
    - text: The arch
      span: {start: 26082, end: 26090}
    metadata:
      id: org037
      canonical: The arch
      aliases: []
      mentions:
      - text: The arch
        span: {start: 26082, end: 26090}
      count: 1
  - id: org038
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 26195, end: 26200}
    metadata:
      id: org038
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 26195, end: 26200}
      count: 1
  - id: org039
    type: ORG
    normalized: Table 3
    aliases: []
    count: 1
    mentions:
    - text: Table 3
      span: {start: 27573, end: 27580}
    metadata:
      id: org039
      canonical: Table 3
      aliases: []
      mentions:
      - text: Table 3
        span: {start: 27573, end: 27580}
      count: 1
  - id: org040
    type: ORG
    normalized: the base
    aliases: []
    count: 1
    mentions:
    - text: the base
      span: {start: 30025, end: 30033}
    metadata:
      id: org040
      canonical: the base
      aliases: []
      mentions:
      - text: the base
        span: {start: 30025, end: 30033}
      count: 1
  - id: org041
    type: ORG
    normalized: the base
    aliases: []
    count: 1
    mentions:
    - text: the base
      span: {start: 30698, end: 30706}
    metadata:
      id: org041
      canonical: the base
      aliases: []
      mentions:
      - text: the base
        span: {start: 30698, end: 30706}
      count: 1
  - id: org042
    type: ORG
    normalized: the base
    aliases: []
    count: 1
    mentions:
    - text: the base
      span: {start: 31568, end: 31576}
    metadata:
      id: org042
      canonical: the base
      aliases: []
      mentions:
      - text: the base
        span: {start: 31568, end: 31576}
      count: 1
  - id: org043
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 31671, end: 31676}
    metadata:
      id: org043
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 31671, end: 31676}
      count: 1
  - id: org044
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 32238, end: 32243}
    metadata:
      id: org044
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 32238, end: 32243}
      count: 1
  - id: org045
    type: ORG
    normalized: superior detail
    aliases: []
    count: 1
    mentions:
    - text: superior detail
      span: {start: 33031, end: 33046}
    metadata:
      id: org045
      canonical: superior detail
      aliases: []
      mentions:
      - text: superior detail
        span: {start: 33031, end: 33046}
      count: 1
  - id: org046
    type: ORG
    normalized: Association for Computing Machinery
    aliases: []
    count: 1
    mentions:
    - text: Association for Computing Machinery
      span: {start: 33691, end: 33726}
    metadata:
      id: org046
      canonical: Association for Computing Machinery
      aliases: []
      mentions:
      - text: Association for Computing Machinery
        span: {start: 33691, end: 33726}
      count: 1
  - id: org047
    type: ORG
    normalized: applied mathematics
    aliases: []
    count: 1
    mentions:
    - text: applied mathematics
      span: {start: 33651, end: 33670}
    metadata:
      id: org047
      canonical: applied mathematics
      aliases: []
      mentions:
      - text: applied mathematics
        span: {start: 33651, end: 33670}
      count: 1
  - id: org048
    type: ORG
    normalized: InternLM
    aliases: []
    count: 1
    mentions:
    - text: InternLM
      span: {start: 35159, end: 35167}
    metadata:
      id: org048
      canonical: InternLM
      aliases: []
      mentions:
      - text: InternLM
        span: {start: 35159, end: 35167}
      count: 1
  - id: org049
    type: ORG
    normalized: Level
    aliases: []
    count: 1
    mentions:
    - text: Level
      span: {start: 37367, end: 37372}
    metadata:
      id: org049
      canonical: Level
      aliases: []
      mentions:
      - text: Level
        span: {start: 37367, end: 37372}
      count: 1
  - id: org050
    type: ORG
    normalized: Suzuki
    aliases: []
    count: 1
    mentions:
    - text: Suzuki
      span: {start: 39132, end: 39138}
    metadata:
      id: org050
      canonical: Suzuki
      aliases: []
      mentions:
      - text: Suzuki
        span: {start: 39132, end: 39138}
      count: 1
  - id: org051
    type: ORG
    normalized: Internlm
    aliases: []
    count: 1
    mentions:
    - text: Internlm
      span: {start: 41174, end: 41182}
    metadata:
      id: org051
      canonical: Internlm
      aliases: []
      mentions:
      - text: Internlm
        span: {start: 41174, end: 41182}
      count: 1
  - id: org052
    type: ORG
    normalized: StackExchange
    aliases: []
    count: 1
    mentions:
    - text: StackExchange
      span: {start: 42590, end: 42603}
    metadata:
      id: org052
      canonical: StackExchange
      aliases: []
      mentions:
      - text: StackExchange
        span: {start: 42590, end: 42603}
      count: 1
  - id: org053
    type: ORG
    normalized: unico
    aliases: []
    count: 1
    mentions:
    - text: unico
      span: {start: 44478, end: 44483}
    metadata:
      id: org053
      canonical: unico
      aliases: []
      mentions:
      - text: unico
        span: {start: 44478, end: 44483}
      count: 1
  - id: org054
    type: ORG
    normalized: rent source
    aliases: []
    count: 1
    mentions:
    - text: rent source
      span: {start: 45625, end: 45636}
    metadata:
      id: org054
      canonical: rent source
      aliases: []
      mentions:
      - text: rent source
        span: {start: 45625, end: 45636}
      count: 1
  - id: org055
    type: ORG
    normalized: The bar
    aliases: []
    count: 1
    mentions:
    - text: The bar
      span: {start: 47378, end: 47385}
    metadata:
      id: org055
      canonical: The bar
      aliases: []
      mentions:
      - text: The bar
        span: {start: 47378, end: 47385}
      count: 1
  - id: org056
    type: ORG
    normalized: the U
    aliases: []
    count: 1
    mentions:
    - text: the U
      span: {start: 48030, end: 48035}
    metadata:
      id: org056
      canonical: the U
      aliases: []
      mentions:
      - text: the U
        span: {start: 48030, end: 48035}
      count: 1
  - id: org057
    type: ORG
    normalized: the den
    aliases: []
    count: 1
    mentions:
    - text: the den
      span: {start: 48397, end: 48404}
    metadata:
      id: org057
      canonical: the den
      aliases: []
      mentions:
      - text: the den
        span: {start: 48397, end: 48404}
      count: 1
  - id: org058
    type: ORG
    normalized: On the other hand
    aliases: []
    count: 1
    mentions:
    - text: On the other hand
      span: {start: 48685, end: 48702}
    metadata:
      id: org058
      canonical: On the other hand
      aliases: []
      mentions:
      - text: On the other hand
        span: {start: 48685, end: 48702}
      count: 1
  - id: gpe008
    type: GPE
    normalized: Shanghai
    aliases: []
    count: 1
    mentions:
    - text: Shanghai
      span: {start: 189, end: 197}
      subcategory: major_cities
    metadata:
      subcategory: major_cities
      gpe_type: city
      political_level: municipal
      normalization_confidence: 0.8500000000000001
      standardization_applied: false
  - id: gpe009
    type: GPE
    normalized: European
    aliases: []
    count: 1
    mentions:
    - text: European
      span: {start: 36100, end: 36108}
      subcategory: regional_and_geopolitical_blocs
    metadata:
      subcategory: regional_and_geopolitical_blocs
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe010
    type: GPE
    normalized: izhevsk
    aliases: []
    count: 1
    mentions:
    - text: izhevsk
      span: {start: 7988, end: 7995}
      subcategory: major_cities
    metadata:
      subcategory: major_cities
      gpe_type: city
      political_level: municipal
      normalization_confidence: 0.8
      standardization_applied: false
  - id: gpe011
    type: GPE
    normalized: Chinese
    aliases: []
    count: 1
    mentions:
    - text: Chinese
      span: {start: 13778, end: 13785}
      subcategory: demonyms_individuals
    metadata:
      subcategory: demonyms_individuals
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe012
    type: GPE
    normalized: English
    aliases: []
    count: 1
    mentions:
    - text: English
      span: {start: 13790, end: 13797}
      subcategory: demonyms_individuals
    metadata:
      subcategory: demonyms_individuals
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe013
    type: GPE
    normalized: Europe
    aliases: []
    count: 1
    mentions:
    - text: Europe
      span: {start: 36874, end: 36880}
      subcategory: regional_and_geopolitical_blocs
    metadata:
      subcategory: regional_and_geopolitical_blocs
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe014
    type: GPE
    normalized: essen
    aliases: []
    count: 1
    mentions:
    - text: essen
      span: {start: 1843, end: 1848}
      subcategory: major_cities
    metadata:
      subcategory: major_cities
      gpe_type: city
      political_level: municipal
      normalization_confidence: 0.8
      standardization_applied: false
  - id: gpe015
    type: GPE
    normalized: paris
    aliases: []
    count: 1
    mentions:
    - text: paris
      span: {start: 2557, end: 2562}
      subcategory: major_cities
    metadata:
      subcategory: major_cities
      gpe_type: city
      political_level: municipal
      normalization_confidence: 0.8
      standardization_applied: false
  - id: gpe016
    type: GPE
    normalized: Other
    aliases: []
    count: 1
    mentions:
    - text: Other
      span: {start: 3102, end: 3107}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe017
    type: GPE
    normalized: turin
    aliases: []
    count: 1
    mentions:
    - text: turin
      span: {start: 7509, end: 7514}
      subcategory: major_cities
    metadata:
      subcategory: major_cities
      gpe_type: city
      political_level: municipal
      normalization_confidence: 0.8
      standardization_applied: false
  - id: gpe018
    type: GPE
    normalized: other
    aliases: []
    count: 1
    mentions:
    - text: other
      span: {start: 11255, end: 11260}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe019
    type: GPE
    normalized: Greek
    aliases: []
    count: 1
    mentions:
    - text: Greek
      span: {start: 47510, end: 47515}
      subcategory: language_linked_identities
    metadata:
      subcategory: language_linked_identities
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe020
    type: GPE
    normalized: ROME
    aliases: []
    count: 1
    mentions:
    - text: ROME
      span: {start: 15193, end: 15197}
      subcategory: major_cities
    metadata:
      subcategory: major_cities
      gpe_type: city
      political_level: municipal
      normalization_confidence: 0.8500000000000001
      standardization_applied: false
  - id: gpe021
    type: GPE
    normalized: mali
    aliases: []
    count: 1
    mentions:
    - text: mali
      span: {start: 20286, end: 20290}
      subcategory: countries
    metadata:
      subcategory: countries
      gpe_type: country
      political_level: national
      normalization_confidence: 0.9
      standardization_applied: false
  - id: gpe022
    type: GPE
    normalized: cali
    aliases: []
    count: 1
    mentions:
    - text: cali
      span: {start: 39778, end: 39782}
      subcategory: major_cities
    metadata:
      subcategory: major_cities
      gpe_type: city
      political_level: municipal
      normalization_confidence: 0.8
      standardization_applied: false
  - id: gpe023
    type: GPE
    normalized: loc
    aliases: []
    count: 1
    mentions:
    - text: loc
      span: {start: 1059, end: 1062}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe024
    type: GPE
    normalized: doc
    aliases: []
    count: 1
    mentions:
    - text: doc
      span: {start: 1697, end: 1700}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe025
    type: GPE
    normalized: cia
    aliases: []
    count: 1
    mentions:
    - text: cia
      span: {start: 2156, end: 2159}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe026
    type: GPE
    normalized: arc
    aliases: []
    count: 1
    mentions:
    - text: arc
      span: {start: 2183, end: 2186}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe027
    type: GPE
    normalized: usa
    aliases: []
    count: 1
    mentions:
    - text: usa
      span: {start: 3972, end: 3975}
      subcategory: government_forms
    metadata:
      subcategory: government_forms
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe028
    type: GPE
    normalized: nsf
    aliases: []
    count: 1
    mentions:
    - text: nsf
      span: {start: 5682, end: 5685}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe029
    type: GPE
    normalized: opm
    aliases: []
    count: 1
    mentions:
    - text: opm
      span: {start: 6453, end: 6456}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe030
    type: GPE
    normalized: Doc
    aliases: []
    count: 1
    mentions:
    - text: Doc
      span: {start: 9464, end: 9467}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe031
    type: GPE
    normalized: eac
    aliases: []
    count: 1
    mentions:
    - text: eac
      span: {start: 18187, end: 18190}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe032
    type: GPE
    normalized: fec
    aliases: []
    count: 1
    mentions:
    - text: fec
      span: {start: 18345, end: 18348}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe033
    type: GPE
    normalized: epa
    aliases: []
    count: 1
    mentions:
    - text: epa
      span: {start: 19865, end: 19868}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe034
    type: GPE
    normalized: nea
    aliases: []
    count: 1
    mentions:
    - text: nea
      span: {start: 25848, end: 25851}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe035
    type: GPE
    normalized: Arc
    aliases: []
    count: 1
    mentions:
    - text: Arc
      span: {start: 26912, end: 26915}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe036
    type: GPE
    normalized: sec
    aliases: []
    count: 1
    mentions:
    - text: sec
      span: {start: 29745, end: 29748}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe037
    type: GPE
    normalized: ssa
    aliases: []
    count: 1
    mentions:
    - text: ssa
      span: {start: 29805, end: 29808}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe038
    type: GPE
    normalized: Gao
    aliases: []
    count: 1
    mentions:
    - text: Gao
      span: {start: 34490, end: 34493}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe039
    type: GPE
    normalized: nib
    aliases: []
    count: 1
    mentions:
    - text: nib
      span: {start: 37661, end: 37664}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe040
    type: GPE
    normalized: ibb
    aliases: []
    count: 1
    mentions:
    - text: ibb
      span: {start: 38138, end: 38141}
      subcategory: major_cities
    metadata:
      subcategory: major_cities
      gpe_type: city
      political_level: municipal
      normalization_confidence: 0.8
      standardization_applied: false
  - id: gpe041
    type: GPE
    normalized: Sec
    aliases: []
    count: 1
    mentions:
    - text: Sec
      span: {start: 42051, end: 42054}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe042
    type: GPE
    normalized: Eac
    aliases: []
    count: 1
    mentions:
    - text: Eac
      span: {start: 48047, end: 48050}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: gpe043
    type: GPE
    normalized: doe
    aliases: []
    count: 1
    mentions:
    - text: doe
      span: {start: 49118, end: 49121}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.7
      standardization_applied: false
  - id: gpe044
    type: GPE
    normalized: Ero
    aliases: []
    count: 1
    mentions:
    - text: Ero
      span: {start: 49501, end: 49504}
      subcategory: us_government_agencies
    metadata:
      subcategory: us_government_agencies
      gpe_type: geopolitical_entity
      political_level: unknown
      normalization_confidence: 0.75
      standardization_applied: false
  - id: loc003
    type: LOCATION
    normalized: Shanghai
    aliases: []
    count: 1
    mentions:
    - text: Shanghai
      span: {start: 189, end: 197}
      subcategory: urban_settlements
    metadata:
      subcategory: urban_settlements
      location_type: location
      normalization_confidence: 0.7
      standardization_applied: false
      geographic_level: geographic_feature
  - id: loc004
    type: LOCATION
    normalized: izhevsk
    aliases: []
    count: 1
    mentions:
    - text: izhevsk
      span: {start: 7988, end: 7995}
      subcategory: urban_settlements
    metadata:
      subcategory: urban_settlements
      location_type: location
      normalization_confidence: 0.7
      standardization_applied: false
      geographic_level: geographic_feature
  - id: loc005
    type: LOCATION
    normalized: Europe
    aliases: []
    count: 1
    mentions:
    - text: Europe
      span: {start: 36100, end: 36106}
      subcategory: continents
    metadata:
      subcategory: continents
      location_type: location
      normalization_confidence: 0.7
      standardization_applied: false
      geographic_level: geographic_feature
  - id: loc006
    type: LOCATION
    normalized: essen
    aliases: []
    count: 1
    mentions:
    - text: essen
      span: {start: 1843, end: 1848}
      subcategory: urban_settlements
    metadata:
      subcategory: urban_settlements
      location_type: location
      normalization_confidence: 0.7
      standardization_applied: false
      geographic_level: geographic_feature
  - id: loc007
    type: LOCATION
    normalized: paris
    aliases: []
    count: 1
    mentions:
    - text: paris
      span: {start: 2557, end: 2562}
      subcategory: urban_settlements
    metadata:
      subcategory: urban_settlements
      location_type: location
      normalization_confidence: 0.7
      standardization_applied: false
      geographic_level: geographic_feature
  - id: loc008
    type: LOCATION
    normalized: turin
    aliases: []
    count: 1
    mentions:
    - text: turin
      span: {start: 7509, end: 7514}
      subcategory: urban_settlements
    metadata:
      subcategory: urban_settlements
      location_type: location
      normalization_confidence: 0.7
      standardization_applied: false
      geographic_level: geographic_feature
  - id: loc009
    type: LOCATION
    normalized: ROME
    aliases: []
    count: 1
    mentions:
    - text: ROME
      span: {start: 15193, end: 15197}
      subcategory: urban_settlements
    metadata:
      subcategory: urban_settlements
      location_type: location
      normalization_confidence: 0.7
      standardization_applied: false
      geographic_level: geographic_feature
  - id: loc010
    type: LOCATION
    normalized: cali
    aliases: []
    count: 1
    mentions:
    - text: cali
      span: {start: 39778, end: 39782}
      subcategory: urban_settlements
    metadata:
      subcategory: urban_settlements
      location_type: location
      normalization_confidence: 0.7
      standardization_applied: false
      geographic_level: geographic_feature
  - id: loc011
    type: LOCATION
    normalized: ibb
    aliases: []
    count: 1
    mentions:
    - text: ibb
      span: {start: 37662, end: 37665}
      subcategory: urban_settlements
    metadata:
      subcategory: urban_settlements
      location_type: location
      normalization_confidence: 0.7
      standardization_applied: false
      geographic_level: geographic_feature
  - id: d001
    type: DATE
    normalized: '2024-02-29'
    aliases: []
    count: 1
    mentions:
    - text: 2024-2-29
      span: {start: 0, end: 9}
    metadata:
      original_format: 2024-2-29
      iso_date: '2024-02-29'
      epoch_timestamp: 1709164800
      day_of_week: Thursday
      quarter: Q1
      fiscal_year: FY2024
      relative_reference: past
      year: 2024
      month: 2
      day: 29
  - id: meas002
    type: MEASUREMENT
    normalized: '1.0'
    aliases: []
    count: 1
    mentions:
    - text: 1M
      span: {start: 0, end: 2}
    metadata:
      value: 1.0
      unit: M
      si_value: 1.0
      si_unit: meters
      measurement_type: length
      display_value: 1.0 M (1.00 meters)
  - id: meas003
    type: MEASUREMENT
    normalized: '791.0'
    aliases: []
    count: 1
    mentions:
    - text: 791 l
      span: {start: 0, end: 5}
    metadata:
      value: 791.0
      unit: l
      si_value: 791.0
      si_unit: liters
      measurement_type: volume
      display_value: 791.0 l (791.00 liters)
  - id: meas004
    type: MEASUREMENT
    normalized: '0.08'
    aliases: []
    count: 1
    mentions:
    - text: 80G
      span: {start: 0, end: 3}
    metadata:
      value: 80.0
      unit: G
      si_value: 0.08
      si_unit: kilograms
      measurement_type: weight
      display_value: 80.0 G (0.08 kilograms)
  - id: meas005
    type: MEASUREMENT
    normalized: '100.0'
    aliases: []
    count: 1
    mentions:
    - text: 100M
      span: {start: 0, end: 4}
    metadata:
      value: 100.0
      unit: M
      si_value: 100.0
      si_unit: meters
      measurement_type: length
      display_value: 100.0 M (100.00 meters)
  - id: meas006
    type: MEASUREMENT
    normalized: '202.0'
    aliases: []
    count: 1
    mentions:
    - text: 202M
      span: {start: 0, end: 4}
    metadata:
      value: 202.0
      unit: M
      si_value: 202.0
      si_unit: meters
      measurement_type: length
      display_value: 202.0 M (202.00 meters)
  - id: meas007
    type: MEASUREMENT
    normalized: '325.0'
    aliases: []
    count: 1
    mentions:
    - text: 325M
      span: {start: 0, end: 4}
    metadata:
      value: 325.0
      unit: M
      si_value: 325.0
      si_unit: meters
      measurement_type: length
      display_value: 325.0 M (325.00 meters)
  - id: meas008
    type: MEASUREMENT
    normalized: '7.0'
    aliases: []
    count: 1
    mentions:
    - text: '7


        M'
      span: {start: 0, end: 4}
    metadata:
      value: 7.0
      unit: M
      si_value: 7.0
      si_unit: meters
      measurement_type: length
      display_value: 7.0 M (7.00 meters)
  - id: meas009
    type: MEASUREMENT
    normalized: '16.0'
    aliases: []
    count: 1
    mentions:
    - text: 16 m
      span: {start: 0, end: 4}
    metadata:
      value: 16.0
      unit: m
      si_value: 16.0
      si_unit: meters
      measurement_type: length
      display_value: 16.0 m (16.00 meters)
  - id: meas010
    type: MEASUREMENT
    normalized: '0.005'
    aliases: []
    count: 1
    mentions:
    - text: '5

        G'
      span: {start: 0, end: 3}
    metadata:
      value: 5.0
      unit: G
      si_value: 0.005
      si_unit: kilograms
      measurement_type: weight
      display_value: 5.0 G (0.01 kilograms)
  - id: meas011
    type: MEASUREMENT
    normalized: '2.0'
    aliases: []
    count: 1
    mentions:
    - text: '2

        L'
      span: {start: 0, end: 3}
    metadata:
      value: 2.0
      unit: L
      si_value: 2.0
      si_unit: liters
      measurement_type: volume
      display_value: 2.0 L (2.00 liters)
  - id: meas012
    type: MEASUREMENT
    normalized: '5.0'
    aliases: []
    count: 1
    mentions:
    - text: '5

        L'
      span: {start: 0, end: 3}
    metadata:
      value: 5.0
      unit: L
      si_value: 5.0
      si_unit: liters
      measurement_type: volume
      display_value: 5.0 L (5.00 liters)
  - id: meas013
    type: MEASUREMENT
    normalized: '2.0'
    aliases: []
    count: 1
    mentions:
    - text: '2

        M'
      span: {start: 0, end: 3}
    metadata:
      value: 2.0
      unit: M
      si_value: 2.0
      si_unit: meters
      measurement_type: length
      display_value: 2.0 M (2.00 meters)
  - id: meas014
    type: MEASUREMENT
    normalized: '3.0'
    aliases: []
    count: 1
    mentions:
    - text: '3

        M'
      span: {start: 0, end: 3}
    metadata:
      value: 3.0
      unit: M
      si_value: 3.0
      si_unit: meters
      measurement_type: length
      display_value: 3.0 M (3.00 meters)
  - id: meas015
    type: MEASUREMENT
    normalized: '9.0'
    aliases: []
    count: 1
    mentions:
    - text: '9


        M'
      span: {start: 0, end: 4}
    metadata:
      value: 9.0
      unit: M
      si_value: 9.0
      si_unit: meters
      measurement_type: length
      display_value: 9.0 M (9.00 meters)
  - id: meas016
    type: MEASUREMENT
    normalized: '4.0'
    aliases: []
    count: 1
    mentions:
    - text: 4 m
      span: {start: 0, end: 3}
    metadata:
      value: 4.0
      unit: m
      si_value: 4.0
      si_unit: meters
      measurement_type: length
      display_value: 4.0 m (4.00 meters)
  - id: meas017
    type: MEASUREMENT
    normalized: '2.0'
    aliases: []
    count: 1
    mentions:
    - text: 2 l
      span: {start: 0, end: 3}
    metadata:
      value: 2.0
      unit: l
      si_value: 2.0
      si_unit: liters
      measurement_type: volume
      display_value: 2.0 l (2.00 liters)
  entity_reduction_percent: 3.149606299212598
---



# Page 1

UniMERNet: A Universal Network for
Real-World Mathematical Expression Recognition
||Bin Wang||p001||∗1, Zhuangcheng Gu∗1, ||Guang Liang||p002||∗1,
Chao Xu 1, Bo Zhang 1, Botian Shi 1, Conghui He†
1Shanghai AI Laboratory,
Abstract
The paper introduces the UniMER dataset, marking the first
study on Mathematical Expression Recognition (MER) tar-
geting complex real-world scenarios. The UniMER dataset
includes a large-scale training set, UniMER-||1.0||meas002||, which of-
fers unprecedented scale and diversity with one million train-
ing instances to train high-quality, robust models. Addition-
ally, UniMER features a meticulously designed, diverse test
set, UniMER-Test, which covers a variety of formula distri-
butions found in real-world scenarios, providing a more com-
prehensive and fair evaluation. To better utilize the UniMER
dataset, the paper proposes a Universal Mathematical Ex-
pression Recognition Network (UniMERNet), tailored to the
characteristics of formula recognition. UniMERNet consists
of a carefully designed encoder that incorporates detail-aware
and local context features, and an optimized decoder for ac-
celerated performance. Extensive experiments conducted us-
ing the UniMER-||1.0||meas002|| dataset and UniMERNet demonstrate
that training on the large-scale UniMER-||1.0||meas002|| dataset can pro-
duce a more generalizable formula recognition model, sig-
nificantly outperforming all previous datasets. Furthermore,
the introduction of UniMERNet enhances the model’s per-
formance in formula recognition, achieving higher accuracy
and speeds. All data, models, and code are available at https:
//github.com/opendatalab/UniMERNet.
Introduction
Mathematical Expression Recognition (MER) is a critical
task in document analysis, aiming to convert image-based
mathematical expressions into corresponding markup lan-
guages such as LaTeX or Markdown. MER is essential in
applications like scientific document extraction, where a ro-
bust MER model helps maintain the logical coherence of
documents. Unlike typical Optical Character Recognition
(OCR) tasks, MER requires a deeper understanding of com-
plex structures, including superscripts, subscripts, and vari-
ous special symbols.
Existing research has primarily focused on enhancing the
recognition accuracy of relatively simple rendered expres-
sions (Deng et al. 2017) and handwritten data (Mahdavi et al.
2019; Le, Indurkhya, and Nakagawa 2019; Wu et al. 2020;
Zhao et al. 2021) through a series of MER algorithms. Some
*Equal contribution.
†Corresponding author (heconghui@pjlab.org.cn).
Figure 1: ||Performance||org011|| comparison (BLEU Score) of main-
stream models and UniMERNet in recognizing real-world
mathematical expressions: Evaluation across Simple Printed
Expressions (SPE), Complex Printed Expressions (CPE),
Screen-Captured Expressions (SCE), and Handwritten Ex-
pressions (HWE).
researchers have begun to optimize MER algorithms by scal-
ing up the training data and integrating them with trans-
former models (Vaswani et al. 2017), ensuring their appli-
cability in diverse scenarios (Kim et al. 2022; Blecher 2022;
Blecher et al. 2023; Paruchuri 2023). ||Other||gpe016|| researchers have
attempted to directly employ Large Vision-Language Mod-
els (LVLMs) for document content extraction, including
MER (Wei et al. 2023; Blecher et al. 2023). However, ex-
isting MER benchmarks (Deng et al. 2017; Mahdavi et al.
2019) primarily focus on simple printed or handwritten ex-
pressions. Consequently, these models often struggle with
diverse real-world expressions, such as lengthy equations
and noisy scanned document screenshots.
In practice, real-world scenarios require the handling of
complex, long expressions and noisy, distorted images from
scanned documents or webpage screenshots. To fill this gap,
we introduce a comprehensive benchmark, UniMER-Test,
which extends the existing test set with longer and real-
world scenario expressions. Our benchmark aims to stim-
ulate progress in MER by focusing on robustness and prac-
tical usage. As depicted in Figure 1, we conduct exhaustive
evaluations of state-of-the-art MER methods (Blecher 2022;
Paruchuri 2023) using our novel benchmark, UniMER-Test.
These methods demonstrate remarkable competence in rec-
arXiv:2404.15254v2  [cs.CV]  5 Sep 2024


# Page 2

ognizing simple printed expressions. However, their per-
formance noticeably declines when tested with more com-
plex printed expressions, particularly long formulas. The
performance degradation becomes even more pronounced
when these methods are applied to real-world expressions,
such as screen-captured expressions embedded in noisy
backgrounds and handwritten expressions. Moreover, large
vision-language models such as Nougat (Blecher et al. 2023)
and Vary (Wei et al. 2023), despite their capacity for conve-
nient end-to-end document content extraction, exhibit only
mediocre performance in MER.
To train a high-quality formula recognition model capa-
ble of accurately predicting results in diverse scenarios, we
have constructed the UniMER-||1.0||meas002|| dataset. This large-scale
dataset is specifically designed for Mathematical Expres-
sion Recognition (MER) and includes over one million di-
verse formula Image-LaTeX pairs. During its construction,
we considered various levels of formula complexity, rang-
ing from simple to complex long formulas, as well as dif-
ferent types, including printed and handwritten formulas.
This ensures the dataset’s suitability for training a model
that generalizes well to real-world scenarios. Furthermore,
to fully leverage the UniMER dataset, we propose an in-
novative formula recognition model—UniMERNet. Unlike
the mainstream document recognition frameworks that di-
rectly use the Swin-Transformer encoder and mBART de-
coder, we have optimized the model structure specifically
for the formula recognition task. In the encoder, we intro-
duce the Fine-Grained Embedding (FGE) module and the
Convolutional Enhancement (CE) module for local context
awareness. In the decoder, we incorporate the Squeeze At-
tention (SA) module to accelerate inference. These enhance-
ments result in significant improvements in both inference
speed and accuracy.
The main contributions of this paper are as follows:
• We introduce UniMER1 (He et al. 2024), a universal
MER dataset, with the training set UniMER-||1.0||meas002|| and the
test set UniMER-Test, which encompasses all types of
expressions in practical situations, offering a diverse and
comprehensive foundation for MER model development
and evaluation.
• We propose a novel network structure, UniMERNet,
specifically designed for the formula recognition task. By
designing a more precise encoder and a faster decoder,
we can freely combine models to achieve higher accu-
racy and faster speed in formula recognition.
• Validation
of
UniMERNet’s
superior
performance
through extensive experiments, establishing it as the new
benchmark in ||open||org014||-source MER solutions by outper-
forming existing models in a variety of scenarios.
Related Work
Traditional Machine Learning Methods in MER
Decades ago, researchers recognized the importance of
Mathematical
Expression
Recognition
(MER).
Ander-
son (Anderson 1967) pioneered MER in irregular documents
1https://opendatalab.com/OpenDataLab/UniMER-Dataset
by introducing a parsing algorithm for two-dimensional
character configurations. Miller and Viola (Miller and Vi-
ola 1998) proposed a system integrating character segmen-
tation with the grammar of mathematical layouts. Chan et
al. (Chan and Yeung 1999) developed an online MER sys-
tem featuring error detection and correction mechanisms.
INFTY (||Suzuki||org050|| et al. 2003) presented an OCR system for
mathematical documents that achieved high character recog-
nition accuracy through novel techniques. However, despite
these advancements, MER precision was limited by hand-
crafted features in traditional machine learning.
Deep Learning and Transformer Methods in MER
With the advent of deep learning, various MER al-
gorithms
based
on
Convolutional
Neural
Networks
(CNN)(Krizhevsky, Sutskever, and Hinton 2012; Simonyan
and Zisserman 2015) were proposed. Deng et al.(Deng
et al. 2017) introduced an encoder-decoder model with a
coarse-to-fine attention mechanism, demonstrating superior
performance over traditional OCR systems using the
IM2LATEX-100K dataset. The WAP model (Zhang et al.
2017) autonomously learned mathematical grammar and
symbol segmentation, aligning closely with human intu-
ition, while the PAL-v2 model (Wu et al. 2020) used paired
adversarial learning to excel in handwritten expression
recognition on the CROHME dataset. Zhang et al.(Zhang
et al. 2020) proposed a tree-structured decoder for complex
markups, and Zhao et al.(Zhao et al. 2021) and Bian et
al.(Bian et al. 2022) enhanced MER with bi-directional
learning in encoder-decoder models, advancing Handwritten
Mathematical Expression Recognition (HMER). The CAN
model(Li et al. 2022) improved HMER by incorporating
a weakly supervised counting module, while Le et al.(Le,
Indurkhya, and Nakagawa 2019) and Li et al.(Li et al. 2020)
employed data augmentation strategies to enhance MER
performance.
More recently, the rapid development of Transformer
models (Vaswani et al. 2017) and large vision-language
models (Zhu et al. 2023; Liu et al. 2024; Dong et al. 2024;
Liu et al. 2023; Wang et al. 2024; Zhang et al. 2024; Chen
et al. 2024) led researchers to explore document information
extraction task based on meticulously constructed evaluation
benchmarks, such as DocGenome (Xia et al. 2024) and MM-
Sci (Li et al. 2024). For example, Donut (Kim et al. 2022)
introduced an end-to-end model that converts document im-
ages into structured outputs without relying on OCR, while
Nougat (Blecher et al. 2023) utilized auto-generated image-
to-markup samples to train a Transformer-based encoder-
decoder model. Vary (Wei et al. 2023) offered a fine-grained
multimodal model for document parsing. However, these
methods often overlook the unique characteristics of mathe-
matical expressions, leading to limitations in their MER ca-
pabilities. To address this, Pix2tex (Blecher 2022) and Tex-
ify (Paruchuri 2023) trained encoder-decoder models on ren-
dered mathematical expressions, though they struggle with
complex or noisy expressions.
In response to these challenges, the UniMERNet model
proposed in this paper aims to build a robust and practi-
cal MER model that not only achieves state-of-the-art per-


# Page 3

Figure 2: Visualization of the UniMER-Test dataset with
four data types: Simple Printed Expressions (SPE), Com-
plex Printed Expressions (CPE), Screen Capture Expres-
sions (SCE), and Handwritten Expressions (HWE).
formance but also optimizes inference speed, enhancing the
model’s applicability in real-world scenarios.
UniMER Dataset
The UniMER dataset addresses the diversity of formula
recognition challenges in real-world scenarios. It consists of
two main components: UniMER-||1.0||meas002||, a large-scale training
set, and UniMER-Test, a comprehensive evaluation set.
UniMER-||1.0||meas002|| includes 1,061,791 latex-image pairs, cov-
ering both simple and complex printed and handwritten for-
mulas (Table 1). This extensive dataset surpasses existing
formula recognition training sets, enabling the development
of more robust models.
UniMER-Test, on the ||other||gpe018|| hand, is a test set contain-
ing 23,789 samples. Unlike existing evaluation sets that pri-
marily focus on simple printed and handwritten formulas,
UniMER-Test comprehensively evaluates formula recogni-
tion across varying complexities and types, reflecting real-
world scenarios (Figure 2). Specifically, UniMER-Test in-
cludes the following types of formulas:
• SPE: Formula images rendered from simple LaTeX ex-
pressions, characterized by uniform font size, clean back-
ground, and relatively short formulas.
• CPE: Formula images rendered from complex, long La-
TeX expressions, characterized by uniform font size,
clean background, and longer, more intricate formulas.
• SCE: Screen-captured images of formulas from docu-
ments and the web, characterized by inconsistent fonts
and sizes, background noise, and image deformation.
• HWE: Collected from referenced handwriting recogni-
tion datasets (Mouchere et al. 2014; Mouch`ere et al.
2016; Mahdavi et al. 2019; Yuan et al. 2022), these are
complex and diverse, with varying backgrounds, but are
relatively short.
This comprehensive approach ensures that UniMER-Test
serves as a robust benchmark for evaluating formula recog-
nition systems, setting a new standard for future research
and development in the field.
Data Collection Process
Printed Rendered Expressions (SPE, CPE)
The assem-
bly of our dataset begins with the Pix2tex (Blecher 2022)
public dataset, which serves as ||the base||org042|| for our SPE. Due
to the limitations in volume and complexity, we expand
the dataset by sourcing additional LaTeX expression codes
from platforms like Arxiv, Wikipedia, and ||StackExchange||org052||.
These codes are regularized (Deng et al. 2017) to resolve
LaTeX syntax ambiguities, then compiled into expression
PDFs in various fonts using XeLaTeX. Uncompilable ex-
pressions are discarded. Subsequently, ImageMagic’s con-
version function is utilized to transform these images into
expressions with multiple DPIs, with data balancing ensur-
ing an even distribution of different lengths.
Following this data expansion pipeline, we sample
725,246 simple formulas from the augmented data and com-
bine them with the Pix2tex training set to form the SPE train-
ing data. The Pix2tex test set is designated as the SPE test
data. In contrast, the CPE is derived independently of the
Pix2tex dataset. We randomly select 110,332 complex for-
mulas from the expanded data for training and test sets.
Screen-Captured Expressions (SCE)
For SCE, we com-
pile 1,000 diverse PDF pages in both ||Chinese||gpe011|| and ||English||gpe012||,
covering books, papers, textbooks, magazines, and news-
papers. This variety ensures a wide range of fonts, sizes,
and backgrounds for the formulas. Two annotators iden-
tify and label the formula boxes in the documents, captur-
ing the content automatically. This process produces over
6,000 formula boxes, which are processed through Math-
pix for formula recognition. After manual corrections and
cross-verification by two annotators, redundant formulas are
removed, resulting in 4,744 unique mathematical expression
for the SCE test set.
Handwritten Expressions (HWE)
For HWE, we uti-
lize the public datasets CROHME (Mouchere et al.
2014; Mouch`ere et al. 2016; Mahdavi et al. 2019) and
HME100K (Yuan et al. 2022). CROHME, a well-known
dataset in HMER, originates from the handwritten digit
recognition competition and includes 8,836 training expres-
sions and 3,332 test expressions. HME100K, a real-world
handwritten expression dataset, provides 74,502 training
and 24,607 test images. Due to the high annotation accu-
racy of these datasets, we combine them for our HWE data.
Specifically, the HWE training set consists of 8,836 formu-
las from CROHME and 74,502 from HME100K, totaling
83,338 samples. The HWE test set includes 3,332 formulas
from CROHME and 3,000 from HME100K, totaling 6,332
test formulas.
Diversified Training Data Sampling
Existing formula datasets, such as HWE (CHROME &
HME100K) (Mouchere et al. 2014; Mouch`ere et al. 2016;
Mahdavi et al. 2019), IM2LATEX (Deng et al. 2017), and
Pix2tex (Blecher 2022), primarily consist of rendered and


# Page 4

Dataset
Type
Train Size
Test Size
||Max Len||p004||
||Avg Len||p003||
HME100K
HWE
74,502
24,607
311
24.05
CROHME
8,836
3233
147
22.27
IM2LATEX-100K
SPE
83,883
10,354
440
96.01
Pix2tex
158,480
30,637
2949
93.35
UniMER
Mixed
1,061,791
23,757
7037
79.48
Table 1: Statistical comparison of the MER dataset. “||Max Len||p004||” and “||Avg Len||p003||” mean the maximum length and average string
length of the mathematical expression.
Figure 3: The overall framework of UniMERNet. The UniMER-Encoder incorporates Fine-Grained Embedding (FGE), Convo-
lutional Enhancement (CE), and Removal of Shift Window (RSW) to enhance recognition capabilities. The UniMER-Decoder
employs Squeeze Attention (SA) to accelerate inference speed.
handwritten formulas, but they have limitations in formula
length and complexity. For example, Pix2tex mostly con-
tains regular formulas, lacking extremely short or complex
long formulas, while handwritten formulas are generally
short with diverse styles, none exceeding 256 characters.
To address these limitations, we expand our UniMER-
||1.0||meas002|| dataset with a wider range of formulas, sampled from
sources like Arxiv and Wikipedia to ensure a balanced dis-
tribution of lengths and complexity. This varied sampling
strategy enhances the model’s ability to recognize formulas
across different complexity levels, improving overall per-
formance. The formula length distribution of IM2LATEX,
Pix2tex, HWE, and UniMER-||1.0||meas002|| datasets is shown in Fig-
ure 4.
UniMERNet
In real-world scenarios, mathematical formulas come from
diverse sources such as electronic documents, scanned im-
ages, screenshots, and photographs. They range from sin-
gle symbols to complex, lengthy expressions. Unlike gen-
eral text recognition, formula recognition poses unique chal-
lenges in three dimensions. Visual Similarity: Many for-
mula symbols look similar, e.g., µ and u, β and B. This
requires the model to have precise recognition capabili-
ties. Spatial Information: Formulas often contain super-
scripts, subscripts, and ||other||gpe018|| spatial arrangements, necessi-
tating model’s contextual awareness. Inference Speed: For
complex and lengthy formulas, the symbol generation based
on an encoder-decoder structure can be time-consuming,
slowing down the model’s inference speed.
To address these challenges, we design the UniMER-
Net formula recognition network. This network enhances
recognition capabilities by incorporating fine-grained and
context-aware modules in the encoder stage and accelerates
inference speed by compressing the attention operation in
the decoder stage.
Our architecture is based on the Swin-Transformer En-
coder and mBART Decoder, which have been validated in
various document processing tasks (Kim et al. 2022; Blecher
et al. 2023; Paruchuri 2023). The overall framework of
UniMERNet is shown in Figure 3.
During
training,
each
input
formula
image
I
∈
R3×H0×W0 undergoes an image augmentation module,
transforming a single image representation into a diverse
set of images. This effectively handles the varied representa-
tions of formulas in real-world scenarios. The UniMERNet
encoder processes the image to generate a feature vector Z,
which is then fed into the UniMERNet decoder. The decoder
interacts with the feature vector Z and the output text se-
quence via a cross-attention mechanism to generate the pre-
dicted formula. The decoder combines the feature vector Z,
token embedding, and position embedding to predict the for-
mula. For language modeling loss, we employ cross-entropy
loss to minimize the difference between the predicted prob-


# Page 5

IM2LATEX-Train
Pix2tex-Train
HWE-Train
UniMER-||1.0||meas002||
Frequency
||Length||org029||
Frequency
||Length||org029||
45k
30k
15k
0
80k
60k
40k
20k
0
60k
45k
30k
15k
0
～8
～64 ～256～512 513+
～8
～64 ～256～512 513+
～8
～64 ～256～512 513+
～8
～64 ～256～512 513+
400k
300k
200k
100k
0
Figure 4: Formula string length distribution across datasets
ability distribution of the next token and the actual distribu-
tion observed in the training data. The loss is defined as:
ℓlm(ˆy, y) = −
C
X
c=1
yo,c log(ˆyo,c),
(1)
Next, we detail the improvements in the UniMERNet net-
work architecture tailored for formula recognition tasks.
UniMERNet-Encoder
Fine-Grained Embedding (FGE). The original Swin-
Transformer uses a 4 × 4 single-layer convolutional ker-
nel with a stride of 4 for patch embedding, resulting in a
fourfold reduction in input resolution. However, these non-
overlapping convolutional kernels often extract fragmented
features, causing characters within formulas to be separated
and adversely affecting recognition accuracy. Conversely,
using larger overlapping convolutional kernels may lead to
redundancy, as MER typically involves simpler elements
like superscripts, subscripts, and various special characters,
necessitating precise and streamlined feature extraction.
To address this, we implement an overlapping fine convo-
lution layer composed of two convolutional layers, a Layer
Normalization (LN) layer, and a GELU activation layer.
Both convolutional layers utilize a kernel size of 3, a stride
of 2, and padding of 1. This overlapping fine convolution
expands the receptive field while mitigating fragmentation,
thereby enhancing the model’s overall performance.
Convolutional Enhancement (CE). While we retain the
Window Attention mechanism from the Swin-Transformer,
the receptive field within each window remains relatively
large, limiting attention to small details such as labels in for-
mulas and lacking inductive bias. Research in the visual do-
main (Guo et al. 2022) (Chu et al. 2021) (d’Ascoli et al.
2021) suggests that convolutional and transformer models
are complementary. In formula recognition, both global and
local information are crucial. Global information enables the
model to discern where to pause and where a new line be-
gins, while local information helps the model understand the
relationships between adjacent characters, particularly for
local relationships like superscripts and subscripts.
Transformer architectures capture global information
well, but discerning local details is critical. To address
this, we introduce a local perception module called Con-
vEnhance, which enhances the model’s ability to identify
superscripts and subscripts. ConvEnhance, placed before
each attention and MLP module, consists of 3x3 depthwise
convolutions and GELU activation functions. The convolu-
tions provide local perception, while the GELU activation
implements gated threshold filtering. This addition allows
UniMERNet to alternate between local and global informa-
tion, significantly improving its performance.
Removal of Shift Window (RSW). The Shift Window op-
eration aims to increase the model’s receptive field by over-
lapping windows between layers, similar to convolutional
kernels. However, with the introduction of FGE and CE, the
receptive field is already significantly enlarged compared to
the original Swin architecture. This makes the ||Shift Win||p005||-
dow operation redundant. Removing it not only improves
the model’s performance but also speeds up the model.
UniMERNet-Decoder
Squeeze Attention (SA). Experiments presented in the ap-
pendix indicate that the throughput bottleneck of UniMER-
Net lies within the language model, mBART. This is pri-
marily because the visual model retrieves all visual features
in a single pass, while the language model must iteratively
utilize the previous prediction results to generate each suc-
cessive token. Consequently, the throughput limitation is at-
tributable to the language model. To address this, we intro-
duce the concept of bottleneck from ResNet, implementing
Squeeze Attention. Specifically, Squeeze Attention maps the
query and key to a lower-dimensional space without exces-
sive loss of information, thereby accelerating the computa-
tion of attention.
The
enhanced
UniMERNet-Encoder
captures
fine-
grained
and
local
contextual
information,
improving
formula recognition accuracy. The optimized UniMER-
Decoder, with slightly reduced precision, achieves signif-
icant speed improvements, making it highly effective for
practical formula recognition systems.
Experiments
Datasets and Evaluation Metrics
We utilize the UniMER-||1.0||meas002|| dataset to train our model
and evaluate its formula recognition performance using the
UniMER-Test. Our evaluation relies on BLEU, Edit Dis-
tance, and ExpRate metrics.
BLEU: The BLEU score (Papineni et al. 2002), initially de-
veloped for machine translation, quantifies the match of n-
grams between candidate and reference sentences. Its appli-
cation to a similar conversion task of formula recognition
provides a robust, quantitative performance measure.
Edit distance: The Edit Distance (Levenshtein et al. 1966)
measures the minimum character changes needed to convert


# Page 6

Train
Dataset
SPE
CPE
SCE
HWE
BLEU ↑
EditDis ↓
BLEU ↑
EditDis ↓
BLEU ↑
EditDis ↓
BLEU ↑
EditDis ↓
Pix2tex
0.911
0.063
0.773
0.194
0.527
0.371
0.067
0.800
Pix2tex&HWE
0.909
0.063
0.724
0.225
0.529
0.309
0.873
0.088
UniMER-||1.0||meas002||
0.915
0.060
0.925
0.056
0.626
0.224
0.895
0.072
Table 2: Ablation results on UniMER-Test for models using different augmentations. Here, “HME” refers to a mixed dataset of
CHROME and HME100K.
FGE
CE
RSW
SA
Params
FPS*
SPE
CPE
SCE
HWE
(M)
(img/s)
BLEU ↑
BLEU ↑
BLEU ↑
BLEU ↑
342
4.12
0.903
0.885
0.579
0.887
"
342
4.10
0.903
0.888
0.584
0.886
"
"
342
4.07
0.912
0.896
0.599
0.895
"
"
"
325
5.04
0.911
0.894
0.599
0.893
"
"
"
"
325
5.06
0.912
0.897
0.601
0.893
||Table 3||org039||: Ablation study of model architecture. FGE: replace with Fine-Grained Embedding, CE: add ConvEnhance module,
RSW: Remove Shift Window, SA: apply Squeeze Attention in mBART decoder. FPS*: The model’s throughput is tested on an
A100 GPU with a ||batch||org034|| size of 128, processing each sample up to a maximum sequence length of 1536.
one string to another. Its use in formula recognition offers
a precise, character-level accuracy assessment, making it a
valuable performance metric.
ExpRate: Expression Recognition Rate (Yuan et al. 2022)
is a widely used metric for handwritten formula recognition,
defined as the percentage of predicted mathematical expres-
sions that perfectly match the actual results.
Implementation Details
The proposed model, UniMERNet, uses PyTorch with a
maximum sequence length set to 1536. Training is con-
ducted on a single GPU equipped with CUDA. Specifically,
we utilize an ||NVIDIA||org035|| A100 with 80GB of memory. During
the training phase, we employ eight such GPUs with a ||batch||org034||
size of 64. The learning rate schedule is linear warmup co-
sine, with an initial learning rate of 1 × 10−4, a minimum
learning rate of 1 × 10−8, and a warmup learning rate of
1 × 10−5. Weight decay is set to 0.05. The total iteration is
set to 300,000 by our default settings.
The architectural hyperparameters of UniMERNet in-
stances are illustrated in Table 4. Let N denote the depth
of the UniMERNet encoder, where [6, 6, 6, 6] indicates that
each stage, from the first to the last, consists of six trans-
former layers. Meanwhile, M represents the depth of the
UniMERNet Decoder. C signifies the dimensionality of the
vectors after processing through the Encoder, and Params
refers to the total number of parameters in the model.
Ablation Study
UniMER-||1.0||meas002||
The diversity and quantity of training data
are crucial for accurate formula recognition. As shown in
Table 2, UniMERNet-B, trained solely on Pix2tex, achieves
a BLEU score of 0.911 on SPE but performs poorly on CPE
N
M
C
Params
UniMERNet-T
[6, 6, 6, 6]
8
512
||100.0||meas005||
UniMERNet-S
[6, 6, 6, 6]
8
768
||202.0||meas006||
UniMERNet-B
[6, 6, 6, 6]
8
1024
||325.0||meas007||
Table 4: Architectural hyper-parameters of UniMERNet.
and HWE. The simplicity of Pix2tex leads to overfitting on
SPE and difficulties with complex and handwritten formu-
las. Training on both Pix2tex and HWE improves the BLEU
score on HWE to 0.873 but slightly declines on CPE. No-
tably, training with our UniMER-||1.0||meas002|| dataset, UniMERNet-
B excels across all subsets. Compared to Pix2tex&HWE,
the CPE BLEU score improves by 0.201, and Edit Distance
decreases from 0.225 to 0.056. On SCE, BLEU improves
by 0.097, and Edit Distance decreases from 0.309 to 0.224.
For HWE, BLEU improves by 0.022, and Edit Distance de-
creases to 0.072.
Model Architecture Design
As shown in ||Table 3||org039||, we
conduct ablation experiments to validate our proposed op-
timization modules for the encoder, including Fine-Grained
Embedding (FGE) and ConvEnhance (CN), and the de-
coder optimization module, Squeeze Attention (SA), along
with the Remove Shift Window (RSW) operation. Our
baseline architecture, Texify (Paruchuri 2023), uses the
Swin-Transformer Encoder and mBART Decoder, similar to
Donut (Kim et al. 2022) and Nougat (Blecher et al. 2023).
We train randomly initialized models from scratch using the
UniMER-||1.0||meas002|| dataset.
From the comparison results, it is evident that incorporat-
ing the FGE and CE modules leads to a stable performance


# Page 7

Method
Params
FPS
SPE
CPE
SCE
HWE
(M)
(img/s)
BLEU ↑
EditDis ↓
BLEU ↑
EditDis ↓
BLEU ↑
EditDis ↓
BLEU ↑
EditDis ↓
Pix2tex (Blecher 2022)
-
-
0.873
0.088
0.655
0.408
0.092
0.817
0.012
0.920
Texify (Paruchuri 2023)
312
4.16
0.906
0.061
0.690
0.230
0.420
0.390
0.341
0.522
Texify*
312
4.16
0.906
0.067
0.900
0.077
0.599
0.224
0.888
0.075
UniMERNet-T
107
7.20
0.909
0.066
0.902
0.075
0.566
0.239
0.883
0.078
UniMERNet-S
202
6.04
0.913
0.061
0.920
0.060
0.618
0.228
0.889
0.075
UniMERNet-B
325
5.06
0.915
0.060
0.925
0.056
0.626
0.224
0.895
0.072
Table 5: Comparison with SOTA methods on UniMER-Test. Note: Texify* is trained using UniMER-||1.0||meas002|| and the same data
augmentation methods described in this paper.
Model
Pre
SPE
CPE
SCE
HWE
BLEU ↑
BLEU ↑
BLEU ↑
BLEU ↑
Texify
%
0.903
0.884
0.576
0.886
Texify
"
0.906
0.903
0.599
0.888
UniMERNet-B
%
0.912
0.897
0.601
0.893
UniMERNet-B
"
0.915
0.925
0.626
0.895
Table 6: Ablation of pre-training with text-image pairs. The
Pre column indicates whether pre-training is used. Note: For
Texify and UniMERNet-B, we use the same in-house text-
image pre-training data for fair comparison.
improvement across all subsets, with a significant increase
observed in the SCE subset. The combination of these two
modules improves the BLEU score from 0.579 to 0.599, an
increase of nearly 2%. When the SA module is applied to the
decoder, the model’s inference speed increases from 4.07 to
5.04, a relative improvement of 24%, with minimal loss in
accuracy. As mentioned in the methods section, with the CE
module, the Shift Window becomes unnecessary. Removing
the Shift Window results in slight improvements in both ac-
curacy and speed, achieving the optimal configuration.
At this optimal configuration, the model’s accuracy sig-
nificantly improves compared to the baseline model, with
BLEU score increases of 0.9%, 1.2%, 2.2%, and 0.6% on
the SPE, CPE, SCE, and HWE subsets, respectively. The
speed also improves by 23%, demonstrating the substantial
advantages of our UniMERNet in formula recognition.
Pre-training with Text Image Pairs
Pretraining on large-scale datasets significantly boosts
model performance in document recognition. Models like
Donut, Texify, and Nougat benefit greatly from this ap-
proach. Due to limited available data, we use Arxiv pa-
pers, applying text layout detection and OCR to extract text
blocks and match them with source code, resulting in 16 mil-
lion image-text pairs. As shown in Table 6, both the baseline
model Texify and our UniMERNet-B exhibit significant im-
provements in SPE, CPE, and SCE metrics, with a relatively
smaller gain in HWE. This is expected, as Arxiv papers
predominantly feature printed text, differing from handwrit-
ten digit recognition. Incorporating more diverse pretraining
UniMERNet
Texify
Sample B
Sample C
Pix2tex
Syntax Error
UniMERNet
Nougat
Nougat
Texify
Syntax Error
Pix2tex
Sample A
UniMER 
Net
Texify
Pix2tex
Nougat
Syntax Error
Figure 5: Comparative Visualization of Recognition Results
Using Different Methods.
data would further improve overall performance.
Comparison with SOTA Methods
To more intuitively
evaluate the formula recognition performance of UniMER-
Net, we compared it with the state-of-the-art methods in
the document recognition field. As shown in Table 5, when
using the same network architecture as Texify, ||the base||org042||-
line model Texify* significantly outperforms original Texify,
underscoring the importance of the UniMER-||1.0||meas002|| dataset.
Moreover, our lightweight model UniMERNet-T, with an
inference speed 1.73 times faster, already surpasses the pre-
vious SOTA model Texify. Using UniMERNet-B, the infer-
ence speed is 21.6% faster compared to Texify. The BLEU
scores on the SPE, CPE, SCE, and HWE subsets show abso-
lute improvements of 0.009, 0.235, 0.206, and 0.554 respec-
tively, demonstrating the superior recognition accuracy and
robustness of our model across various scenarios.
Qualitative Comparisons
As shown in Figure 5, we
selected three representative samples from the UniMER-
Test set to thoroughly compare the performance between
Pix2tex, Texify, Nougat, and UniMERNet. It’s important to
highlight that Nougat, being primarily designed for full-page
recognition, tends to underperform with isolated formulas;
thus, we prepared the test images by integrating random text
with the formulas to adapt to Nougat’s inference capabilities.
Notably, while the ||other||gpe018|| models exhibit certain shortcom-
ings in handling these test samples, our model consistently
delivers robust and accurate recognition results.


# Page 8

Conclusion
This paper introduces the large-scale training dataset
UniMER-||1.0||meas002|| and the diverse evaluation dataset UniMER-
Test, contributing significantly to robust formula recognition
and fair evaluation. We also designe UniMERNet, a model
with ||superior detail||org045|| perception and contextual understand-
ing, achieving high accuracy and speed, making it valuable
for practical applications. Moving forward, we will explore
using larger and more diverse pre-training data to enhance
formula recognition. Additionally, we will investigate inte-
grating UniMERNet with large vision-language models to
improve the recognition of documents containing text, for-
mulas, and tables, advancing document understanding.
References
Anderson, R. H. 1967. Syntax-directed recognition of hand-
printed two-dimensional mathematics.
In Symposium on
interactive systems for experimental ||applied mathematics||org047||:
Proceedings of the ||Association for Computing Machinery||org046||
Inc. Symposium, 436–459. 2
Bian, X.; Qin, B.; Xin, X.; Li, J.; Su, X.; and Wang, Y.
2022. Handwritten mathematical expression recognition via
attention aggregation based bi-directional mutual learning.
In Proceedings of the AAAI Conference on Artificial Intelli-
gence (AAAI), volume 36, 113–121. 2
Blecher, L. 2022. pix2tex - LaTeX OCR. https://github.com/
lukas-blecher/LaTeX-OCR. Accessed: ||2024-02-29||d001||. 1, 2, 3,
7, 10, 11
Blecher, L.; Cucurull, G.; Scialom, T.; and Stojnic, R. 2023.
Nougat: Neural optical understanding for academic docu-
ments. arXiv.org, 2308.13418. 1, 2, 4, 6
Chan, K.; and Yeung, D. 1999. Error detection, error cor-
rection and performance evaluation in on-line mathematical
expression recognition. 2
Chen, Z.; Wang, W.; Tian, H.; Ye, S.; ||Gao||gpe038||, Z.; Cui, E.; Tong,
W.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024. How far are we to
gpt-4v? closing the gap to commercial multimodal models
with ||open||org014||-source suites. arXiv preprint arXiv:2404.16821.
2
Chu, X.; Tian, Z.; Zhang, B.; Wang, X.; and Shen, C. 2021.
Conditional positional encodings for vision transformers.
arXiv preprint arXiv:2102.10882. 5
Deng, Y.; Kanervisto, A.; Ling, J.; and Rush, A. M. 2017.
Image-to-markup generation with coarse-to-fine attention.
In International Conference on Machine Learning (ICML),
980–989. PMLR. 1, 2, 3, 10, 11
Dong, X.; Zhang, P.; Zang, Y.; Cao, Y.; Wang, B.; Ouyang,
L.; Wei, X.; Zhang, S.; Duan, H.; Cao, M.; et al. 2024.
||InternLM||org048||-XComposer2: Mastering Free-form Text-Image
Composition and Comprehension in Vision-Language Large
Model. arXiv.org, 2401.16420. 2
d’Ascoli, S.; Touvron, H.; Leavitt, M. L.; Morcos, A. S.;
Biroli, G.; and Sagun, L. 2021. Convit: Improving vision
transformers with soft convolutional inductive biases.
In
International conference on machine learning, 2286–2296.
PMLR. 5
Guo, J.; Han, K.; Wu, H.; Tang, Y.; Chen, X.; Wang, Y.;
and Xu, C. 2022. Cmt: Convolutional neural networks meet
vision transformers. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, 12175–
12185. 5
He, C.; Li, W.; Jin, Z.; Xu, C.; Wang, B.; and Lin, D.
2024. Opendatalab: Empowering general artificial intelli-
gence with ||open||org014|| datasets. arXiv preprint arXiv:2407.13773.
2
Kim, G.; Hong, T.; Yim, M.; Nam, J.; Park, J.; Yim, J.;
Hwang, W.; Yun, S.; Han, D.; and Park, S. 2022. Ocr-free
document understanding transformer. In ||European||gpe009|| Confer-
ence on Computer Vision (ECCV), 498–517. Springer. 1, 2,
4, 6
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-
agenet classification with deep convolutional neural net-
works. NeurIPS, 25. 2
Le, A. D.; Indurkhya, B.; and Nakagawa, M. 2019. Pattern
generation strategies for improving recognition of handwrit-
ten mathematical expressions. Pattern Recognition Letters,
128: 255–262. 1, 2
Levenshtein, V. I.; et al. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet physics
doklady, volume 10, 707–710. Soviet Union. 5
Li, B.; Yuan, Y.; Liang, D.; Liu, X.; Ji, Z.; Bai, J.; Liu,
W.; and Bai, X. 2022.
When counting meets HMER:
counting-aware network for handwritten mathematical ex-
pression recognition. In ||European||gpe009|| Conference on Computer
Vision (ECCV), 197–214. Springer. 2
Li, Z.; Jin, L.; Lai, S.; and Zhu, Y. 2020.
Improving
attention-based handwritten mathematical expression recog-
nition with scale augmentation and drop attention. In Inter-
national Conference on Frontiers in Handwriting Recogni-
tion (ICFHR), 175–180. IEEE. 2
Li, Z.; Yang, X.; Choi, K.; Zhu, W.; Hsieh, R.; Kim, H.; Lim,
J. H.; Ji, S.; Lee, B.; Yan, X.; et al. 2024. MMSci: A Mul-
timodal Multi-Discipline Dataset for PhD-||Level||org049|| Scientific
Comprehension. arXiv preprint arXiv:2407.04903. 2
Liu, H.; Li, C.; Li, Y.; and Lee, Y. J. 2023. Improved base-
lines with visual instruction tuning. arXiv.org, 2310.03744.
2
Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2024. Visual instruc-
tion tuning. volume 36. 2
Mahdavi, M.; Zanibbi, R.; Mouchere, H.; Viard-Gaudin, C.;
and Garain, U. 2019. ICDAR 2019 CROHME+ TFD: Com-
petition on recognition of handwritten mathematical expres-
sions and typeset formula detection. In International Con-
ference on Document Analysis and Recognition (ICDAR),
1533–1538. IEEE. 1, 3
Miller, E.; and Viola, P. 1998. Ambiguity and constraint in
mathematical expression recognition. National Conference
on Artificial Intelligence (NCAI). 2
Mouchere, H.; Viard-Gaudin, C.; Zanibbi, R.; and Garain,
U. 2014. ICFHR 2014 competition on recognition of on-line
handwritten mathematical expressions (CROHME 2014).
In International Conference on Frontiers in Handwriting
Recognition (ICFHR), 791–796. IEEE. 3


# Page 9

Mouch`ere, H.; Viard-Gaudin, C.; Zanibbi, R.; and Garain,
U. 2016. ICFHR2016 CROHME: Competition on recogni-
tion of online handwritten mathematical expressions. In In-
ternational Conference on Frontiers in Handwriting Recog-
nition (ICFHR), 607–612. IEEE. 3
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
Bleu: a method for automatic evaluation of machine trans-
lation.
In Proceedings of the 40th annual meeting of the
Association for Computational Linguistics, 311–318. 5
Paruchuri,
V.
2023.
Texify.
https://github.com/
VikParuchuri/texify. Accessed: ||2024-02-29||d001||. 1, 2, 4, 6, 7
Simonyan, K.; and Zisserman, A. 2015. Very deep convolu-
tional networks for large-scale image recognition. In ICLR.
Computational and Biological Learning Society. 2
||Suzuki||org050||, M.; Tamari, F.; Fukuda, R.; Uchida, S.; and Kana-
hori, T. 2003. Infty: an integrated ocr system for mathemati-
cal documents. In Proceedings of the 2003 ACM symposium
on Document engineering, 95–104. 2
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. NeurIPS, 30. 1, 2
Wang, B.; Wu, F.; Han, X.; Peng, J.; Zhong, H.; Zhang, P.;
Dong, X.; Li, W.; Li, W.; Wang, J.; et al. 2024. Vigc: Visual
instruction generation and correction. In AAAI. 2
Wei, H.; Kong, L.; Chen, J.; Zhao, L.; Ge, Z.; Yang, J.; Sun,
J.; Han, C.; and Zhang, X. 2023. Vary: Scaling up the vi-
sion vocabulary for large vision-language models. arXiv.org,
2312.06109. 1, 2
Wu, J.-W.; Yin, F.; Zhang, Y.-M.; Zhang, X.-Y.; and Liu,
C.-L. 2020. Handwritten mathematical expression recogni-
tion via paired adversarial learning. International Journal of
Computer Vision (IJCV), 128: 2386–2401. 1, 2
Xia, R.; Mao, S.; Yan, X.; Zhou, H.; Zhang, B.; Peng, H.;
Pi, J.; Fu, D.; Wu, W.; Ye, H.; et al. 2024. DocGenome:
An ||Open||org015|| Large-scale Scientific Document Benchmark for
Training and Testing Multi-modal Large Language Models.
arXiv preprint arXiv:2406.11633. 2
Yuan, Y.; Liu, X.; Dikubab, W.; Liu, H.; Ji, Z.; Wu, Z.;
and Bai, X. 2022.
Syntax-aware network for handwrit-
ten mathematical expression recognition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 4553–4562. 3, 6
Zhang, J.; Du, J.; Yang, Y.; Song, Y.-Z.; Wei, S.; and Dai, L.
2020. A tree-structured decoder for image-to-markup gen-
eration. In International Conference on Machine Learning
(ICML), 11076–11085. PMLR. 2
Zhang, J.; Du, J.; Zhang, S.; Liu, D.; Hu, Y.; Hu, J.; Wei, S.;
and Dai, L. 2017. Watch, attend and parse: An end-to-end
neural network based approach to handwritten mathematical
expression recognition. Pattern Recognition, 71: 196–206.
2
Zhang, P.; Dong, X.; Zang, Y.; Cao, Y.; Qian, R.; Chen,
L.; Guo, Q.; Duan, H.; Wang, B.; Ouyang, L.; et al. 2024.
||Internlm||org051||-xcomposer-2.5: A versatile large vision language
model supporting long-contextual input and output. arXiv
preprint arXiv:2407.03320. 2
Zhao, W.; ||Gao||gpe038||, L.; Yan, Z.; Peng, S.; Du, L.; and Zhang,
Z. 2021.
Handwritten mathematical expression recogni-
tion with bidirectionally trained transformer.
In Interna-
tional Conference on Document Analysis and Recognition
(ICDAR), 570–584. Springer. 1, 2
Zhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M. 2023.
Minigpt-4: Enhancing vision-language understanding with
advanced large language models. arXiv.org, 2304.10592. 2


# Page 10

Appendix
Details of UniMER Dataset
SPE and CPE Sampling
Existing datasets, such as
IM2LATEX-100k and Pix2tex, present two primary chal-
lenges. Firstly, the size of these datasets, typically ranging
from 100k to 200k formulas, is insufficient for training a
precise and robust MER model. Secondly, these datasets
contain a limited number of complex formulas, which com-
promises the model’s performance, particularly in handling
multi-line complex expressions.
Figure 6: Formula length before and after re-sampling.
To address the limited size of the dataset, we expand
it by incorporating an additional 4 million LaTeX expres-
sion source codes, building on the previously mentioned
||open||org014||-source datasets. These new entries are predominantly
sourced from Arxiv (89%), with supplementary contribu-
tions from Wikipedia (9%) and ||StackExchange||org052|| (2%). This
initial dataset expansion enhances the model’s overall capa-
bilities. However, the proportion of long formulas in the ini-
tial collection is relatively small (2.3%), which may cause
inadequate training for complex expressions. To address
this, we extract the longest formulas as CPE and adjust their
ratio with randomly sampled SPE. This rearrangement en-
sures a balanced representation of varying lengths within the
dataset, thereby significantly improving the model’s ability
to recognize complex multi-line mathematical expressions.
The distribution after rearrangement is shown in Figure 6.
SCE Deduplication
When extracting mathematical for-
mulas from PDF pages, we face a unique challenge: for-
mulas originating from the same page often appear identi-
cal in content, leading to potential duplicates. Simple dedu-
plication based on textual content alone risks significant
data loss, as identical formulas can appear across different
pages, each bearing distinct visual characteristics such as
font styles, sizes, and backgrounds. To preserve the rich-
ness of visual diversity while eliminating true duplicates, we
adopted an image-based deduplication strategy, employing
Perceptual Hashing to assess image similarity. This method
allows us to compare the visual features of the formula im-
ages directly, ensuring that only those with high similar-
ity—indicating true duplicates—are removed. Through this
meticulous process of image similarity analysis, we effec-
tively reduced the dataset to 4,744 unique Screen-Captured
Expressions (SCE), each representing a distinct visual in-
stance of mathematical expressions, thereby constituting our
refined SCE test set.
Figure 7: Most frequent occurring latex symbols in
UniMER-||1.0||meas002|| and UniMER-Test subsets
Listing 1: XeLaTeX rendering setting
1
\documentclass[varwidth]{standalone}
2
\usepackage{fontspec,unicode-math}
3
\usepackage[active,displaymath,textmath,tightpage]{
preview}
4
\usepackage[total={16in, 16in}]{geometry}
5
\setmathfont{
6
% MATH_FONT
7
}
8
\begin{document}
9
\thispagestyle{empty}
10
\begin{displaymath}
11
% MATH_FORMULA
12
\end{displaymath}
13
\end{document}
Rendering Settings
For the rendering settings, we fol-
low the similar procedure used in (Deng et al. 2017) and
(Blecher 2022). The dataset is rendered using XeLaTeX with
a diverse range of math fonts and DPI settings. The cho-
sen fonts included ||Asana Math||p006||, ||Cambria Math||p007||, XITS Math,
GFS Neohellenic Math, TeX Gyre Bonum Math, TeX Gyre
Dejavu Math, TeX Gyre Pagella Math, and Latin Modern
Math, with Latin Modern Math as the default math font be-
ing employed in approximately 22% of the cases. To accom-
modate different levels of clarity and detail, the DPI setting
varies between 80 to 350 when converting to PNG format,
allowing for adjustments in the resolution and sharpness of
the rendered mathematical expressions. The rendering tem-
plate we use is shown in Listing 1.
Formula Text Normalization
LaTeX syntax inherently
contains ambiguous information, as different source codes
can produce the same rendering. This presents significant
challenges in the evaluation phase of the math formula
recognition task’s benchmark because it potentially leads to
incorrect assessments of a model’s performance despite pro-
ducing visually identical formula renderings. In handwritten


# Page 11

Figure 8: Height width scatter plot in UniMER-Test subsets
with sampling
math recognition datasets, such as CROHME, a self-defined
label graphs format is used, eliminating ambiguous expres-
sions by employing a character relation-based method. We
do not adopt these methods for normalization as they in-
volve format conversions during model training and, more
importantly, use only partial LaTeX syntax.
The LaTeX normalization is first introduced in (Deng
et al. 2017). This preprocessing operation involves fixing
super-script and sub-script order, replacing ambiguity with
unified expressions while resulting in no or minimal visual
changes in rendering, preserving the integrity of the origi-
nal mathematical expressions. Subsequent datasets, such as
IM2LATEX-100K (Deng et al. 2017) and Pix2tex (Blecher
2022), have adopted similar methods. Building on this foun-
dation of normalization, we have adjusted the normaliza-
tion rules for certain LaTeX environments, enabling bet-
ter support for multi-line formula expressions and previ-
ously unsupported syntax. All formulas in UniMER-||1.0||meas002|| and
UniMER-Test undergo this normalization process to facili-
tate better horizontal comparison with previous datasets that
employ a similar normalization process.
Data Statistics
Most Occurring Symbols
Diving into the dataset’s La-
TeX symbols offers intriguing insights into the most fre-
quently utilized mathematical notations. ||The bar||org055|| chart pro-
vided in the Figure 7 illustrates the frequency of specific
LaTeX symbols that appear in UniMER. Symbols such as
||Greek||gpe019|| letters, operators, and various mathematical func-
tions are universally prevalent in each dataset, underlining
their fundamental role in articulating complex mathematical
ideas. A subtle variation is observed in the SCE and HWE
datasets, where numbers and letters are noticeably more fre-
quently occurring, as they contain relatively easier and less
structured math expressions.
Image Size Distribution
The scatter plot in Figure 8 pro-
vides a visual distribution of image sizes across different
subsets within the UniMER-Test. Each point on the plot
represents an individual image, with its position determined
by the image’s width and height. The SPE, CPE, SCE, and
HWE subsets each exhibit unique clusters, indicating the
variety in dimensionality they encompass. It’s evident that
the SPE and SCE subsets tend to have a higher concentra-
tion of smaller images, as shown by the dense clustering of
points towards the lower end of the spectrum. The distribu-
tion of image sizes within the CPE dataset exhibits a consid-
erable spread, highlighting the diversity of dimensions that
this particular subset encompasses, indicating its complex-
ity compared to SPE. On the ||other||gpe018|| hand, the HWE subset is
characterized by images with generally larger dimensions.
This can be attributed to the fact that these images are of-
ten photographed and contain noise, necessitating a higher
resolution to ensure that the finer details of the handwritten
expressions are preserved and recognizable.
Data Augmentation
While introducing additional training data in UniMER-||1.0||meas002||
enhances the variety of formulas, it does not account for
the diversity of real-world formula images, which can come
from scanned documents or photos and can exhibit noise and
distortion. We employ various image augmentation tech-
niques during model training to simulate this diversity with
extra transformations from Albumentations 2 library and
self-defined transformations, which include but are not lim-
ited to:
• Erosion/Dilation - To simulate the textural imperfec-
tions often found in screen-captured formulas, these op-
erations modulate the thickness of characters, mirroring
the effects of resolution differences and printer anoma-
lies.
• Degradation Simulation (Fog, Frost, Rain, Snow,
Shadow) - These augmentations introduce environmental
artifacts to mimic the conditions under which documents
might be photographed in real-world scenarios, adding
layers of complexity such as blurriness and occlusions.
• Geometric Transformations (Rotation, Distortion ...) -
To account for the angle and perspective distortions typ-
ical in photographed or scanned documents, these opera-
tions adjust the orientation and shape of the mathematical
expressions.
Each image undergoes a sequence of these augmentation op-
erations with a given probability. This helps to bridge the
gap between the pristine, synthesized training data and the
noisy, real-world test images and improves UniMERNet’s
performance for real piratical use. Figure 9 provides a visu-
alization of selected transformations.
Impact of Data Augmentation on ||Performance||org011||
Data
augmentation proves to be significantly beneficial for real-
world formula recognition tasks. As shown in Table 7, incor-
porating image augmentation into the training process with
the Pix2tex dataset leads to varying degrees of improvement
across all evaluation subsets. Notably, on the SCE subset,
the BLEU score increases by 2.5%. Similar trends are ob-
served when training with the UniMER-||1.0||meas002|| dataset, where
the BLEU score on the SCE subset improves significantly
from 0.601 to 0.626, and the edit distance decreases from
0.251 to 0.224,/;.
2https://albumentations.ai


# Page 12

Train
Dataset
Augment
SPE
CPE
SCE
HWE
BLEU ↑
EditDis ↓
BLEU ↑
EditDis ↓
BLEU ↑
EditDis ↓
BLEU ↑
EditDis ↓
Pix2tex
%
0.909
0.064
0.764
0.198
0.512
0.380
0.065
0.807
"
0.911
0.063
0.773
0.194
0.527
0.371
0.067
0.800
UniMER-||1.0||meas002||
%
0.912
0.064
0.911
0.063
0.601
0.251
0.886
0.078
"
0.915
0.060
0.925
0.056
0.626
0.224
0.895
0.072
Table 7: Ablation results on UniMER-Test with models using different augmentations.
Optimal Depth Configuration for UniMERNet
In this section, we investigate the optimal depth configura-
tion for the encoder and decoder of the UniMERNet model.
Through a series of comparative experiments, we analyze
the impact of varying depths on model performance and
throughput.
Encoder Depth Analysis
Table 8 presents the results of
our experiments on different encoder depths while keep-
ing the decoder depth constant. The findings indicate that
increasing the encoder depth enhances model performance
up to a certain point. Specifically, when the encoder depth
reaches six layers per stage, the performance gains begin to
plateau. This is evident from the BLEU scores across various
evaluation metrics, which show diminishing returns beyond
this depth. Additionally, the frames per second (FPS) met-
ric reveals that increasing the encoder depth has a minimal
impact on model throughput.
Decoder Depth Analysis
Table 9 explores the effects of
varying decoder depths while keeping the encoder depth
fixed at six layers per stage. Similar to the encoder, increas-
ing the decoder depth improves performance up to a depth
of eight layers, after which the performance gains dimin-
ish significantly. However, unlike the encoder, the decoder
depth has a more pronounced impact on throughput, with
FPS decreasing notably as the decoder depth increases.
By applying the law of diminishing marginal utility and
Pareto optimality principles, we determine that the optimal
configuration for UniMERNet is an encoder depth of six
layers per stage and a decoder depth of eight layers. This
configuration balances model performance and throughput,
ensuring efficient and effective processing.


# Page 13

Figure 9: Visualization of selected image augmentations applied during training.
N
M
Params
FPS
SPE
CPE
SCE
HWE
AVG
(M)
(img/s)
BLEU ↑
BLEU ↑
BLEU ↑
BLEU ↑
BLEU ↑
[2, 2, 2, 2]
6
148
7.25
0.890
0.832
0.496
0.833
0.763
[4, 4, 4, 4]
6
167
7.23
0.897 (+0.7%)
0.856 (+2.4%)
0.544 (+4.8%)
0.870 (+3.7%)
0.792 (+2.9%)
[6, 6, 6, 6]
6
186
7.20
0.901 (+0.4%)
0.878 (+1.2%)
0.564 (+2.0%)
0.886 (+1.6%)
0.807 (+1.5%)
[8, 8, 8, 8]
6
205
7.15
0.902 (+0.1%)
0.880 (+0.2%)
0.578 (+1.4%)
0.878 (-0.8%)
0.809 (+0.2%)
Table 8: Comparative study of the impact of encoder depth on model performance. Let N denote the depth of the UniMERNet
encoder, where [6, 6, 6, 6] indicates that each stage, from the first to the last, consists of six transformer layers. Meanwhile, M
represents the depth of the UniMERNet Decoder.
N
M
Params
FPS
SPE
CPE
SCE
HWE
AVG
(M)
(img/s)
BLEU ↑
BLEU ↑
BLEU ↑
BLEU ↑
BLEU ↑
[6, 6, 6, 6]
4
169
8.36
0.893
0.850
0.532
0.863
0.785
[6, 6, 6, 6]
6
186
7.20
0.901(+0.8%)
0.878(+1.8%)
0.564(+3.2%)
0.886(+2.3%)
0.807 (+2.2%)
[6, 6, 6, 6]
8
202
6.04
0.905(+0.4%)
0.892(+1.6%)
0.587(+2.3%)
0.888(+0.2%)
0.818(+1.1%)
[6, 6, 6, 6]
10
219
4.99
0.907(+0.2%)
0.894(+0.2%)
0.582(-0.5%)
0.893(+0.5%)
0.819(+0.1%)
Table 9: Comparative study of the impact of decoder depth on model performance. Let N denote the depth of the UniMERNet
encoder, where [6, 6, 6, 6] indicates that each stage, from the first to the last, consists of six transformer layers. Meanwhile, M
represents the depth of the UniMERNet Decoder.
