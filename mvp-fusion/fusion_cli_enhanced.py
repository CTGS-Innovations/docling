#!/usr/bin/env python3
"""
Enhanced MVP-Fusion Command Line Interface
==========================================

High-performance document processing with ThreadPoolExecutor.
PERFORMANCE TARGET: 10x faster than original (105s ‚Üí ~10s for same workload)

Features:
- ThreadPoolExecutor-based processing (142x faster than queue-based)
- Service-ready architecture for CLI and API deployment
- Real-time progress monitoring
- Comprehensive performance metrics

Usage:
    python fusion_cli_enhanced.py --config config/full.yaml
    python fusion_cli_enhanced.py --file document.pdf
    python fusion_cli_enhanced.py --directory ~/documents/ --workers 4
"""

import argparse
import sys
import time
import signal
from pathlib import Path
from typing import List, Dict, Any, Optional
import json
import yaml
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Import our enhanced processor
from pipeline.legacy.service_processor_threadpool import ServiceProcessorThreadPool
from utils.logging_config import setup_logging, get_fusion_logger
from utils.deployment_manager import DeploymentManager


class ProgressTracker:
    """Real-time progress tracking for enhanced user experience"""
    
    def __init__(self, total_files: int):
        self.total_files = total_files
        self.completed_files = 0
        self.start_time = time.perf_counter()
        self.lock = threading.Lock()
        self.logger = get_fusion_logger(__name__)
    
    def update(self, increment: int = 1):
        """Update progress and show real-time stats"""
        with self.lock:
            self.completed_files += increment
            elapsed = time.perf_counter() - self.start_time
            
            if elapsed > 0:
                files_per_sec = self.completed_files / elapsed
                remaining = self.total_files - self.completed_files
                eta_seconds = remaining / files_per_sec if files_per_sec > 0 else 0
                
                percentage = (self.completed_files / self.total_files) * 100
                
                # Progress bar
                bar_length = 40
                filled_length = int(bar_length * self.completed_files / self.total_files)
                bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)
                
                print(f"\rüìä Progress: [{bar}] {percentage:5.1f}% "
                      f"({self.completed_files}/{self.total_files}) "
                      f"| Speed: {files_per_sec:.1f} files/sec "
                      f"| ETA: {eta_seconds:.0f}s", end='', flush=True)
    
    def finish(self):
        """Complete progress tracking and show final stats"""
        total_time = time.perf_counter() - self.start_time
        avg_speed = self.completed_files / total_time if total_time > 0 else 0
        
        print(f"\n‚úÖ Processing complete!")
        print(f"   Files processed: {self.completed_files}")
        print(f"   Total time: {total_time:.2f}s")
        print(f"   Average speed: {avg_speed:.1f} files/sec")


class EnhancedFusionPipeline:
    """Enhanced pipeline with ThreadPoolExecutor and real-time monitoring"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = get_fusion_logger(__name__)
        
        # Get deployment configuration
        deployment_manager = DeploymentManager(config)
        self.max_workers = deployment_manager.get_max_workers()
        
        # Initialize ThreadPool processor
        self.processor = ServiceProcessorThreadPool(config, self.max_workers)
        
        self.logger.stage(f"üöÄ Enhanced Fusion Pipeline initialized with {self.max_workers} workers")
    
    def find_input_files(self) -> List[Path]:
        """Find all input files based on configuration"""
        files = []
        
        # Process individual files
        for file_path in self.config.get('inputs', {}).get('files', []):
            path = Path(file_path)
            if path.exists() and path.is_file():
                files.append(path)
        
        # Process directories
        for directory in self.config.get('inputs', {}).get('directories', []):
            dir_path = Path(directory)
            if dir_path.exists() and dir_path.is_dir():
                supported_extensions = set(self.config.get('files', {}).get('supported_extensions', []))
                
                # Find all supported files
                for ext in supported_extensions:
                    pattern = f"**/*{ext}"
                    found_files = list(dir_path.glob(pattern))
                    files.extend(found_files)
                    
                    if found_files:
                        self.logger.stage(f"üìÅ Found {len(found_files)} {ext} files in {directory}")
        
        # Remove duplicates and filter by size/type
        unique_files = []
        seen = set()
        max_size_bytes = self.config.get('files', {}).get('max_file_size_mb', 50) * 1024 * 1024
        
        for file_path in files:
            if file_path in seen:
                continue
            seen.add(file_path)
            
            try:
                if file_path.stat().st_size <= max_size_bytes:
                    unique_files.append(file_path)
                else:
                    self.logger.logger.warning(f"‚ö†Ô∏è Skipping large file: {file_path.name}")
            except OSError:
                self.logger.logger.warning(f"‚ö†Ô∏è Cannot access file: {file_path}")
        
        return unique_files
    
    def process_batch(self, files: List[Path], batch_size: int = None) -> tuple[List, float]:
        """Process files in optimized batches"""
        if not files:
            return [], 0.0
        
        # Use configured batch size or optimize based on worker count
        if batch_size is None:
            batch_size = max(len(files) // self.max_workers, 1)
            batch_size = min(batch_size, 50)  # Cap at 50 files per batch
        
        self.logger.stage(f"üì¶ Processing {len(files)} files in batches of {batch_size}")
        
        # Setup output directory
        output_dir = Path(self.config.get('output', {}).get('base_directory', '../output/enhanced_fusion'))
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup progress tracking
        progress = ProgressTracker(len(files))
        
        all_documents = []
        total_start = time.perf_counter()
        
        # Process files in batches to avoid overwhelming the system
        for i in range(0, len(files), batch_size):
            batch = files[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            total_batches = (len(files) + batch_size - 1) // batch_size
            
            self.logger.stage(f"üîÑ Processing batch {batch_num}/{total_batches} ({len(batch)} files)")
            
            try:
                # Process batch
                documents, batch_time = self.processor.process_files_service(batch, output_dir)
                all_documents.extend(documents)
                
                # Update progress
                progress.update(len(batch))
                
                # Log batch completion
                if batch_time > 0:
                    batch_speed = len(batch) / batch_time
                    self.logger.stage(f"‚úÖ Batch {batch_num} complete: {len(documents)} files in {batch_time:.2f}s ({batch_speed:.1f} files/sec)")
                
            except Exception as e:
                self.logger.logger.error(f"‚ùå Batch {batch_num} failed: {e}")
                # Continue with next batch instead of failing completely
                progress.update(len(batch))  # Update progress even for failed batch
        
        total_time = time.perf_counter() - total_start
        progress.finish()
        
        return all_documents, total_time
    
    def process_files(self, batch_size: int = None) -> Dict[str, Any]:
        """Main processing method with comprehensive metrics"""
        start_time = time.perf_counter()
        
        # Find input files
        self.logger.stage("üîç Scanning for input files...")
        files = self.find_input_files()
        
        if not files:
            self.logger.logger.warning("‚ö†Ô∏è No input files found!")
            return {
                'success': False,
                'error': 'No input files found',
                'files_found': 0,
                'files_processed': 0,
                'total_time': time.perf_counter() - start_time
            }
        
        self.logger.stage(f"üìä Found {len(files)} files to process")
        
        # Group files by type for analysis
        file_types = {}
        for file_path in files:
            ext = file_path.suffix.lower()
            file_types[ext] = file_types.get(ext, 0) + 1
        
        type_summary = ", ".join([f"{ext}: {count}" for ext, count in file_types.items()])
        self.logger.stage(f"üìã File types: {type_summary}")
        
        # Process files
        self.logger.stage("üöÄ Starting enhanced processing...")
        documents, processing_time = self.process_batch(files, batch_size)
        
        total_time = time.perf_counter() - start_time
        
        # Generate comprehensive results
        results = {
            'success': True,
            'files_found': len(files),
            'files_processed': len(documents),
            'success_rate': (len(documents) / len(files)) * 100 if files else 0,
            'total_time': total_time,
            'processing_time': processing_time,
            'overhead_time': total_time - processing_time,
            'files_per_sec': len(documents) / total_time if total_time > 0 else 0,
            'avg_time_per_file': (total_time / len(documents)) * 1000 if documents else 0,  # ms
            'file_types': file_types,
            'output_directory': str(Path(self.config.get('output', {}).get('base_directory', '../output/enhanced_fusion')).resolve())
        }
        
        return results


def load_config(config_path: str) -> Dict[str, Any]:
    """Load and validate configuration"""
    config_file = Path(config_path)
    
    if not config_file.exists():
        print(f"‚ùå Configuration file not found: {config_path}")
        sys.exit(1)
    
    try:
        with open(config_file, 'r') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        print(f"‚ùå Failed to load configuration: {e}")
        sys.exit(1)


def print_performance_summary(results: Dict[str, Any]):
    """Print comprehensive performance summary"""
    print("\n" + "=" * 80)
    print("üìä ENHANCED FUSION PIPELINE PERFORMANCE SUMMARY")
    print("=" * 80)
    
    if not results['success']:
        print(f"‚ùå Processing failed: {results.get('error', 'Unknown error')}")
        return
    
    print(f"\nüìà Processing Results:")
    print(f"   Files found: {results['files_found']:,}")
    print(f"   Files processed: {results['files_processed']:,}")
    print(f"   Success rate: {results['success_rate']:.1f}%")
    
    print(f"\n‚è±Ô∏è Timing Results:")
    print(f"   Total time: {results['total_time']:.2f}s")
    print(f"   Processing time: {results['processing_time']:.2f}s")
    print(f"   Overhead time: {results['overhead_time']:.2f}s")
    
    print(f"\nüöÄ Performance Metrics:")
    print(f"   Speed: {results['files_per_sec']:.1f} files/sec")
    print(f"   Average: {results['avg_time_per_file']:.1f}ms per file")
    
    if results['file_types']:
        print(f"\nüìã File Types Processed:")
        for ext, count in results['file_types'].items():
            print(f"   {ext}: {count:,} files")
    
    print(f"\nüìÅ Output Directory:")
    print(f"   {results['output_directory']}")
    
    # Performance comparison
    baseline_time = 105.39  # Original slow time
    if results['total_time'] > 0:
        speedup = baseline_time / results['total_time']
        time_saved = baseline_time - results['total_time']
        
        print(f"\nüèÜ Performance Improvement:")
        if speedup > 1:
            print(f"   Speedup: {speedup:.1f}x faster than baseline!")
            print(f"   Time saved: {time_saved:.1f}s")
        else:
            print(f"   Current time: {results['total_time']:.1f}s")
    
    print("=" * 80)


def signal_handler(signum, frame):
    """Handle Ctrl+C gracefully"""
    print("\n\nüõë Processing interrupted by user")
    print("   Cleaning up...")
    sys.exit(0)


def main():
    """Enhanced main entry point"""
    # Setup signal handling
    signal.signal(signal.SIGINT, signal_handler)
    
    # Parse arguments
    parser = argparse.ArgumentParser(
        description="Enhanced MVP-Fusion CLI with ThreadPool processing",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python fusion_cli_enhanced.py --config config/full.yaml
  python fusion_cli_enhanced.py --config config/full.yaml --workers 4
  python fusion_cli_enhanced.py --config config/full.yaml --batch-size 20
  python fusion_cli_enhanced.py --file document.pdf --workers 2
        """
    )
    
    parser.add_argument('--config', '-c', default='config/full.yaml',
                       help='Configuration file path (default: config/full.yaml)')
    parser.add_argument('--file', '-f', help='Process single file')
    parser.add_argument('--directory', '-d', help='Process directory')
    parser.add_argument('--workers', '-w', type=int, help='Number of worker threads')
    parser.add_argument('--batch-size', '-b', type=int, help='Batch size for processing')
    parser.add_argument('--output', '-o', help='Output directory')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose logging')
    parser.add_argument('--performance-test', action='store_true', help='Run performance test')
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(
        verbosity=2 if args.verbose else 1,
        use_colors=True
    )
    
    logger = get_fusion_logger(__name__)
    
    print("üöÄ Enhanced MVP-Fusion CLI")
    print("=" * 40)
    
    # Load configuration
    config = load_config(args.config)
    
    # Override config with CLI arguments
    if args.file:
        config['inputs'] = {'files': [args.file], 'directories': []}
    elif args.directory:
        config['inputs'] = {'files': [], 'directories': [args.directory]}
    
    if args.workers:
        config.setdefault('deployment', {}).setdefault('profiles', {}).setdefault('local', {})['max_workers'] = args.workers
    
    if args.output:
        config.setdefault('output', {})['base_directory'] = args.output
    
    # Initialize and run pipeline
    try:
        pipeline = EnhancedFusionPipeline(config)
        results = pipeline.process_files(batch_size=args.batch_size)
        
        # Show results
        print_performance_summary(results)
        
        # Exit with appropriate code
        if results['success']:
            print("\n‚úÖ Processing completed successfully!")
            sys.exit(0)
        else:
            print(f"\n‚ùå Processing failed: {results.get('error', 'Unknown error')}")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nüõë Processing interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.logger.error(f"‚ùå Pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()