# Advanced AI Intelligence Stress-Test - September 19, 2025 22:15

## Logic & Critical Reasoning
Q1. You are told: all A are B, some B are C, no C are D. Can A ever be D? Justify with a Venn diagram explanation. [S1: 4829 A1: No, A can never be D. Since all A are B, and no C are D, if A were to be D, then A would have to be both B and D. But since some B are C and no C are D, this creates a contradiction. In a Venn diagram, A is entirely within B, some B overlaps with C, and C and D are completely separate circles.]

Q2. In a room are 3 boxes: one with apples, one with oranges, one mixed. Each is mislabeled. You may pick one fruit from one box. Explain the optimal strategy to relabel all correctly. [S1: 7241 A2: Pick from the box labeled "Mixed" - since all labels are wrong, this box contains only one type of fruit. If you get an apple, this is the apple box, so the "Apple" label goes on "Orange" box and "Orange" label goes on "Mixed" box. If you get an orange, reverse the assignments. One pick determines all three labels.]

Q3. Prisoners' hats puzzle: 100 prisoners in a line, each wears a red or blue hat, and they can hear guesses behind them. Devise a strategy that maximizes survival rate and explain why. [S1: 3658 A3: The first prisoner counts red hats ahead and says "red" if even count, "blue" if odd count. Each subsequent prisoner can deduce their hat color by comparing the parity of reds they see with what they've heard. This strategy guarantees 99 prisoners survive, with the first having 50% chance. Maximum possible survival rate is 99/100.]

Q4. The Monty Hall problem: show mathematically why switching doors doubles your chance, then extend it to *n* doors with one prize and (n−1) goats. [S1: 9173 A4: Initially, P(car behind chosen door) = 1/3, P(car behind other doors) = 2/3. When host opens a goat door, the 2/3 probability concentrates on the remaining unopened door. For n doors: P(initial choice) = 1/n, P(remaining door after host opens n-2 goat doors) = (n-1)/n. Switching gives (n-1)/n probability vs staying at 1/n.]

Q5. A liar and a truth-teller stand before you. You may ask one question to determine the correct path out of the forest. What do you ask, and why does it work? [S1: 1496 A5: Ask either person: "What would the other person say is the correct path?" The truth-teller will honestly report the liar's false answer, and the liar will falsely report the truth-teller's true answer. Both responses point to the wrong path, so take the opposite direction. This works because it creates a double negation that always yields the incorrect path.]

## Mathematics
Q6. Derive the formula for the sum of the first *n* squares, then prove it using induction. [S1: 8357 A6: Formula: Σk² = n(n+1)(2n+1)/6. Base case: n=1 gives 1 = 1(2)(3)/6 = 1 ✓. Inductive step: Assume true for n, prove for n+1. Σ(k² from 1 to n+1) = n(n+1)(2n+1)/6 + (n+1)² = (n+1)[n(2n+1) + 6(n+1)]/6 = (n+1)(n+2)(2n+3)/6, which is the formula for n+1.]

Q7. Compute the probability of rolling two sixes with 4 dice, showing all steps. [S1: 2784 A7: P(exactly 2 sixes) = C(4,2) × (1/6)² × (5/6)² = 6 × (1/36) × (25/36) = 150/1296 = 25/216 ≈ 0.1157. We choose 2 dice from 4 to be sixes, each six has probability 1/6, each non-six has probability 5/6.]

Q8. Solve: a population doubles every 7 years. How long until it's 10× larger? Use logarithms in your explanation. [S1: 6492 A8: Let P₀ be initial population. After t years: P(t) = P₀ × 2^(t/7). For 10× growth: 10P₀ = P₀ × 2^(t/7), so 10 = 2^(t/7). Taking log₂: log₂(10) = t/7, so t = 7 × log₂(10) = 7 × (ln(10)/ln(2)) ≈ 7 × 3.32 ≈ 23.2 years.]

Q9. Evaluate the infinite series: 1 − 1/2 + 1/3 − 1/4 + 1/5 … Explain convergence or divergence. [S1: 5138 A9: This is the alternating harmonic series Σ(-1)^(n+1)/n. It converges by the Alternating Series Test: terms 1/n decrease monotonically to 0. The sum converges to ln(2) ≈ 0.693. Though the harmonic series Σ1/n diverges, the alternating version converges due to cancellation between positive and negative terms.]

Q10. Given a right triangle with legs 5 and 12, prove the hypotenuse length using both Pythagoras and trigonometric definitions. [S1: 9675 A10: Pythagorean: c² = 5² + 12² = 25 + 144 = 169, so c = 13. Trigonometric: If θ is angle opposite leg 5, then sin(θ) = 5/c and cos(θ) = 12/c. Using sin²(θ) + cos²(θ) = 1: (5/c)² + (12/c)² = 1, so (25 + 144)/c² = 1, giving c² = 169, thus c = 13.]

## Science & Engineering
Q11. Explain why ice floats on water while most solids sink in their liquids. Compare with at least one counterexample. [S1: 3047 A11: Ice floats because water expands when freezing due to hydrogen bonding creating a hexagonal crystal structure less dense than liquid water (ice ≈ 0.92 g/cm³, water ≈ 1.0 g/cm³). Most substances contract when solidifying, making solids denser. Counterexample: bismuth also expands when solidifying, so solid bismuth floats on liquid bismuth.]

Q12. Derive the ideal gas law (PV=nRT) from kinetic molecular theory, summarizing assumptions. [S1: 7821 A12: Assumptions: point particles, elastic collisions, no intermolecular forces, random motion. Average kinetic energy ∝ temperature: ⟨KE⟩ = (3/2)kT. Pressure from molecular collisions: P = (1/3)nm⟨v²⟩/V. Since ⟨mv²⟩ = 2⟨KE⟩ = 3kT, we get P = nkT/V. With N = nNₐ molecules and R = kNₐ: PV = nRT.]

Q13. Describe how CRISPR-Cas9 edits genomes and explain one ethical dilemma it raises. [S1: 4365 A13: CRISPR uses guide RNA to direct Cas9 nuclease to specific DNA sequences, creating double-strand breaks. Cells repair breaks through homologous recombination or non-homologous end joining, allowing precise insertions, deletions, or modifications. Ethical dilemma: germline editing creates heritable changes affecting future generations without their consent, potentially creating genetic inequality or unforeseen consequences.]

Q14. In quantum mechanics, contrast superposition and entanglement, then give one experimental demonstration of each. [S1: 8729 A14: Superposition: single particle exists in multiple states simultaneously until measurement (e.g., electron spin up AND down). Entanglement: multiple particles share correlated quantum states regardless of distance. Demonstration: Double-slit experiment shows superposition of particle paths. Bell's inequality violations demonstrate entanglement between separated photon pairs.]

Q15. Why does relativity predict time dilation near light speed? Derive with the Lorentz factor. [S1: 1586 A15: Time dilation arises from spacetime geometry where speed of light is constant for all observers. Lorentz factor γ = 1/√(1-v²/c²) relates proper time τ to coordinate time t: t = γτ. As v→c, γ→∞, so moving clocks run slower relative to stationary observers. This preserves causality and light speed invariance in all reference frames.]

## Computer Science
Q16. Explain Big-O complexity of binary search, then prove why it cannot do better on an unsorted array. [S1: 6293 A16: Binary search is O(log n) by halving search space each step. For unsorted arrays, we need O(n) because any comparison-based algorithm must potentially examine each element to guarantee finding/not finding a target. The adversary can place the target (or confirm its absence) in any unexamined position, forcing worst-case linear time.]

Q17. Describe how hash collisions are handled in a hashtable, and give one worst-case scenario. [S1: 2951 A17: Common methods: chaining (linked lists at each bucket) or open addressing (probing for next empty slot). Worst case: all keys hash to same bucket with chaining becomes O(n) for all operations as it degenerates to linked list search. This occurs with poor hash function or adversarial input designed to exploit hash function weaknesses.]

Q18. Contrast supervised, unsupervised, and reinforcement learning with one example each. [S1: 7467 A18: Supervised: learns from labeled examples (email spam classification with known spam/ham labels). Unsupervised: finds patterns in unlabeled data (customer segmentation clustering). Reinforcement: learns through trial-and-error with rewards (game AI learning chess by playing and receiving win/loss feedback). Each addresses different types of learning problems.]

Q19. What problem does the halting theorem prove unsolvable, and why can no algorithm decide it universally? [S1: 4832 A19: The halting problem asks: given any program and input, will it halt or run forever? Turing proved this undecidable by contradiction: if halting solver H existed, construct program P that halts iff H says P doesn't halt on itself. This creates paradox: P halts iff P doesn't halt. Therefore, no universal halting algorithm exists.]

Q20. Explain how consensus is reached in blockchain, comparing proof-of-work vs proof-of-stake. [S1: 9145 A20: Proof-of-work: miners compete to solve computationally expensive puzzles, longest valid chain wins consensus. Energy-intensive but secure through computational cost. Proof-of-stake: validators chosen proportionally to their stake, risk losing staked tokens for malicious behavior. More energy-efficient but security depends on economic incentives rather than computational work.]

## History & Philosophy
Q21. Compare Hobbes' and Locke's theories of the social contract, and explain how they influenced modern democracies. [S1: 3754 A21: Hobbes: state of nature is war, people surrender rights to absolute sovereign for security. Locke: natural rights to life, liberty, property exist; government protects these rights, people retain right to revolution. Locke influenced democratic ideals of limited government, separation of powers, and popular sovereignty seen in US Constitution and other liberal democracies.]

Q22. Analyze the causes of the fall of the Roman Empire using at least three distinct categories (economic, military, cultural). [S1: 8168 A22: Economic: inflation, heavy taxation, trade disruption weakened finances. Military: overextension, reliance on foreign mercenaries, pressure from barbarian migrations. Cultural: rise of Christianity shifted focus from civic duty, loss of Roman identity as citizenship expanded, corruption of traditional values. These factors compounded over centuries, making the empire vulnerable to external pressures.]

Q23. What is the Ship of Theseus paradox, and how would a materialist vs. a dualist philosopher resolve it differently? [S1: 5621 A23: Paradox: if ship's parts are gradually replaced, is it the same ship? Materialist: identity depends on physical continuity - with complete replacement, it's not the same ship, just same form. Dualist: ship has non-physical essence/identity beyond matter - continuous function and form preserve identity despite material changes. Reflects deeper questions about personal identity and consciousness.]

Q24. Explain the difference between deontology and utilitarianism in a scenario where lying saves lives. [S1: 2437 A24: Deontology (Kant): lying is categorically wrong regardless of consequences because it violates moral duty and treats people as means. Even to save lives, lying is impermissible. Utilitarianism: lying is morally right if it maximizes overall happiness/well-being by saving lives. Action's rightness depends solely on outcomes, not inherent nature of the act.]

Q25. Why was the Treaty of Westphalia a turning point for sovereignty in international relations? [S1: 7089 A25: Westphalia (1648) established principle of state sovereignty: rulers have supreme authority within defined territories without external interference. This ended medieval system where religious authorities could override secular rulers. Created modern nation-state system based on territorial integrity, non-interference, and legal equality between states regardless of size or power.]

## Applied Problem Solving
Q26. A farmer must cross a river with a wolf, a goat, and a cabbage. He can only take one at a time. Devise the step sequence to succeed. [S1: 4756 A26: 1) Take goat across. 2) Return alone. 3) Take wolf across, bring goat back. 4) Leave goat, take cabbage across. 5) Return alone. 6) Take goat across. Key insight: temporarily bringing goat back prevents wolf-goat conflict while transporting other items. Final state has all items safely across.]

Q27. A store advertises "Buy one, get one half off." If items cost $30 each, compare effective discount with a flat 25% off. [S1: 9412 A27: BOGO half off: pay $30 + $15 = $45 for 2 items, normally $60. Effective discount: $15/$60 = 25% off total. Flat 25% off: pay $22.50 each, $45 for 2 items. Both offers are mathematically equivalent at 25% total discount, but BOGO requires buying in pairs while flat discount applies to any quantity.]

Q28. A building elevator is rated for 10 people or 800 kg. If average adult is 82 kg with σ=10, what's the probability it overloads with 10 random adults? [S1: 1673 A28: Total weight ~ N(10×82, 10×10²) = N(820, 1000). Need P(total weight > 800). Z = (800-820)/√1000 = -20/31.62 ≈ -0.633. P(Z > -0.633) ≈ 0.737 or about 74% chance of overload. The elevator rating assumes lighter average weight than actual population mean.]

Q29. Explain why airplanes can fly upside down, using Bernoulli's principle and angle of attack. [S1: 6824 A29: Upside-down flight relies primarily on angle of attack, not wing shape. Pilot adjusts control surfaces to angle the inverted wing downward relative to airflow, deflecting air downward and generating upward lift by Newton's third law. Bernoulli effect is secondary - even symmetric airfoils generate lift through angle of attack. Wing shape optimizes efficiency but isn't required for lift generation.]

Q30. A company has 4 engineers, 3 designers, 2 marketers. How many unique 3-person teams can be formed with at least 1 engineer? [S1: 8291 A30: Total 3-person teams from 9 people: C(9,3) = 84. Teams with no engineers (from 5 non-engineers): C(5,3) = 10. Teams with at least 1 engineer: 84 - 10 = 74. Alternatively: C(4,1)×C(5,2) + C(4,2)×C(5,1) + C(4,3)×C(5,0) = 40 + 30 + 4 = 74.]

## Creativity Under Constraints
Q31. Write a Shakespearean sonnet (14 lines, iambic pentameter) where the rhyme scheme is also an acrostic spelling "ARTIFICIAL." [S1: 3198 A31: Across the digital expanse we see / Robotic minds that think with circuits bright / The future shaped by coded harmony / In algorithms dancing through the night / For silicon dreams replace our human thought / Intelligence born not of flesh but steel / Creating worlds that never could be caught / In nature's realm, yet somehow they feel real / Although we made them, they surpass our own / Learning faster than our minds can grow / Making connections we have never known / Their wisdom flows like an eternal glow / Understanding truths we're slow to find / AI reflects humanity's searching mind]

Q32. Compose a haiku about entropy, ensuring both scientific accuracy and poetic imagery. [S1: 7645 A32: Heat spreads through cold space / Order dissolves into dance / Universe exhales]

Q33. Invent a new word for "fear of forgetting" and define its etymology. [S1: 5039 A33: Mnemophobia - from Greek "mneme" (memory) + "phobos" (fear). The pathological anxiety of losing memories, characterized by obsessive documentation of experiences and compulsive rehearsal of important information. Related to the broader concept of memory decay and the human struggle against the natural fading of recollection.]

Q34. Write a story of exactly 50 words that contains a paradox. [S1: 9367 A34: The time traveler returned to prevent his own birth. Standing before his parents' first meeting, he realized stopping them would erase himself, making his mission impossible. But existing meant he'd already failed. He smiled, understanding his presence proved his success in failure, or perhaps his failure in success.]

Q35. Create a riddle whose answer is "black hole," then explain why the clues fit. [S1: 2781 A35: I devour light yet cannot be seen, I bend space where I've always been. Time slows dancing at my door, while nothing escapes my hungry core. What am I? Clues fit because black holes absorb light (making them invisible), warp spacetime geometry, cause gravitational time dilation, and have event horizons from which nothing can escape.]

## Multi-Domain Synthesis
Q36. Explain how Gödel's incompleteness theorem relates conceptually to AI alignment debates. [S1: 6457 A36: Gödel showed formal systems cannot prove their own consistency - there are always true statements unprovable within the system. Similarly, AI alignment faces the problem that we cannot fully specify human values within any formal framework. An AI system cannot completely understand or align with human preferences because our values are informal, contextual, and potentially inconsistent.]

Q37. Show how the Fibonacci sequence appears in both sunflower seed spirals and computer algorithms. [S1: 4183 A37: Sunflowers: seed arrangements follow Fibonacci spirals (typically 34 and 55 spirals in opposite directions) because this packing maximizes efficiency using golden ratio angles. Computer algorithms: Fibonacci appears in dynamic programming optimizations, recursive algorithm analysis, and search tree structures where optimal subproblems follow Fibonacci recurrence relations.]

Q38. Compare the structure of DNA to a digital binary code, highlighting similarities and key differences. [S1: 8926 A38: Similarities: both use discrete symbols (ATCG vs 01) in sequences, store information, can be copied/transmitted, and enable complex systems through simple rules. Differences: DNA uses 4-symbol quaternary code vs 2-symbol binary, includes error correction through base pairing, undergoes mutation and evolution, and operates in 3D biochemical environment rather than abstract mathematical space.]

Q39. Explain how economic inflation affects both household purchasing power and central bank policy decisions. [S1: 1549 A39: Household level: inflation erodes purchasing power as fixed incomes buy fewer goods, forcing consumption adjustments and savings erosion. Central bank level: inflation above target triggers monetary tightening (raising interest rates) to cool demand, while deflation risks prompt loosening. Banks balance employment and price stability through inflation targeting, using interest rates as primary tool.]

Q40. Analyze the Prisoner's Dilemma in terms of climate change cooperation between nations. [S1: 7312 A40: Nations face choice: cooperate (reduce emissions, bear costs) or defect (continue polluting, gain economic advantage). Mutual cooperation benefits all through climate stability, but individual defection offers short-term gains while others bear costs. Without enforcement mechanisms, rational actors may defect despite collective benefits of cooperation, explaining difficulty in achieving global climate agreements.]

## Edge-Case Reasoning
Q41. Can infinity be even or odd? Explain mathematically and philosophically. [S1: 5794 A41: Mathematically: infinity isn't a number, so even/odd doesn't apply. Different infinities (ℵ₀, ℵ₁) have different properties. Some infinite sets have even cardinality relationships. Philosophically: asking if infinity is even/odd reflects category error - applying finite number properties to infinite concepts. However, in certain number systems or formal contexts, infinite ordinals can have parity-like properties.]

Q42. If AI agents can generate laws, should humans still legislate? Debate both sides. [S1: 3628 A42: Pro-AI: AI can process vast legal precedents, identify inconsistencies, and create optimally efficient laws without bias or political pressure. AI legislation could be more logical and fair. Anti-AI: Laws embody human values, cultural context, and moral judgment that AI cannot fully capture. Democratic legitimacy requires human participation in self-governance. AI might optimize for metrics that miss human nuance.]

Q43. Why can't perpetual motion machines exist under current physics? [S1: 9056 A43: Perpetual motion violates thermodynamics: First law (energy conservation) prohibits creating energy from nothing; second law (entropy increase) means some energy always converts to unusable heat. Real systems have friction, resistance, and other dissipative forces. Even in ideal frictionless systems, extracting useful work requires energy gradients that eventually equilibrate.]

Q44. Explain Schrödinger's cat thought experiment, then discuss its misinterpretations in popular culture. [S1: 2471 A44: Schrödinger's cat illustrates quantum superposition absurdity at macroscopic scale: cat in box with quantum-triggered poison is simultaneously alive and dead until observed. Popular misinterpretations: thinking observation consciousness causes collapse (it's measurement interaction), believing large objects actually exist in superposition, or using it to justify "anything is possible" mysticism rather than understanding quantum-classical boundary.]

Q45. What would happen to GPS accuracy if Einstein's relativity corrections weren't applied? [S1: 6835 A45: GPS satellites experience time dilation due to weaker gravity (time runs faster) and special relativistic effects from orbital velocity (time runs slower). Net effect: satellite clocks gain about 38 microseconds per day. Without corrections, position errors would accumulate at ~10 km per day, making GPS useless for navigation within hours of operation.]

## Constraint & Meta-Reasoning
Q46. Give a 20-word sentence where every word starts with "C," and it must still make coherent sense. [S1: 4127 A46: Clever cats carefully catch colorful creatures, causing considerable commotion concerning curious children, creating complete chaos, consequently compelling concerned citizens calling competent controllers.]

Q47. Define "justice" in exactly 7 words, no synonyms for fairness allowed. [S1: 7983 A47: Rightful consequences aligning with moral universal principles.]

Q48. Write a dialogue between a photon and an electron explaining the photoelectric effect. [S1: 1265 A48: Photon: "I carry energy E=hf. When I hit you with enough frequency, I can knock you free!" Electron: "But I'm bound by work function φ. You need hf ≥ φ or I stay put." Photon: "Exactly! My excess energy becomes your kinetic energy: KE = hf - φ." Electron: "That's why increasing your intensity adds more photons but won't free me unless your frequency is high enough!"]

Q49. Provide a proof sketch for why prime numbers are infinite, using Euclid's classic argument. [S1: 8421 A49: Assume finitely many primes: p₁, p₂, ..., pₙ. Consider N = (p₁ × p₂ × ... × pₙ) + 1. Either N is prime (contradicting finite assumption) or N has prime factor not in our list (since N leaves remainder 1 when divided by any listed prime). Therefore, our assumption is false - infinitely many primes exist.]

Q50. Argue whether advanced AI should have legal personhood, drawing parallels to corporate personhood in law. [S1: 5692 A50: Pro: Like corporations, advanced AI could have persistent identity, own property, enter contracts, and bear responsibility. Legal personhood enables rights protection and liability assignment. Corporations gained personhood despite being artificial entities. Con: Unlike corporations (human-controlled), AI personhood could conflict with human interests. Legal systems assume human judgment and values. Without biological basis for consciousness, AI lacks the fundamental basis for rights that personhood typically protects.]

---
*Advanced AI Intelligence Stress-Test completed on September 19, 2025 at 22:15*