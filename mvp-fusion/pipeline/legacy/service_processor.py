#!/usr/bin/env python3
"""
MVP-Fusion Service Processor
============================
Clean I/O + CPU worker architecture for edge service deployment.

Architecture:
- 1 I/O Worker: Non-blocking ingestion (downloads, file reads, PDF conversion)
- N CPU Workers: Pure compute (entity extraction, classification, semantic analysis)
- Work Queue: Async queue between I/O and CPU layers with backpressure
"""

import time
import yaml
try:
    from knowledge.extractors.fast_regex import FastRegexEngine
    import knowledge.extractors.fast_regex as regex_module
    # Use the existing fast_regex wrapper which handles FLPC + fallback
    _regex_engine = FastRegexEngine()
    # Create re-like interface
    re = _regex_engine
    # Get flags from the module
    re.IGNORECASE = regex_module.IGNORECASE
    re.MULTILINE = regex_module.MULTILINE
    re.DOTALL = regex_module.DOTALL
    FLPC_AVAILABLE = True
except ImportError:
    # Rule #12: NO Python regex fallback - fail fast if FLPC unavailable
    re = None
    FLPC_AVAILABLE = False
    print("‚ö†Ô∏è FLPC not available - Python regex disabled per Rule #12")
import threading
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor
from queue import Queue, Empty, Full
from pathlib import Path
from typing import List, Dict, Any, Union, Optional, Set
from dataclasses import dataclass

from utils.logging_config import get_fusion_logger
from utils.phase_manager import get_phase_manager, set_current_phase, add_files_processed, add_pages_processed, get_phase_performance_report
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))
from in_memory_document import InMemoryDocument, NoAliasDumper, force_flow_style_spans
from metadata.yaml_metadata_engine import YAMLMetadataEngine

# Import real extraction functions
try:
    import fitz  # PyMuPDF for PDF processing
except ImportError:
    fitz = None

# Import intelligent entity extraction
try:
    from knowledge.extractors.standalone_intelligent_extractor import StandaloneIntelligentExtractor
    from knowledge.aho_corasick_engine import AhoCorasickKnowledgeEngine
except ImportError:
    StandaloneIntelligentExtractor = None
    AhoCorasickKnowledgeEngine = None

# Import conservative person entity extractor for validation
try:
    from utils.person_entity_extractor import PersonEntityExtractor
    CONSERVATIVE_PERSON_AVAILABLE = True
except ImportError:
    CONSERVATIVE_PERSON_AVAILABLE = False

# Import entity normalizer for structured data enhancement
try:
    from knowledge.extractors.entity_normalizer import EntityNormalizer
    ENTITY_NORMALIZER_AVAILABLE = True
except ImportError:
    ENTITY_NORMALIZER_AVAILABLE = False


@dataclass
class WorkItem:
    """Work item passed from I/O worker to CPU workers"""
    document: 'InMemoryDocument'  # Pass the actual document object, not just markdown
    metadata: Dict[str, Any]
    ingestion_time: float


class ServiceProcessor:
    """
    Clean I/O + CPU service processor for edge deployment.
    
    Separates I/O-bound ingestion from CPU-bound processing with async queue.
    """
    
    def __init__(self, config: Dict[str, Any], max_workers: int = None):
        self.config = config
        self.logger = get_fusion_logger(__name__)
        self.yaml_engine = YAMLMetadataEngine()  # Initialize YAML metadata engine
        
        # Thread safety for file I/O operations
        import threading
        self._write_lock = threading.Lock()
        
        # PERFORMANCE FIX: Initialize Aho-Corasick systems ONCE, not per document
        self._hybrid_system = None
        
        # Phase-specific loggers for clearer identification
        self.pdf_converter = get_fusion_logger("pdf_converter")
        self.document_classifier = get_fusion_logger("document_classifier") 
        self.core8_extractor = get_fusion_logger("core8_extractor")
        self.entity_extraction = get_fusion_logger("entity_extraction")
        self.entity_normalizer_logger = get_fusion_logger("entity_normalizer")
        self.semantic_analyzer = get_fusion_logger("semantic_analyzer")
        
        # Service coordination loggers
        self.service_coordinator = get_fusion_logger("service_coordinator")
        self.document_processor = get_fusion_logger("document_processor")
        self.file_writer = get_fusion_logger("file_writer")
        self.memory_manager = get_fusion_logger("memory_manager")
        self.queue_manager = get_fusion_logger("queue_manager")
        
        # Initialize phase manager
        self.phase_manager = get_phase_manager()
        
        # PERFORMANCE FIX: Initialize FLPC engine ONCE for high-speed pattern matching
        self.flpc_engine = None
        
        # PERFORMANCE OPTIMIZATION: Pre-build YAML template structure (avoid per-document overhead)
        self.yaml_template = {
            'conversion': {
                'engine': 'mvp-fusion-highspeed',
                'page_count': 0,  # Will be updated per document
                'conversion_time_ms': 0,  # Will be updated per document
                'source_file': '',  # Will be updated per document
                'format': ''  # Will be updated per document
            },
            'content_detection': {},  # Will be updated per document
            'processing': {
                'stage': 'converted',
                'content_length': 0  # Will be updated per document
            }
        }
        
        # Worker configuration - use CLI override if provided, otherwise config
        if max_workers is not None:
            self.cpu_workers = max_workers  # Use CLI override
        else:
            # Fallback to config file
            from utils.deployment_manager import DeploymentManager
            deployment_manager = DeploymentManager(config)
            self.cpu_workers = deployment_manager.get_max_workers()
        self.queue_size = config.get('pipeline', {}).get('queue_size', 100)
        self.memory_limit_mb = config.get('pipeline', {}).get('memory_limit_mb', 100)
        
        # Work queue between I/O and CPU layers
        self.work_queue = Queue(maxsize=self.queue_size)
        
        # Processing state
        self.active = False
        self.io_worker = None
        self.cpu_executor = None
        
        # Initialize real extractors (shared across CPU workers)
        self.aho_corasick_engine = None
        self.semantic_extractor = None
        self.person_extractor = None
        self.pos_gap_discovery = None
        self._initialize_extractors()
        
        self.service_coordinator.staging(f"ThreadPool Processor initialized: {self.cpu_workers} workers")
        set_current_phase('initialization')
        self.phase_manager.log('memory_manager', f"üìã Queue size: {self.queue_size}, Memory limit: {self.memory_limit_mb}MB")
    
    def _initialize_extractors(self):
        """Initialize real entity extractors for CPU workers"""
        if AhoCorasickKnowledgeEngine and StandaloneIntelligentExtractor:
            try:
                # Initialize Aho-Corasick engine for pattern matching (needs knowledge dir path)
                self.aho_corasick_engine = AhoCorasickKnowledgeEngine("knowledge")
                
                # PERFORMANCE FIX: Initialize optimized system ONCE, not per document
                try:
                    from utils.high_performance_entity_metadata import HighPerformanceEntityMetadata
                    self._hybrid_system = HighPerformanceEntityMetadata()
                    self.logger.logger.info("üü¢ High-performance Aho-Corasick entity system initialized")
                except Exception as e:
                    self.logger.logger.warning(f"ÔøΩ Ô∏è High-performance extraction system failed: {e}")
                    self._hybrid_system = None
                
                # CORE-8 ENTITY CORPUS INITIALIZATION
                try:
                    from utils.core8_corpus_loader import Core8CorpusLoader
                    # Initialize in non-verbose mode (only shows summary)
                    corpus_loader = Core8CorpusLoader(verbose=False)
                    # Store the automatons for use in entity extraction
                    self.core8_automatons = corpus_loader.automatons
                    self.core8_corpus_data = corpus_loader.corpus_data
                    self.core8_corpus_loader = corpus_loader  # Store for subcategory lookup
                except Exception as e:
                    self.logger.logger.warning(f"ÔøΩ Ô∏è Core-8 corpus loader failed: {e}")
                    self.core8_automatons = {}
                    self.core8_corpus_data = {}
                
                # WORLD-SCALE PERSON EXTRACTOR: Initialize with comprehensive corpus
                try:
                    from utils.world_scale_person_extractor import WorldScalePersonExtractor
                    # Use actual corpus paths
                    corpus_dir = Path("knowledge/corpus/foundation_data")
                    first_names_path = corpus_dir / "first_names_2025_09_18.txt"
                    last_names_path = corpus_dir / "last_names_2025_09_18.txt"
                    organizations_path = corpus_dir / "organizations_2025_09_18.txt"  # Fixed: use correct filename
                    
                    self.world_scale_person_extractor = WorldScalePersonExtractor(
                        first_names_path=first_names_path,
                        last_names_path=last_names_path, 
                        organizations_path=organizations_path if organizations_path.exists() else None,
                        min_confidence=0.7
                    )
                    self.logger.logger.info("üü¢ World-scale person extractor initialized (AC + FLPC strategy)")
                except Exception as e:
                    self.logger.logger.warning(f"ÔøΩ Ô∏è World-scale person extractor failed: {e}")
                    self.world_scale_person_extractor = None
                
                # PERFORMANCE FIX: Initialize FLPC engine ONCE (14.9x faster than Python regex)
                try:
                    from fusion.flpc_engine import FLPCEngine
                    self.flpc_engine = FLPCEngine(self.config)
                    self.logger.logger.info("üü¢ FLPC Rust engine initialized (69M+ chars/sec)")
                except Exception as e:
                    self.logger.logger.warning(f"ÔøΩ Ô∏è FLPC engine failed to initialize: {e}")
                    self.flpc_engine = None
                
                # Initialize standalone intelligent fact extractor for meaningful knowledge extraction
                self.semantic_extractor = StandaloneIntelligentExtractor()
                
                # Initialize conservative person entity extractor for validation
                if CONSERVATIVE_PERSON_AVAILABLE:
                    # Point to the actual corpus files with hundreds of thousands of names
                    corpus_dir = Path("knowledge/corpus/foundation_data")
                    self.person_extractor = PersonEntityExtractor(
                        first_names_path=corpus_dir / "first_names_2025_09_18.txt",
                        last_names_path=corpus_dir / "last_names_2025_09_18.txt", 
                        organizations_path=corpus_dir / "organizations_2025_09_18.txt",
                        min_confidence=0.5  # Lowered from 0.7 to catch legitimate names
                    )
                    set_current_phase('initialization')
                    self.phase_manager.log('core8_extractor', "‚úÖ Conservative person extractor initialized with 429K first names, 99K last names, 139K orgs")
                
                # Load GPE corpus for geopolitical entity recognition
                self.gpe_corpus = self._load_gpe_corpus()
                
                # Initialize entity normalizer for structured data enhancement
                if ENTITY_NORMALIZER_AVAILABLE:
                    self.entity_normalizer = EntityNormalizer(self.config)
                    set_current_phase('initialization')
                    self.phase_manager.log('core8_extractor', "‚úÖ Entity normalizer initialized for structured data enhancement and normalization phase")
                else:
                    self.entity_normalizer = None
                
                # Initialize POS gap discovery (optional feature)
                try:
                    from knowledge.extractors.pos_gap_discovery import POSGapDiscovery
                    self.pos_gap_discovery = POSGapDiscovery(self.config)
                    if self.pos_gap_discovery.is_enabled():
                        self.phase_manager.log('core8_extractor', "‚úÖ POS Gap Discovery initialized and enabled")
                    else:
                        self.phase_manager.log('core8_extractor', "‚ÑπÔ∏è  POS Gap Discovery disabled in configuration")
                except Exception as e:
                    self.logger.logger.warning(f"ÔøΩ Ô∏è POS Gap Discovery initialization failed: {e}")
                    self.pos_gap_discovery = None
                
                # Initialize tentative corpus manager for auto-learning
                try:
                    from knowledge.corpus.tentative_corpus_manager import TentativeCorpusManager
                    self.tentative_corpus_manager = TentativeCorpusManager(
                        corpus_dir=Path("knowledge/corpus/foundation_data"),
                        config=self.config
                    )
                    if self.tentative_corpus_manager.is_enabled():
                        self.phase_manager.log('core8_extractor', "‚úÖ Tentative corpus auto-learning initialized")
                    else:
                        self.phase_manager.log('core8_extractor', "‚ÑπÔ∏è  Tentative corpus auto-learning disabled")
                except Exception as e:
                    self.logger.logger.warning(f"ÔøΩ Ô∏è Tentative corpus manager initialization failed: {e}")
                    self.tentative_corpus_manager = None
                
                set_current_phase('initialization')
                self.phase_manager.log('core8_extractor', "‚úÖ Intelligent fact extractors initialized (meaningful relationships, actionable facts)")
            except Exception as e:
                self.logger.logger.warning(f"ÔøΩ Ô∏è  Failed to initialize extractors: {e}")
                self.aho_corasick_engine = None
                self.semantic_extractor = None
                self.person_extractor = None
                self.entity_normalizer = None
        else:
            self.logger.logger.warning("ÔøΩ Ô∏è  Real extractors not available - using mock processing")
            self.entity_normalizer = None
    
    def start_service(self):
        """Start the I/O + CPU worker service"""
        if self.active:
            self.logger.logger.warning("ÔøΩ Ô∏è  Service already running")
            return
        
        self.active = True
        # Start CPU worker pool
        self.cpu_executor = ThreadPoolExecutor(
            max_workers=self.cpu_workers, 
            thread_name_prefix="CPUWorker"
        )
        
        self.service_coordinator.staging(f"ThreadPool service ready: {self.cpu_workers} workers")
    
    def stop_service(self):
        """Stop the I/O + CPU worker service"""
        if not self.active:
            return
        
        self.service_coordinator.staging("Stopping I/O + CPU service...")
        self.active = False
        
        # Stop CPU workers
        if self.cpu_executor:
            self.cpu_executor.shutdown(wait=True)
            self.cpu_executor = None
        
        self.service_coordinator.staging("Service stopped")
    
    def _extract_enhanced_gpe_loc(self, content: str) -> Dict[str, List]:
        """High-performance GPE/LOC extraction using single-pass Aho-Corasick - USES PRE-INITIALIZED SYSTEM"""
        try:
            # PERFORMANCE FIX: Use pre-initialized optimized system
            if not self._hybrid_system:
                self.logger.logger.warning("ÔøΩ Ô∏è High-performance system not initialized, falling back to basic extraction")
                return {'gpe': [], 'location': []}
            
            # Single-pass Aho-Corasick extraction (1,656x faster than O(n¬≤) loops)
            result = self._hybrid_system.extract_entities_fast(content)
            
            # Results are already in the correct format
            return {
                'gpe': result.get('gpe', []),
                'location': result.get('location', [])
            }
            
        except Exception as e:
            self.logger.logger.warning(f"High-performance GPE/LOC extraction failed: {e}")
            return {'gpe': [], 'location': []}
    
    def _convert_flpc_entities(self, flpc_matches: list, entity_type: str) -> list:
        """Convert FLPC match results to service processor entity format"""
        entities = []
        for match in flpc_matches:
            if isinstance(match, dict):
                # FLPC returns structured match data
                entity = {
                    'value': match.get('text', ''),
                    'text': match.get('text', ''),
                    'type': entity_type,
                    'span': {
                        'start': match.get('start', 0),
                        'end': match.get('end', 0)
                    }
                }
                entities.append(entity)
            elif isinstance(match, str):
                # Simple string match
                entity = {
                    'value': match,
                    'text': match,
                    'type': entity_type,
                    'span': {'start': 0, 'end': len(match)}
                }
                entities.append(entity)
        return entities
    
    def _filter_parenthetical_measurements(self, entities: List[Dict], content: str) -> List[Dict]:
        """
        Filter out measurements that appear within parentheses to preserve ONLY original units.
        
        This prevents tagging of converted units like (1.8 meters) when the original 
        is **6 feet**, ensuring we only tag user's original measurement units.
        
        Args:
            entities: List of measurement entities from FLPC
            content: Original content to check for parenthetical context
            
        Returns:
            Filtered list with parenthetical measurements removed
        """
        filtered_entities = []
        
        for entity in entities:
            measurement_text = entity.get('text', '')
            
            # Find all occurrences of this measurement in the content
            import re
            # Escape special regex characters in measurement text
            escaped_text = re.escape(measurement_text)
            
            # Find all matches of this measurement in content
            matches = list(re.finditer(escaped_text, content, re.IGNORECASE))
            
            # Check each occurrence to see if any are NOT in parentheses
            has_non_parenthetical_occurrence = False
            
            for match in matches:
                start_pos = match.start()
                end_pos = match.end()
                
                # Check context around this specific occurrence
                context_start = max(0, start_pos - 50)
                context_end = min(len(content), end_pos + 50)
                
                # Get text before and after this occurrence
                before_text = content[context_start:start_pos]
                after_text = content[end_pos:context_end]
                
                # Look for parentheses pattern
                last_open_paren = before_text.rfind('(')
                first_close_paren = after_text.find(')')
                
                # Check if this occurrence is within parentheses
                is_in_parentheses = False
                if last_open_paren != -1 and first_close_paren != -1:
                    # Make sure there's no closing paren between opening paren and measurement
                    between_open_and_measurement = before_text[last_open_paren:]
                    if ')' not in between_open_and_measurement:
                        is_in_parentheses = True
                        self.logger.logger.debug(f"Found parenthetical occurrence: '{measurement_text}' in '{before_text[last_open_paren:]}...{after_text[:first_close_paren+1]}'")
                
                # If this occurrence is NOT in parentheses, keep the entity
                if not is_in_parentheses:
                    has_non_parenthetical_occurrence = True
                    self.logger.logger.debug(f"Found non-parenthetical occurrence: '{measurement_text}'")
                    break
            
            # Only keep entities that have at least one non-parenthetical occurrence
            if has_non_parenthetical_occurrence:
                filtered_entities.append(entity)
            else:
                self.logger.logger.debug(f"Filtering out measurement '{measurement_text}' - only found in parentheses")
        
        return filtered_entities
    
    def _deduplicate_range_measurements(self, entities: List[Dict], content: str) -> List[Dict]:
        """
        Deduplicate overlapping measurements to prefer range matches over individual matches.
        
        For example, if we detect both "30-37 inches" and "37 inches" from the same text,
        we keep only "30-37 inches" as it's the more complete measurement.
        
        Args:
            entities: List of measurement entities from FLPC
            content: Original content for text analysis
            
        Returns:
            Deduplicated list preferring longer/more complete measurements
        """
        if not entities:
            return entities
        
        # Group entities by whether they are ranges or individual measurements
        range_entities = []
        individual_entities = []
        
        for entity in entities:
            entity_text = entity.get('text', '')
            entity_type = entity.get('type', '')
            
            # Check if this is a range entity (contains range indicators or is from range pattern)
            is_range = False
            
            # Check if it's from a range pattern type
            if 'RANGE' in entity_type:
                is_range = True
            # Check if it contains range indicators
            elif any(indicator in entity_text for indicator in ['-', '‚Äì', '‚Äî', 'to', 'through']):
                is_range = True
            
            if is_range:
                range_entities.append(entity)
            else:
                individual_entities.append(entity)
        
        # For each individual entity, check if it's contained within a range entity
        deduplicated_entities = range_entities.copy()  # Always keep range entities
        
        for individual in individual_entities:
            individual_text = individual.get('text', '')
            
            # Check if this individual measurement is contained within any range measurement
            is_contained_in_range = False
            
            for range_entity in range_entities:
                range_text = range_entity.get('text', '')
                
                # Find all occurrences of both measurements in content
                import re
                escaped_individual = re.escape(individual_text)
                escaped_range = re.escape(range_text)
                
                individual_matches = list(re.finditer(escaped_individual, content, re.IGNORECASE))
                range_matches = list(re.finditer(escaped_range, content, re.IGNORECASE))
                
                # Check if any individual occurrence is contained within a range occurrence
                for ind_match in individual_matches:
                    for range_match in range_matches:
                        # Check if individual measurement is contained within range text
                        if (range_match.start() <= ind_match.start() and 
                            ind_match.end() <= range_match.end()):
                            is_contained_in_range = True
                            self.logger.logger.debug(f"Individual measurement '{individual_text}' contained in range '{range_text}'")
                            break
                    if is_contained_in_range:
                        break
                
                if is_contained_in_range:
                    break
            
            # Only keep individual measurement if it's not contained in any range
            if not is_contained_in_range:
                deduplicated_entities.append(individual)
        
        self.logger.logger.debug(f"Measurement deduplication: {len(entities)} ‚Üí {len(deduplicated_entities)} entities")
        return deduplicated_entities
    
    def _detect_ranges_from_text(self, text: str) -> List[Dict]:
        """
        Post-processing range detection using FLPC-compatible span analysis.
        
        Follows WIP.md approved approach: detect ranges from text spans
        without modifying existing FLPC patterns. Uses FLPC engine for 
        range detection to maintain 14.9x performance advantage.
        
        Args:
            text: The markdown content to analyze for ranges
            
        Returns:
            List of range entities with span information
        """
        range_entities = []
        
        # Use FLPC engine for range detection (Rule #12 compliance)
        if hasattr(self, 'flpc_engine') and self.flpc_engine:
            # Create temporary FLPC config for range patterns
            range_config = {
                'flpc_regex_patterns': {
                    'range_entities': {
                        'measurement_range': {
                            'pattern': r'(?i)\d+(?:\.\d+)?\s*[-‚Äì‚Äî]\s*\d+(?:\.\d+)?\s+(?:inches?|cm|mm|meters?|metres?|feet|ft|yards?|miles?)',
                            'description': 'Measurement ranges with units',
                            'priority': 'high'
                        },
                        'money_range': {
                            'pattern': r'(?i)(?:\$\d+(?:\.\d+)?\s*[-‚Äì‚Äî]\s*\$?\d+(?:\.\d+)?(?:\s*(?:million|billion|thousand|M|B|K))?)|(?:\d+(?:\.\d+)?\s*[-‚Äì‚Äî]\s*\d+(?:\.\d+)?\s+(?:million|billion|thousand)\s+(?:dollars?|USD|EUR))',
                            'description': 'Money ranges with currency',
                            'priority': 'high'
                        },
                        'percent_range': {
                            'pattern': r'(?i)\d+(?:\.\d+)?\s*[-‚Äì‚Äî]\s*\d+(?:\.\d+)?\s*%',
                            'description': 'Percentage ranges',
                            'priority': 'high'
                        }
                    }
                }
            }
            
            # Initialize temporary FLPC engine for range detection
            try:
                from fusion.flpc_engine import FLPCEngine
                range_flpc = FLPCEngine(range_config)
                
                # Extract range entities using FLPC
                flpc_results = range_flpc.extract_entities(text, 'complete')
                range_entities_raw = flpc_results.get('entities', {})
                
                # Convert FLPC results to our format
                for entity_type, entity_matches in range_entities_raw.items():
                    if entity_type in ['MEASUREMENT_RANGE', 'MONEY_RANGE', 'PERCENT_RANGE', 'RANGE']:
                        for match in entity_matches:
                            # Filter out phone numbers and non-measurement ranges
                            if isinstance(match, str):
                                # Skip phone numbers (like 321-6742) - only keep measurement ranges
                                if any(unit in text[text.find(match):text.find(match)+50].lower() for unit in ['inches', 'inch', 'cm', 'mm', 'feet', 'ft', 'meters', 'metres']):
                                    range_entity = {
                                        'value': match,
                                        'text': match,
                                        'type': 'measurement_range',
                                        'span': {
                                            'start': text.find(match),
                                            'end': text.find(match) + len(match)
                                        },
                                        'confidence': 'high',
                                        'detection_method': 'flpc_post_processing'
                                    }
                                    range_entities.append(range_entity)
                            elif isinstance(match, dict):
                                range_entity = {
                                    'value': match.get('text', ''),
                                    'text': match.get('text', ''),
                                    'type': entity_type,
                                    'span': {
                                        'start': match.get('start', 0),
                                        'end': match.get('end', 0)
                                    },
                                    'confidence': 'high',
                                    'detection_method': 'flpc_post_processing'
                                }
                                range_entities.append(range_entity)
                
            except Exception as e:
                # Fallback: Simple string-based detection for critical ranges
                # Only used if FLPC fails, to ensure functionality
                range_entities = self._fallback_range_detection(text)
        else:
            # Fallback if FLPC not available
            range_entities = self._fallback_range_detection(text)
        
        return range_entities
    
    def _fallback_range_detection(self, text: str) -> List[Dict]:
        """
        Fallback range detection using simple string analysis.
        Only used when FLPC engine is not available or fails.
        
        Args:
            text: The markdown content to analyze
            
        Returns:
            List of range entities detected using string analysis
        """
        range_entities = []
        
        # Simple string-based detection for critical test cases
        # This ensures the WIP.md test cases work even without full FLPC
        
        # Look for measurement ranges like "30-37 inches", "76-94 cm"
        measurement_units = ['inches', 'inch', 'cm', 'mm', 'meters', 'metres', 'feet', 'ft', 'yards', 'miles']
        
        # Split text into words for analysis
        words = text.split()
        for i in range(len(words) - 1):  # Need at least 2 words: number-number unit
            word1 = words[i]
            word2 = words[i + 1] if i + 1 < len(words) else ""
            
            # Clean word1 and word2 of punctuation 
            word1_clean = word1.strip('().,;:!?')
            word2_clean = word2.strip('().,;:!?').lower()
            
            # Check for pattern: number-number unit (including within parentheses)
            if '-' in word1_clean and word2_clean in measurement_units:
                # Try to extract numbers from word1_clean
                parts = word1_clean.split('-')
                if len(parts) == 2:
                    try:
                        start_num = float(parts[0])
                        end_num = float(parts[1])
                        
                        # Create range entity
                        full_text = f"{word1_clean} {word2_clean}"
                        
                        # Find position in original text (approximate)
                        start_pos = text.find(full_text)
                        if start_pos >= 0:
                            range_entity = {
                                'value': full_text,
                                'text': full_text,
                                'type': 'measurement_range',
                                'span': {
                                    'start': start_pos,
                                    'end': start_pos + len(full_text)
                                },
                                'range_components': {
                                    'start_value': str(start_num),
                                    'end_value': str(end_num),
                                    'unit': word2_clean
                                },
                                'confidence': 'medium',  # Fallback has medium confidence
                                'detection_method': 'fallback_string_analysis'
                            }
                            range_entities.append(range_entity)
                    
                    except ValueError:
                        # Not valid numbers, skip
                        continue
        
        return range_entities
    
    def _deduplicate_overlapping_ranges(self, ranges: List[Dict]) -> List[Dict]:
        """
        Remove overlapping range detections, preferring longer/more specific matches.
        
        Args:
            ranges: List of range entities with span information
            
        Returns:
            Deduplicated list of range entities
        """
        if not ranges:
            return ranges
        
        # Sort by start position, then by length (longest first)
        sorted_ranges = sorted(ranges, key=lambda r: (r['span']['start'], -(r['span']['end'] - r['span']['start'])))
        
        deduplicated = []
        for current_range in sorted_ranges:
            current_start = current_range['span']['start']
            current_end = current_range['span']['end']
            
            # Check for overlap with already accepted ranges
            overlaps = False
            for accepted_range in deduplicated:
                accepted_start = accepted_range['span']['start']
                accepted_end = accepted_range['span']['end']
                
                # Check if ranges overlap
                if (current_start < accepted_end and current_end > accepted_start):
                    overlaps = True
                    break
            
            if not overlaps:
                deduplicated.append(current_range)
        
        return deduplicated
    
    def _extract_universal_entities(self, content: str) -> Dict[str, List]:
        """
        Extract Core 8 Universal Entities with span information and clean text.
        Core 8: PERSON, ORG, LOC, GPE, DATE, TIME, MONEY, PERCENT
        Additional: phone, email, url, regulations, measurements
        """
        import time
        entities = {}
        timing_breakdown = {}
        
        # PERFORMANCE OPTIMIZATION: Truncate very long documents to prevent extraction slowdown
        MAX_CONTENT_LENGTH = 50000  # 50KB limit for entity extraction
        if len(content) > MAX_CONTENT_LENGTH:
            content = content[:MAX_CONTENT_LENGTH] + "... [truncated for performance]"
        
        # TEXT PREPROCESSING: Clean formatting corruption from HTML-to-markdown conversion
        # This prevents dual entity detection where corrupted text appears as PERSON entities
        # while clean versions appear as GPE/location entities
        content = self._clean_text_formatting(content)
        
        # PERSON - World-scale AC + FLPC strategy (NO Python regex)
        person_start = time.perf_counter()
        # PERSON EXTRACTION FIX: Use conservative extractor directly since world-scale has quality issues
        if self.person_extractor:
            try:
                set_current_phase('entity_extraction')
                conservative_persons = self.person_extractor.extract_persons(content)
                self.phase_manager.log('core8_extractor', f"üéØ Conservative person extractor found {len(conservative_persons)} validated persons")
                
                entities['person'] = []
                for person in conservative_persons:
                    # Standard NER format with clean, consistent structure
                    raw_text = person.get('text', person.get('name', ''))
                    clean_text = self._clean_entity_text(raw_text)
                    entity = {
                        'value': clean_text,
                        'text': clean_text,
                        'type': 'PERSON',
                        'span': {
                            'start': person.get('position', 0),
                            'end': person.get('position', 0) + len(raw_text)  # Use original length for span
                        }
                    }
                    entities['person'].append(entity)
                
                # Limit to 30 persons (increased to capture key people)
                entities['person'] = entities['person'][:30]
                
            except Exception as e:
                self.logger.logger.warning(f"Conservative person extraction failed: {e}")
                entities['person'] = []
        else:
            self.logger.logger.warning("Conservative PersonEntityExtractor not available")
            entities['person'] = []
        
        timing_breakdown['person'] = (time.perf_counter() - person_start) * 1000
        
        # ORGANIZATION - Using sentence-based processing with Core-8 Aho-Corasick automaton
        org_start = time.perf_counter()
        entities['org'] = []
        if hasattr(self, 'core8_automatons') and 'ORG' in self.core8_automatons:
            try:
                entities['org'] = self._extract_entities_sentence_based(content, 'ORG')
                self.logger.logger.info(f"üè¢ ORG extraction (sentence-based): {len(entities['org'])} entities found")
            except Exception as e:
                self.logger.logger.warning(f"ORG extraction error: {e}")
                entities['org'] = []
        else:
            # Fallback to pattern-based extraction if automaton not available
            org_patterns = [
                r'\b[A-Z][A-Za-z]*(?:\s+[A-Z][A-Za-z]*)*\s+(?:Inc|Corp|LLC|Ltd|Company|Organization|Institute|University|College|Department|Agency|Administration|Commission|Bureau|Office)\b',
                r'\b(?:OSHA|FDA|EPA|NASA|FBI|CIA|DOD|DOJ|USDA|CDC|NIH|NSF|NIST|NOAA|FAA|FCC|SEC|IRS|ATF|DEA|DHS|TSA|FEMA)\b'
            ]
            org_entities = []
            for pattern in org_patterns:
                org_entities.extend(self._extract_entities_with_spans(content, pattern, 'ORG'))
            entities['org'] = self._deduplicate_entities(org_entities)[:50]
        timing_breakdown['org'] = (time.perf_counter() - org_start) * 1000
        
        # ENHANCED LOCATION & GPE EXTRACTION with subcategory metadata - TEMPORARILY DISABLED FOR PERFORMANCE TESTING
        # try:
        #     if self._hybrid_system:
        #         self.logger.logger.info("üîß Using enhanced GPE/LOC extraction...")
        #         enhanced_entities = self._extract_enhanced_gpe_loc(content)
        #         entities['location'] = enhanced_entities.get('location', [])
        #         entities['gpe'] = enhanced_entities.get('gpe', [])
        #         
        #         # Log enhancement success
        #         loc_count = len(entities['location'])
        #         gpe_count = len(entities['gpe'])
        #         self.logger.logger.info(f"üü¢ Enhanced extraction SUCCESS: GPE={gpe_count}, LOC={loc_count}")
        #         
        #         # Debug: Show first few entities with subcategories
        #         if entities['gpe']:
        #             sample_gpe = entities['gpe'][0]
        #             subcategory = sample_gpe.get('subcategory', 'NO_SUBCATEGORY')
        #             self.logger.logger.info(f"üîç GPE sample: {sample_gpe['value']} [{subcategory}]")
        #     else:
        #         # No enhanced system - use empty entities
        #         entities['location'] = []
        #         entities['gpe'] = []
        # except Exception as e:
        #     self.logger.logger.warning(f"‚ùå Enhanced extraction failed, using fallback: {e}")
        #     import traceback
        #     self.logger.logger.warning(f"‚ùå Traceback: {traceback.format_exc()}")
        #     
        #     # FALLBACK: Use empty entities instead of slow regex
        #     entities['location'] = []
        #     entities['gpe'] = []
        
        # GPE & LOCATION - Using Core-8 Aho-Corasick automatons
        loc_gpe_start = time.perf_counter()
        
        # Extract GPE entities using Core-8 automaton
        entities['gpe'] = []
        if hasattr(self, 'core8_automatons') and 'GPE' in self.core8_automatons:
            try:
                gpe_automaton = self.core8_automatons['GPE']
                for end_pos, (entity_type, canonical) in gpe_automaton.iter(content.lower()):
                    start_pos = end_pos - len(canonical) + 1
                    # Get original text from content
                    original_text = content[start_pos:end_pos + 1]
                    
                    # Only add if it's a reasonable match (not single letters, etc.)
                    if len(original_text) > 2:
                        # Get subcategory metadata
                        subcategory = None
                        if hasattr(self, 'core8_corpus_loader'):
                            subcategory = self.core8_corpus_loader.get_subcategory('GPE', original_text)
                        
                        clean_text = self._clean_entity_text(original_text)
                        entity = {
                            'value': clean_text,
                            'text': clean_text,
                            'type': 'GPE',
                            'span': {
                                'start': start_pos,
                                'end': end_pos + 1
                            }
                        }
                        
                        # Add subcategory metadata if available
                        if subcategory:
                            entity['metadata'] = {'subcategory': subcategory}
                        
                        entities['gpe'].append(entity)
                
                # Deduplicate and limit (increased from 20 to 50 to capture basic geography)
                entities['gpe'] = self._deduplicate_entities(entities['gpe'])[:50]
                self.logger.logger.info(f"üåç GPE extraction: {len(entities['gpe'])} entities found")
            except Exception as e:
                self.logger.logger.warning(f"GPE extraction error: {e}")
                entities['gpe'] = []
        else:
            entities['gpe'] = []
        
        # Extract LOC entities using Core-8 automaton
        entities['location'] = []
        if hasattr(self, 'core8_automatons') and 'LOC' in self.core8_automatons:
            try:
                loc_automaton = self.core8_automatons['LOC']
                for end_pos, (entity_type, canonical) in loc_automaton.iter(content.lower()):
                    start_pos = end_pos - len(canonical) + 1
                    # Get original text from content
                    original_text = content[start_pos:end_pos + 1]
                    
                    # Only add if it's a reasonable match (not single letters, etc.)
                    if len(original_text) > 2:
                        # Get subcategory metadata
                        subcategory = None
                        if hasattr(self, 'core8_corpus_loader'):
                            subcategory = self.core8_corpus_loader.get_subcategory('LOC', original_text)
                        
                        clean_text = self._clean_entity_text(original_text)
                        entity = {
                            'value': clean_text,
                            'text': clean_text,
                            'type': 'LOC',
                            'span': {
                                'start': start_pos,
                                'end': end_pos + 1
                            }
                        }
                        
                        # Add subcategory metadata if available
                        if subcategory:
                            entity['metadata'] = {'subcategory': subcategory}
                        
                        entities['location'].append(entity)
                
                # Deduplicate and limit (increased from 20 to 50 to capture basic geography)  
                entities['location'] = self._deduplicate_entities(entities['location'])[:50]
                self.logger.logger.info(f"üìç LOC extraction: {len(entities['location'])} entities found")
            except Exception as e:
                self.logger.logger.warning(f"LOC extraction error: {e}")
                entities['location'] = []
        else:
            entities['location'] = []
        
        timing_breakdown['loc_gpe'] = (time.perf_counter() - loc_gpe_start) * 1000
        
        # PERFORMANCE FIX: Replace Python regex violations with FLPC (14.9x faster)
        flpc_start = time.perf_counter()
        flpc_entities = {}
        if self.flpc_engine:
            try:
                # Single FLPC pass for all universal entities (dates, money, time, measurements)
                flpc_results = self.flpc_engine.extract_entities(content, 'complete')
                flpc_entities = flpc_results.get('entities', {})
                
                # Convert FLPC results to service processor format (FLPC uses UPPERCASE keys)
                entities['date'] = self._convert_flpc_entities(flpc_entities.get('DATE', []), 'DATE')[:30]
                entities['time'] = self._convert_flpc_entities(flpc_entities.get('TIME', []), 'TIME')[:10]
                entities['money'] = self._convert_flpc_entities(flpc_entities.get('MONEY', []), 'MONEY')[:40]
                # Combine all measurement subcategories into measurement entity
                measurement_entities = []
                for measurement_type in ['MEASUREMENT_LENGTH', 'MEASUREMENT_WEIGHT', 'MEASUREMENT_TIME', 'MEASUREMENT_TEMPERATURE', 'MEASUREMENT_SOUND', 'MEASUREMENT_PERCENTAGE', 'MEASUREMENT_LENGTH_RANGE', 'MEASUREMENT_TEMPERATURE_RANGE']:
                    measurement_entities.extend(self._convert_flpc_entities(flpc_entities.get(measurement_type, []), measurement_type))
                
                # CRITICAL: Filter out parenthetical measurements to preserve ONLY original units
                # This removes converted units like (1.8 meters) while keeping original **6 feet**
                measurement_entities = self._filter_parenthetical_measurements(measurement_entities, content)
                
                # RANGE OPTIMIZATION: Deduplicate overlapping measurements to prefer range matches
                # This ensures "30-37 inches" is chosen over separate "30 inches" and "37 inches"
                measurement_entities = self._deduplicate_range_measurements(measurement_entities, content)
                entities['measurement'] = measurement_entities[:50]
                
                # Add range indicators for proximity-based range detection
                entities['range_indicator'] = self._convert_flpc_entities(flpc_entities.get('RANGE_INDICATOR', []), 'RANGE_INDICATOR')[:50]
                
                # AC+FLPC HYBRID: Flag entities with range indicators using detected range locations
                entities = self._flag_range_entities(entities, content)
                
                self.logger.logger.info(f"üü¢ FLPC extraction [UPDATED]: DATE={len(entities['date'])}, MONEY={len(entities['money'])}, TIME={len(entities['time'])}, MEASUREMENT={len(entities['measurement'])}")
            except Exception as e:
                self.logger.logger.warning(f"‚ùå FLPC extraction failed: {e}")
                # Fallback to empty entities rather than slow Python regex
                entities['date'] = []
                entities['time'] = []
                entities['money'] = []
                entities['measurement'] = []
                entities['range_indicator'] = []
                flpc_entities = {}
        else:
            # No FLPC available - use empty entities (better than slow Python regex)
            entities['date'] = []
            entities['time'] = []
            entities['money'] = []
            entities['measurement'] = []
        
        # PERCENT handling removed - percentages are just measurements with % unit
        
        # Additional FLPC entities (phone, email, url, measurements) - FLPC uses UPPERCASE keys
        if flpc_entities:
            entities['phone'] = self._convert_flpc_entities(flpc_entities.get('PHONE', []), 'PHONE')[:10]
            entities['email'] = self._convert_flpc_entities(flpc_entities.get('EMAIL', []), 'EMAIL')[:10]
            entities['url'] = self._convert_flpc_entities(flpc_entities.get('URL', []), 'URL')[:10]
            measurement_entities = self._convert_flpc_entities(flpc_entities.get('MEASUREMENT', []), 'MEASUREMENT')[:30]
        else:
            # No FLPC - use empty entities (eliminating slow Python regex)
            entities['phone'] = []
            entities['email'] = []
            entities['url'] = []
            measurement_entities = []
        
        # REGULATIONS - Use FLPC or skip to avoid Rule #12 violations
        if content and ('CFR' in content or 'USC' in content):
            if hasattr(self, 'flpc_engine') and self.flpc_engine:
                # Use FLPC for regulation patterns (faster)
                reg_pattern = r'\b\d+\s+(?:CFR|USC|U\.S\.C\.|C\.F\.R\.)\s+¬ß?\s*\d+(?:\.\d+)?(?:\([a-z]\))?|\b(?:Section|Sec\.)\s+\d+(?:\.\d+)?'
                entities['regulation'] = self._extract_entities_with_spans(content, reg_pattern, 'REGULATION', None)[:30]
            else:
                # Skip regulations to avoid Python regex (Rule #12)
                entities['regulation'] = []
        else:
            entities['regulation'] = []
        
        # All measurements handled by split pattern logic above (line 782)
        timing_breakdown['flpc'] = (time.perf_counter() - flpc_start) * 1000
        
        # POS GAP DISCOVERY: Smart discovery for sentences with no entity hits
        pos_start = time.perf_counter()
        pos_discoveries = 0
        pos_learning_candidates = 0
        
        if self.pos_gap_discovery and self.pos_gap_discovery.is_enabled():
            try:
                # Collect all existing entities for gap analysis
                all_existing_entities = []
                for entity_type, entity_list in entities.items():
                    all_existing_entities.extend(entity_list)
                
                # Discover entities in coverage gaps
                discoveries = self.pos_gap_discovery.discover_entities_in_gaps(content, all_existing_entities)
                pos_discoveries = len(discoveries)
                
                # Convert POS discoveries to standard entity format and add to results
                for discovery in discoveries:
                    entity = {
                        'value': discovery.text,
                        'text': discovery.text,
                        'type': discovery.entity_type,
                        'span': {
                            'start': discovery.start,
                            'end': discovery.end
                        },
                        'metadata': {
                            'source': 'pos_gap_discovery',
                            'confidence': discovery.confidence,
                            'pos_pattern': discovery.pos_pattern
                        }
                    }
                    
                    # Add to appropriate entity type list
                    entity_key = discovery.entity_type.lower()
                    if entity_key == 'org':
                        entity_key = 'org'
                    elif entity_key == 'person':
                        entity_key = 'person'
                    elif entity_key == 'location':
                        entity_key = 'location'
                    
                    if entity_key in entities:
                        entities[entity_key].append(entity)
                    else:
                        entities[entity_key] = [entity]
                
                # Get learning candidates for AC corpus enhancement
                learning_candidates = self.pos_gap_discovery.get_learning_candidates(discoveries)
                pos_learning_candidates = len(learning_candidates)
                
                # TENTATIVE CORPUS AUTO-LEARNING: Feed high-confidence discoveries to tentative corpus
                if self.tentative_corpus_manager and self.tentative_corpus_manager.is_enabled():
                    from knowledge.corpus.tentative_corpus_manager import create_learning_candidate_from_pos_discovery
                    
                    learned_count = 0
                    for pos_discovery in learning_candidates:
                        # Convert POS discovery to learning candidate
                        learning_candidate = create_learning_candidate_from_pos_discovery(
                            pos_discovery, 
                            context=f"Document processing - gap discovery"
                        )
                        
                        # Add to tentative corpus (live automaton updates disabled to prevent breaking AC)
                        success = self.tentative_corpus_manager.add_learning_candidate(
                            learning_candidate,
                            live_automatons=None  # Disabled: requires corpus reload for AC updates
                        )
                        
                        if success:
                            learned_count += 1
                    
                    if learned_count > 0:
                        self.logger.logger.info(f"ÔøΩ  Tentative Learning: Added {learned_count}/{pos_learning_candidates} discoveries to tentative corpus")
                
                if pos_discoveries > 0:
                    self.logger.logger.info(f"üîç POS Gap Discovery: Found {pos_discoveries} new entities ({pos_learning_candidates} learning candidates)")
                
            except Exception as e:
                self.logger.logger.warning(f"ÔøΩ Ô∏è POS Gap Discovery failed: {e}")
        
        timing_breakdown['pos_gap_discovery'] = (time.perf_counter() - pos_start) * 1000
        
        # ENTITY CONFLICT RESOLUTION: Remove overlapping Aho-Corasick entities when FLPC entities take precedence
        # Priority: FLPC (DATE, TIME, MONEY, MEASUREMENT) > Aho-Corasick (PERSON, ORG, LOC, GPE)
        entities = self._resolve_entity_conflicts(entities)
        
        # Log detailed timing breakdown
        total_timing = sum(timing_breakdown.values())
        self.logger.logger.info(f"üïê Core 8 Entity Timing Breakdown ({total_timing:.1f}ms total):")
        for entity_type, timing_ms in timing_breakdown.items():
            percentage = (timing_ms / total_timing * 100) if total_timing > 0 else 0
            self.logger.logger.info(f"   {entity_type}: {timing_ms:.1f}ms ({percentage:.1f}%)")
        
        return entities
    
    def _resolve_entity_conflicts(self, entities: Dict[str, List]) -> Dict[str, List]:
        """
        Resolve conflicts between overlapping entities by prioritizing FLPC over Aho-Corasick.
        Priority: FLPC (DATE, TIME, MONEY, MEASUREMENT) > Aho-Corasick (PERSON, ORG, LOC, GPE)
        
        This prevents cases like "August 15-20, 2024" where:
        - FLPC detects the full date range "August 15-20, 2024"  
        - AC detects "August" as an organization (from corpus)
        - Result should only show the complete date, not both
        
        Uses text-based conflict resolution due to FLPC span reliability issues.
        """
        # FLPC entities have high priority (complex patterns, specific detection)
        high_priority_types = ['date', 'time', 'money', 'measurement']
        # Aho-Corasick entities have lower priority (simple keyword matching)
        low_priority_types = ['person', 'org', 'loc', 'gpe']
        
        # Collect all high-priority entity texts for substring matching
        high_priority_texts = []
        for entity_type in high_priority_types:
            if entity_type in entities:
                for entity in entities[entity_type]:
                    text = entity.get('text', '').strip()
                    if text:
                        high_priority_texts.append({
                            'text': text,
                            'type': entity_type
                        })
        
        # Remove low-priority entities whose text appears within high-priority entity text
        conflicts_removed = 0
        for entity_type in low_priority_types:
            if entity_type in entities:
                original_count = len(entities[entity_type])
                entities[entity_type] = [
                    entity for entity in entities[entity_type]
                    if not self._entity_text_contained_in_priority(entity, high_priority_texts)
                ]
                removed_count = original_count - len(entities[entity_type])
                conflicts_removed += removed_count
                
                if removed_count > 0:
                    self.logger.logger.debug(f"üîß Removed {removed_count} {entity_type} entities due to conflicts")
        
        if conflicts_removed > 0:
            self.logger.logger.info(f"üîß Entity Conflict Resolution: Removed {conflicts_removed} overlapping low-priority entities")
        else:
            self.logger.logger.debug(f"üîß Entity Conflict Resolution: No conflicts detected")
        
        return entities
    
    def _entity_text_contained_in_priority(self, entity: Dict, priority_texts: List[Dict]) -> bool:
        """Check if an entity's text is contained within any priority entity text."""
        entity_text = entity.get('text', '').strip().lower()
        if not entity_text:
            return False
        
        for priority in priority_texts:
            priority_text = priority['text'].strip().lower()
            
            # Check if the entity text appears as a substring in the priority text
            if entity_text in priority_text:
                self.logger.logger.debug(f"üîß Conflict: '{entity_text}' found in priority '{priority_text}' ({priority['type']})")
                return True
        
        return False
    
    def _clean_text_formatting(self, content: str) -> str:
        """
        Clean text formatting corruption from HTML-to-markdown conversion.
        
        This prevents dual entity detection where corrupted text like:
        'Boston\n\n\n      Boston' gets detected as PERSON entity
        while clean 'Boston' gets detected as GPE entity.
        
        Args:
            content: Raw text content potentially with formatting artifacts
            
        Returns:
            Cleaned text with normalized whitespace
        """
        if not content:
            return content
        
        # Replace multiple consecutive newlines with single newlines
        import re
        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)
        
        # Replace excessive whitespace within lines
        content = re.sub(r'[ \t]{3,}', ' ', content)
        
        # Clean up lines that have redundant text separated by whitespace
        # Pattern: "Text\n\n\n    Text" -> "Text"
        lines = content.split('\n')
        cleaned_lines = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                cleaned_lines.append('')
                continue
            
            # Check if this line is a duplicate or near-duplicate of a recent line (within 3 lines)
            is_duplicate = False
            for j in range(max(0, i-3), i):
                if j < len(cleaned_lines):
                    prev_line = cleaned_lines[j].strip()
                    if prev_line and self._is_near_duplicate(prev_line, line):
                        is_duplicate = True
                        break
            
            if not is_duplicate:
                cleaned_lines.append(line)
            else:
                # Skip duplicate line but preserve structure with empty line
                cleaned_lines.append('')
        
        # Rejoin and normalize final whitespace
        cleaned_content = '\n'.join(cleaned_lines)
        
        # Final cleanup: normalize multiple spaces to single space
        cleaned_content = re.sub(r' +', ' ', cleaned_content)
        
        return cleaned_content.strip()
    
    def _is_near_duplicate(self, text1: str, text2: str) -> bool:
        """
        Check if two text strings are near-duplicates (accounting for typos).
        
        This handles cases like 'San Francisco' vs 'San Fransisco' where
        one line has a typo but represents the same entity.
        
        Args:
            text1: First text string
            text2: Second text string
            
        Returns:
            True if strings are near-duplicates, False otherwise
        """
        if not text1 or not text2:
            return False
        
        # Normalize case for comparison
        t1 = text1.lower().strip()
        t2 = text2.lower().strip()
        
        # Exact match
        if t1 == t2:
            return True
        
        # If strings are very different in length, likely not duplicates
        len_diff = abs(len(t1) - len(t2))
        max_len = max(len(t1), len(t2))
        if max_len > 0 and len_diff / max_len > 0.3:  # More than 30% length difference
            return False
        
        # Calculate character similarity using simple distance
        # For strings like "San Francisco" vs "San Fransisco"
        def char_similarity(s1, s2):
            if not s1 or not s2:
                return 0.0
            
            # Count matching characters in order
            matches = 0
            i = j = 0
            while i < len(s1) and j < len(s2):
                if s1[i] == s2[j]:
                    matches += 1
                    i += 1
                    j += 1
                else:
                    # Try advancing either pointer to find next match
                    if i + 1 < len(s1) and s1[i + 1] == s2[j]:
                        i += 1
                    elif j + 1 < len(s2) and s1[i] == s2[j + 1]:
                        j += 1
                    else:
                        i += 1
                        j += 1
            
            return matches / max(len(s1), len(s2))
        
        similarity = char_similarity(t1, t2)
        
        # Consider strings near-duplicates if >85% similar
        return similarity > 0.85
    
    def _clean_entity_text(self, text: str) -> str:
        """
        Clean entity text to remove formatting corruption and normalize whitespace.
        
        This ensures raw entity output doesn't contain corrupted text like:
        'Boston\n\n\n      Boston' or extra symbols/newlines.
        
        Args:
            text: Raw entity text potentially with formatting artifacts
            
        Returns:
            Cleaned entity text suitable for storage and output
        """
        if not text:
            return text
        
        # Remove excessive newlines and whitespace
        import re
        cleaned = re.sub(r'\n+', ' ', text)  # Replace newlines with spaces
        cleaned = re.sub(r'\s+', ' ', cleaned)  # Normalize multiple spaces
        cleaned = cleaned.strip()  # Remove leading/trailing whitespace
        
        # Remove special symbols that shouldn't be in entity text
        cleaned = re.sub(r'[^\w\s\-\.\,\'\"\$‚Ç¨¬£¬•‚Çπ]+', '', cleaned)  # Keep alphanumeric, spaces, basic punctuation, currencies
        
        return cleaned
    
    def _extract_entities_with_spans(self, content: str, pattern: str, entity_type: str, flags: int = 0) -> List[Dict]:
        """Extract entities with span information and clean text."""
        entities = []
        seen = set()
        
        try:
            # Use FLPC regex engine - NO Python regex fallback (Rule #12)
            if hasattr(self, 'flpc_engine') and self.flpc_engine:
                # Use FLPC for high-performance pattern matching
                matches = self.flpc_engine.find_all_matches(pattern, content)
                for match in matches:
                    # Convert FLPC match to regex-like interface
                    raw_text = match.text
                    # Process FLPC match...
                    clean_text = self._clean_entity_text(raw_text)
                    
                    if clean_text not in seen and self._is_valid_entity_text(clean_text):
                        seen.add(clean_text)
                        entities.append({
                            'text': clean_text,
                            'start': match.start,
                            'end': match.end,
                            'entity_type': entity_type.upper(),
                            'confidence': 0.95  # FLPC high confidence
                        })
                return entities[:50]  # Limit results for performance
            else:
                # No FLPC available - return empty to avoid Python regex (Rule #12)
                return []
        except Exception as e:
            # FLPC error - return empty to avoid Python regex fallback (Rule #12)
            self.logger.logger.debug(f"FLPC extraction failed for {entity_type}: {e}")
            return []
        
        return entities
    
    def _clean_entity_text(self, text: str) -> str:
        """Clean entity text by normalizing whitespace and removing line breaks."""
        if not text:
            return ""
        
        # Replace all whitespace (including newlines) with single spaces
        cleaned = ' '.join(text.split()).strip()
        
        # Handle common unicode issues
        cleaned = cleaned.replace('\\u2014', '‚Äî')  # em-dash
        cleaned = cleaned.replace('\\u2013', '‚Äì')  # en-dash
        cleaned = cleaned.replace('\\u2019', "'")  # right single quote
        cleaned = cleaned.replace('\\u201c', '"')  # left double quote
        cleaned = cleaned.replace('\\u201d', '"')  # right double quote
        
        return cleaned
    
    def _is_valid_entity_text(self, text: str) -> bool:
        """Validate entity text to filter out malformed entries."""
        if not text or len(text.strip()) < 2:
            return False
        
        # Filter out entities that are mostly punctuation or whitespace
        alpha_chars = sum(1 for c in text if c.isalnum())
        if alpha_chars < len(text) * 0.3:  # At least 30% alphanumeric
            return False
            
        return True
    
    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """Remove duplicate and overlapping entities, preferring longer matches."""
        if not entities:
            return []
        
        # Sort by length FIRST (longer first), then by position for ties
        sorted_entities = sorted(entities, key=lambda e: (
            -(e.get('span', {}).get('end', 0) - e.get('span', {}).get('start', 0)),  # Length FIRST (longer first)
            e.get('span', {}).get('start', 0)  # Position SECOND (earlier first for ties)
        ))
        
        deduplicated = []
        seen_values = set()
        accepted_spans = []  # Track all accepted spans for overlap checking
        
        for entity in sorted_entities:
            value = entity.get('value', '')
            span = entity.get('span', {})
            start = span.get('start', 0)
            end = span.get('end', 0)
            
            # Skip if we've seen this exact value
            if value in seen_values:
                continue
                
            # Check if this entity overlaps with ANY previously accepted entity
            overlaps = False
            for accepted_start, accepted_end in accepted_spans:
                # True overlap: ranges intersect
                if start < accepted_end and end > accepted_start:
                    overlaps = True
                    break
            
            if overlaps:
                continue
            
            seen_values.add(value)
            deduplicated.append(entity)
            accepted_spans.append((start, end))
        
        return deduplicated
    
    def _extract_entities_sentence_based(self, content: str, entity_type: str) -> List[Dict]:
        """
        Extract entities using sentence-based processing (fast sentence splitting).
        
        This follows the standard NLP approach:
        1. Detect sentence boundaries using simple patterns
        2. Process each sentence independently 
        3. Apply longest-match-first within each sentence
        4. No cross-sentence overlaps possible
        
        Args:
            content: Full document text
            entity_type: Type to extract ('ORG', 'GPE', 'LOC', etc.)
            
        Returns:
            List of entities with proper span information
        """
        all_entities = []
        
        # Step 1: Fast sentence detection using simple patterns (no spaCy needed)
        # Split on sentence boundaries: periods, exclamation marks, question marks
        # Keep track of character positions for span calculation
        sentences = self._fast_sentence_split(content)
        
        # Step 2: Process each sentence independently 
        automaton = self.core8_automatons[entity_type]
        
        for sentence_text, sentence_start in sentences:
            sentence_entities = []
            
            # Extract entities from this sentence with word boundary validation
            for end_pos, (ent_type, canonical) in automaton.iter(sentence_text.lower()):
                start_pos = end_pos - len(canonical) + 1
                original_text = sentence_text[start_pos:end_pos + 1]
                
                # WORD BOUNDARY VALIDATION: Ensure we're matching whole words, not substrings
                # Check that the match is bounded by non-alphanumeric characters or string boundaries
                is_valid_word_boundary = True
                
                # Check character before the match (if exists)
                if start_pos > 0:
                    char_before = sentence_text[start_pos - 1]
                    if char_before.isalnum():  # If previous character is alphanumeric, it's not a word boundary
                        is_valid_word_boundary = False
                
                # Check character after the match (if exists)  
                if end_pos + 1 < len(sentence_text):
                    char_after = sentence_text[end_pos + 1]
                    if char_after.isalnum():  # If next character is alphanumeric, it's not a word boundary
                        is_valid_word_boundary = False
                
                # Skip this match if it's not a valid word boundary
                if not is_valid_word_boundary:
                    continue
                
                # Only add reasonable matches - filter out short ambiguous org names and common words
                common_words = {
                    # Original common words
                    'standard', 'addition', 'current', 'time', 'side', 'express', 'general', 'national', 'international', 'global', 'service', 'services', 'company', 'group', 'systems', 'solutions', 'technologies', 'management', 'development', 'research', 'institute', 'center', 'association', 'corporation', 'foundation',
                    # Common verbs that appear as false organizations
                    'built', 'made', 'used', 'said', 'done', 'come', 'gone', 'seen', 'been', 'have', 'will', 'can', 'may', 'must', 'should', 'would', 'could', 'might',
                    # Common nouns/adjectives/prepositions that appear as false organizations  
                    'place', 'front', 'back', 'here', 'there', 'where', 'when', 'what', 'which', 'that', 'this', 'these', 'those',
                    'above', 'below', 'under', 'over', 'into', 'onto', 'from', 'with', 'without', 'through', 'around', 'about',
                    'good', 'bad', 'new', 'old', 'big', 'small', 'long', 'short', 'high', 'low', 'wide', 'narrow', 'deep', 'shallow',
                    'right', 'left', 'up', 'down', 'in', 'out', 'on', 'off', 'at', 'by', 'for', 'to', 'of', 'and', 'or', 'but',
                    # Common incomplete words/fragments
                    'angl', 'ange', 'orkin', 'ment', 'tion', 'ing', 'ed', 'ly', 'er', 're', 'un', 'pre', 'pro', 'anti', 'non',
                    # Technical/document artifacts
                    'figure', 'table', 'section', 'page', 'line', 'item', 'part', 'step', 'note', 'see', 'ref', 'example',
                    # Common single letters and short fragments that shouldn't be organizations
                    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'be', 'am', 'do', 'did', 'has', 'had', 'get', 'got', 'let', 'set', 'put', 'cut', 'run', 'go', 'no', 'so', 'my', 'me', 'we', 'us', 'he', 'she', 'it', 'they', 'you', 'your', 'our', 'his', 'her', 'its', 'their'
                }
                is_valid = (len(original_text) >= 4 or (entity_type != 'ORG' and len(original_text) > 2))
                is_not_common_word = (entity_type != 'ORG' or original_text.lower() not in common_words)
                
                # Enhanced validation for single-word organizations
                if entity_type == 'ORG' and is_valid and is_not_common_word:
                    is_valid = self._validate_organization_entity(original_text, sentence_text, start_pos)
                
                if is_valid and is_not_common_word:
                    entity = {
                        'value': original_text,
                        'text': original_text,
                        'type': entity_type,
                        'span': {
                            'start': sentence_start + start_pos,  # Adjust to document coordinates
                            'end': sentence_start + end_pos + 1
                        }
                    }
                    sentence_entities.append(entity)
            
            # Step 3: Apply longest-match-first deduplication within sentence
            # This works perfectly because no cross-sentence overlaps are possible
            sentence_entities = self._deduplicate_entities(sentence_entities)
            all_entities.extend(sentence_entities)
        
        return all_entities
    
    def _fast_sentence_split(self, content: str) -> List[tuple]:
        """
        Fast sentence splitting without spaCy dependency.
        
        Returns:
            List of (sentence_text, sentence_start_position) tuples
        """
        sentences = []
        current_start = 0
        
        # Simple sentence boundary patterns (much faster than spaCy)
        # Look for periods, exclamation marks, question marks followed by whitespace/newlines
        i = 0
        while i < len(content):
            char = content[i]
            
            # Check for sentence boundary markers
            if char in '.!?':
                # Look ahead to see if this is likely a sentence boundary
                next_pos = i + 1
                
                # Skip if at end of content
                if next_pos >= len(content):
                    # Add final sentence
                    sentence_text = content[current_start:].strip()
                    if sentence_text:
                        sentences.append((sentence_text, current_start))
                    break
                
                # Check if followed by whitespace (strong sentence boundary indicator)
                next_char = content[next_pos]
                if next_char.isspace():
                    # Extract sentence from current_start to i+1
                    sentence_text = content[current_start:i+1].strip()
                    if sentence_text and len(sentence_text) > 5:  # Skip very short "sentences"
                        sentences.append((sentence_text, current_start))
                    
                    # Move to start of next sentence (skip whitespace)
                    while next_pos < len(content) and content[next_pos].isspace():
                        next_pos += 1
                    current_start = next_pos
                    i = next_pos - 1  # Will be incremented by loop
            
            i += 1
        
        # Add any remaining content as final sentence
        if current_start < len(content):
            sentence_text = content[current_start:].strip()
            if sentence_text and len(sentence_text) > 5:
                sentences.append((sentence_text, current_start))
        
        # If no sentences found, treat entire content as one sentence
        if not sentences and content.strip():
            sentences.append((content.strip(), 0))
        
        return sentences
    
    def _extract_entities_document_level(self, content: str, entity_type: str) -> List[Dict]:
        """Fallback document-level extraction (legacy approach)."""
        entities = []
        automaton = self.core8_automatons[entity_type]
        
        for end_pos, (ent_type, canonical) in automaton.iter(content.lower()):
            start_pos = end_pos - len(canonical) + 1
            original_text = content[start_pos:end_pos + 1]
            
            if len(original_text) > 2:
                entity = {
                    'value': original_text,
                    'text': original_text,
                    'type': entity_type,
                    'span': {
                        'start': start_pos,
                        'end': end_pos + 1
                    }
                }
                entities.append(entity)
        
        return self._deduplicate_entities(entities)[:200]
    
    def _extract_role_from_context(self, context: str) -> Optional[str]:
        """Extract person's role from context (from original comprehensive extractor)"""
        if not context:
            return None
            
        context_lower = context.lower()
        
        # Universal role patterns with transitional connectors
        role_patterns = [
            r'(?:chief|senior|executive|deputy|assistant|associate|junior)\s+\w+(?:\s+\w+){0,2}',
            r'(?:president|ceo|cto|cfo|coo|cmo|ciso|chairman|chairwoman|chairperson)',
            r'(?:director|manager|supervisor|head|lead|coordinator|administrator)(?:\s+of\s+\w+(?:\s+\w+){0,2})?',
            r'(?:vice\s+president|vp|evp|svp)(?:\s+of\s+\w+(?:\s+\w+){0,2})?',
        ]
        
        # Use simple string matching instead of regex to avoid Rule #12 violations
        # Convert regex patterns to simple string searches for performance
        role_keywords = [
            'chief executive officer', 'chief financial officer', 'chief operating officer',
            'vice president', 'senior vice president', 'executive vice president',
            'director', 'senior director', 'assistant director', 'deputy director',
            'manager', 'senior manager', 'project manager', 'program manager',
            'supervisor', 'lead', 'principal', 'senior', 'associate'
        ]
        
        for keyword in role_keywords:
            if keyword in context_lower:
                return keyword
        
        # Simple role words
        simple_roles = ['director', 'manager', 'supervisor', 'inspector', 'worker', 'contractor', 'president', 'ceo']
        for role in simple_roles:
            if role in context_lower:
                return role
                
        return None
    
    def _load_gpe_corpus(self) -> Set[str]:
        """Load geopolitical entities corpus for GPE recognition."""
        gpe_file = Path("knowledge/corpus/foundation_data/geopolitical_entities_2025_09_18.txt")
        gpe_entities = set()
        
        try:
            if gpe_file.exists():
                with open(gpe_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        # Skip comments and empty lines
                        if line and not line.startswith('#'):
                            gpe_entities.add(line)
                set_current_phase('entity_extraction')
                self.phase_manager.log('entity_extraction', f"‚úÖ Loaded {len(gpe_entities)} GPE entities from corpus")
            else:
                self.logger.logger.warning(f"ÔøΩ Ô∏è  GPE corpus file not found: {gpe_file}")
        except Exception as e:
            self.logger.logger.warning(f"ÔøΩ Ô∏è  Failed to load GPE corpus: {e}")
        
        return gpe_entities
    
    def _quick_content_scan(self, content: str) -> Dict[str, bool]:
        """
        Quick pre-screening scan for content types (tables, images, formulas, etc.)
        Fast yes/no flags to inform classification downstream.
        PERFORMANCE: Optimized for minimal string operations and single-pass scanning
        """
        content_lower = content.lower()
        
        # OPTIMIZED: Single split for line-based detection
        lines = None
        if '\n' in content:
            lines = content.split('\n')
        
        # Fast single-pass detection
        has_lists = False
        has_headers = False
        if lines:
            for line in lines:
                stripped = line.strip()
                if not has_lists and stripped and (stripped.startswith(('-', '*', '+', '‚Ä¢')) or 
                                                  (stripped[0].isdigit() and '.' in stripped[:10])):
                    has_lists = True
                if not has_headers and stripped.startswith('#'):
                    has_headers = True
                if has_lists and has_headers:
                    break
        
        return {
            'has_tables': '|' in content and content.count('|') >= 3,
            'has_images': '![' in content or 'image' in content_lower,
            'has_formulas': '$' in content or '`' in content,
            'has_code': '```' in content or 'import ' in content,
            'has_links': 'http' in content_lower,
            'has_lists': has_lists,
            'has_headers': has_headers,
            'has_footnotes': '[' in content and ']' in content,
            'has_citations': 'et al' in content_lower or '(202' in content,
            'has_structured_data': 'json' in content_lower or 'xml' in content_lower
        }
    
    def _convert_pdf_to_markdown(self, pdf_path: Path) -> tuple[str, int]:
        """Convert PDF to markdown using PyMuPDF (real conversion) - returns (content, page_count)"""
        if not fitz:
            return f"# {pdf_path.name}\n\nPyMuPDF not available - mock content", 0
        
        doc = None
        try:
            # Suppress ALL MuPDF stderr warnings during PDF opening and processing
            import contextlib
            import os
            import sys
            
            # Create a context manager to suppress stderr
            with open(os.devnull, 'w') as devnull:
                old_stderr = sys.stderr
                sys.stderr = devnull
                try:
                    # Open PDF with stderr suppressed
                    doc = fitz.open(str(pdf_path))
                    page_count = len(doc)
                finally:
                    # Restore stderr after getting page count
                    sys.stderr = old_stderr
            
            # Skip files that are too large
            if page_count > 100:
                return f"# {pdf_path.name}\n\nSkipped: {page_count} pages (>100 limit)", page_count
            
            # Extract text from all pages
            markdown_content = [f"# {pdf_path.stem}\n"]
            
            for page_num in range(page_count):
                try:
                    # Suppress stderr for each page processing too
                    with open(os.devnull, 'w') as devnull:
                        old_stderr = sys.stderr
                        sys.stderr = devnull
                        try:
                            page = doc[page_num]
                            # Use blocks method for fastest extraction
                            blocks = page.get_text("blocks")
                        finally:
                            sys.stderr = old_stderr
                    if blocks:
                        markdown_content.append(f"\n## Page {page_num + 1}\n")
                        for block in blocks:
                            if len(block) > 4 and block[4]:  # Check if block contains text
                                text = block[4].strip()
                                if text:
                                    markdown_content.append(text + "\n")
                except Exception as page_error:
                    # Skip problematic pages but continue processing silently
                    # MuPDF color space errors are common and non-critical
                    if "color space" not in str(page_error).lower():
                        self.logger.logger.debug(f"Skipping page {page_num + 1} in {pdf_path.name}: {page_error}")
                    continue
            return '\n'.join(markdown_content), page_count
            
        except Exception as e:
            return f"# {pdf_path.name}\n\nPDF conversion error: {str(e)}", 0
        finally:
            # Ensure PDF document is always closed
            if doc:
                try:
                    doc.close()
                except:
                    pass  # Ignore errors during cleanup
    
    def _io_worker_ingestion(self, file_paths: List[Path]) -> None:
        """
        I/O Worker: Handle all I/O-bound operations.
        
        Responsibilities:
        - File reading and validation
        - PDF to markdown conversion  
        - URL downloading
        - Feed work items to CPU queue
        """
        # Set thread name for worker tagging
        threading.current_thread().name = "IOWorker-1"
        
        self.queue_manager.staging(f"I/O worker starting ingestion of {len(file_paths)} files")
        
        for i, file_path in enumerate(file_paths):
            if not self.active:
                break
            
            ingestion_start = time.perf_counter()
            
            try:
                # PERFORMANCE TEST: Disable hot path logging
                # self.logger.conversion(f"Reading file: {file_path.name}")
                
                # Real file processing (I/O bound) - FIXED: No double PDF opening
                page_count = 0
                if file_path.suffix.lower() == '.pdf':
                    # TIMING FIX: Measure only PDF conversion time
                    set_current_phase('pdf_conversion')
                    add_pages_processed(0)  # Will update after we know page count
                    pdf_start = time.perf_counter()
                    markdown_content, page_count = self._convert_pdf_to_markdown(file_path)
                    # Update page count after PDF processing
                    add_pages_processed(page_count)
                else:
                    # Read text files directly
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        markdown_content = f.read()
                    page_count = 0  # Non-PDF files don't have page count
                
                # TIMING FIX: Measure document object creation and metadata generation
                set_current_phase('document_processing') 
                add_files_processed(1)
                if page_count == 0:  # For non-PDF files, add them here
                    add_pages_processed(1)  # Treat text files as 1 page equivalent
                
                # Create InMemoryDocument with basic YAML metadata (I/O worker responsibility)
                doc = InMemoryDocument(
                    source_file_path=str(file_path),
                    memory_limit_mb=self.memory_limit_mb
                )
                doc.markdown_content = markdown_content
                
                # Quick content pre-screening (fast yes/no flags) - FIXED: No regex violations
                content_flags = self._quick_content_scan(markdown_content)
                
                # PERFORMANCE OPTIMIZED: Use pre-built template with minimal updates
                conversion_time = time.perf_counter() - ingestion_start
                
                # Copy template and update only necessary fields (faster than dict creation)
                doc.yaml_frontmatter = self.yaml_template.copy()
                doc.yaml_frontmatter['conversion'] = self.yaml_template['conversion'].copy()
                doc.yaml_frontmatter['processing'] = self.yaml_template['processing'].copy()
                
                # Update only the dynamic values
                doc.yaml_frontmatter['conversion']['page_count'] = page_count
                doc.yaml_frontmatter['conversion']['conversion_time_ms'] = conversion_time * 1000
                doc.yaml_frontmatter['conversion']['source_file'] = file_path.name
                doc.yaml_frontmatter['conversion']['format'] = file_path.suffix.upper().replace('.', '') if file_path.suffix else 'TXT'
                doc.yaml_frontmatter['content_detection'] = content_flags
                doc.yaml_frontmatter['processing']['content_length'] = len(markdown_content)
                # self.phase_manager.log('document_processor', f"üìÑ Created InMemoryDocument with basic YAML: {file_path.name}")
                
                # Create work item for CPU workers (now passes the document)
                work_item = WorkItem(
                    document=doc,
                    metadata={'conversion_time': conversion_time},
                    ingestion_time=time.perf_counter()
                )
                
                # Add to queue with backpressure handling
                try:
                    self.work_queue.put(work_item, timeout=5.0)
                    # queue_size = self.work_queue.qsize()
                    # self.logger.queue(f"Queued InMemoryDocument: {file_path.name}", 
                    #                 queue_size=queue_size, queue_max=self.queue_size)
                except Full:
                    self.logger.logger.warning(f"ÔøΩ Ô∏è  Queue full, dropping: {file_path.name}")
                    
            except Exception as e:
                self.logger.logger.error(f"‚ùå I/O error processing {file_path.name}: {e}")
        
        # Signal completion to CPU workers
        for _ in range(self.cpu_workers):
            self.work_queue.put(None)  # Sentinel value
        
        self.queue_manager.staging(f"I/O worker completed ingestion of {len(file_paths)} files")
    
    def _cpu_worker_processing(self, worker_id: int) -> List[InMemoryDocument]:
        """
        CPU Worker: Handle all CPU-bound operations.
        
        Responsibilities:
        - Entity extraction
        - Document classification
        - Semantic analysis
        - JSON generation
        """
        # Set thread name for worker tagging
        threading.current_thread().name = f"CPUWorker-{worker_id}"
        
        results = []
        set_current_phase('service_coordination')
        self.phase_manager.log('service_coordinator', f"ÔøΩ  CPU worker {worker_id} started")
        
        while self.active:
            try:
                # Get work from I/O queue
                work_item = self.work_queue.get(timeout=1.0)
                
                # Check for completion signal
                if work_item is None:
                    break
                
                # Process work item (CPU bound)
                processing_start = time.perf_counter()
                
                # Get the InMemoryDocument from I/O worker (shared memory handoff)
                doc = work_item.document
                
                # Real entity extraction and classification (enrich existing document)
                try:
                    if self.aho_corasick_engine and self.semantic_extractor:
                        # TIMING FIX: Start classification phase  
                        set_current_phase('classification')
                        doc_page_count = doc.yaml_frontmatter.get('conversion', {}).get('page_count', 0)
                        add_pages_processed(doc_page_count)
                        
                        # Real classification with Aho-Corasick
                        self.document_classifier.classification(f"Classifying document: {doc.source_filename}")
                        
                        # Get domain and document type classifications
                        domain_results = self.aho_corasick_engine.classify_domains(doc.markdown_content)
                        doctype_results = self.aho_corasick_engine.classify_document_types(doc.markdown_content)
                        
                        # Determine primary domain and confidence
                        primary_domain = max(domain_results.keys(), key=lambda k: domain_results[k]) if domain_results else 'general'
                        primary_domain_confidence = domain_results.get(primary_domain, 0)
                        
                        # Determine primary document type and confidence
                        primary_document_type = max(doctype_results.keys(), key=lambda k: doctype_results[k]) if doctype_results else 'document'
                        primary_doctype_confidence = doctype_results.get(primary_document_type, 0)
                        
                        # Domain-based routing decisions
                        routing_decisions = {
                            'skip_entity_extraction': primary_domain_confidence < 5.0,
                            'enable_deep_domain_extraction': primary_domain_confidence >= 60.0,
                            'domain_specialization_route': primary_domain if primary_domain_confidence >= 40.0 else 'general'
                        }
                        
                        # Combine results with primary fields
                        classification_result = {
                            'domains': domain_results,
                            'document_types': doctype_results,
                            'primary_domain': primary_domain,
                            'primary_domain_confidence': primary_domain_confidence,
                            'primary_document_type': primary_document_type,
                            'primary_doctype_confidence': primary_doctype_confidence,
                            'domain_routing': routing_decisions
                        }
                        
                        if classification_result:
                            # Enrich existing YAML frontmatter
                            doc.yaml_frontmatter['classification'] = classification_result
                            # Safely update processing stage
                            if 'processing' not in doc.yaml_frontmatter:
                                doc.yaml_frontmatter['processing'] = {}
                            doc.yaml_frontmatter['processing']['stage'] = 'classified'
                            
                            # No page counting here - already done above
                            self.phase_manager.log('document_classifier', f"üìã Added classification to YAML: {doc.source_filename}")
                            
                            # TIMING FIX: Start entity extraction phase
                            set_current_phase('entity_extraction')
                            # Pages already counted in classification phase
                            
                            # Extract Core 8 Universal Entities
                            self.core8_extractor.enrichment(f"Extracting entities: {doc.source_filename}")
                            entities = self._extract_universal_entities(doc.markdown_content)
                            
                            # NEW: Range Post-Processing (WIP.md approved approach) - CORRECT LOCATION
                            if entities:
                                range_start = time.perf_counter()
                                self.core8_extractor.enrichment(f"üîÑ Range Post-Processing: Detecting ranges from extracted entities")
                                
                                # Detect and consolidate ranges using span analysis
                                try:
                                    range_entities = self._detect_ranges_from_text(doc.markdown_content)
                                    
                                    if range_entities:
                                        # Add range entities to the measurement category for normalization
                                        # This ensures they get processed and tagged like other measurements
                                        if 'measurement' not in entities:
                                            entities['measurement'] = []
                                        entities['measurement'].extend(range_entities)
                                        self.core8_extractor.enrichment(f"üìä Range consolidation: {len(range_entities)} ranges added to measurements")
                                    
                                except Exception as e:
                                    self.core8_extractor.enrichment(f"‚ö†Ô∏è Range detection failed: {e}")
                                
                                range_time = (time.perf_counter() - range_start) * 1000
                                self.core8_extractor.enrichment(f"‚úÖ Range post-processing complete: {range_time:.1f}ms")
                            
                            # NOTE: Individual entity normalization removed - now handled by normalization phase below
                            
                            # Add entity extraction to YAML
                            doc.yaml_frontmatter['raw_entities'] = entities
                            doc.yaml_frontmatter['entity_insights'] = {
                                'has_financial_data': len(entities.get('money', [])) > 0,
                                'has_contact_info': len(entities.get('phone', [])) > 0 or len(entities.get('email', [])) > 0,
                                'has_temporal_data': len(entities.get('date', [])) > 0,
                                'has_external_references': len(entities.get('url', [])) > 0,
                                'total_entities_found': sum(len(v) for v in entities.values())
                            }
                            
                            # Log Core 8 entities using Rule #11 structured format
                            core8_counts = []
                            for entity_type in ['person', 'org', 'location', 'gpe', 'date', 'time', 'money', 'measurement']:
                                count = len(entities.get(entity_type, []))
                                if count > 0:
                                    core8_counts.append(f"{entity_type}:{count}")
                            
                            if core8_counts:
                                self.core8_extractor.semantics(f"üìä Global entities: {', '.join(core8_counts)}")
                            
                            set_current_phase('entity_extraction')
                            # Get page count from document metadata
                            doc_page_count = doc.yaml_frontmatter.get('page_count', 0)
                            add_pages_processed(doc_page_count)
                            self.phase_manager.log('entity_extraction', f"üîç Extracted {doc.yaml_frontmatter['entity_insights']['total_entities_found']} total entities")
                            
                            # NEW: Entity Normalization Phase
                            if self.entity_normalizer:
                                normalization_start = time.perf_counter()
                                set_current_phase('normalization')
                                self.entity_normalizer_logger.normalization(f"üîÑ Normalization Phase: Processing entities for canonicalization: {doc.source_filename}")
                                
                                # PRESERVE CLEAN MARKDOWN: Store original text for semantic analysis before normalization
                                doc.clean_markdown_content = doc.markdown_content
                                self.entity_normalizer_logger.normalization(f"üìÑ Clean markdown preserved for semantic analysis: {len(doc.clean_markdown_content)} chars")
                                
                                # Perform entity canonicalization and global text replacement
                                normalization_result = self.entity_normalizer.normalize_entities_phase(
                                    entities, doc.markdown_content
                                )
                                
                                # Log normalized entity counts using structured format
                                normalized_counts = []
                                original_count = sum(len(entity_list) for entity_list in entities.values())
                                canonical_count = len(normalization_result.normalized_entities)
                                
                                for entity_type in ['person', 'org', 'location', 'gpe', 'date', 'time', 'money', 'measurement']:
                                    type_entities = [e for e in normalization_result.normalized_entities if e.type.lower() == entity_type.lower()]
                                    if type_entities:
                                        normalized_counts.append(f"{entity_type}:{len(type_entities)}")
                                
                                if normalized_counts:
                                    reduction_percent = ((original_count - canonical_count) / original_count * 100) if original_count > 0 else 0
                                    self.entity_normalizer_logger.normalization(f"üìä Normalized entities: {', '.join(normalized_counts)} ({original_count}‚Üí{canonical_count} entities, {reduction_percent:.0f}% reduction): {doc.source_filename}")
                                
                                # Apply global text replacement
                                self.entity_normalizer_logger.normalization(f"üîÑ Global text replacement: Applying ||canonical||id|| format to document: {doc.source_filename}")
                                doc.markdown_content = normalization_result.normalized_text
                                
                                # Add normalization data to YAML
                                doc.yaml_frontmatter['normalization'] = {
                                    'normalization_method': 'mvp-fusion-canonicalization',
                                    'processing_time_ms': normalization_result.processing_time_ms,
                                    'statistics': normalization_result.statistics,
                                    'canonical_entities': [
                                        {
                                            'id': entity.id,
                                            'type': entity.type,
                                            'normalized': entity.normalized,
                                            'aliases': entity.aliases,
                                            'count': entity.count,
                                            'mentions': entity.mentions,
                                            'metadata': entity.metadata
                                        }
                                        for entity in normalization_result.normalized_entities
                                    ],
                                    'entity_reduction_percent': normalization_result.statistics.get('entity_reduction_percent', 0),
                                    'performance': {
                                        'entities_per_ms': normalization_result.statistics.get('performance_metrics', {}).get('entities_per_ms', 0),
                                        'edge_compatible': normalization_result.statistics.get('performance_metrics', {}).get('edge_compatible', False)
                                    }
                                }
                                
                                normalization_time = (time.perf_counter() - normalization_start) * 1000
                                # Pages already counted in classification phase
                                self.phase_manager.log('normalization', f"üîÑ Normalized {canonical_count} canonical entities from {original_count} raw entities")
                                self.entity_normalizer_logger.normalization(f"‚úÖ Normalization complete: {normalization_time:.1f}ms (enhanced text ready for semantic analysis): {doc.source_filename}")
                            
                            # TIMING FIX: Start semantic analysis phase
                            set_current_phase('semantic_analysis')
                            # Pages already counted in classification phase
                            
                            # Intelligent semantic extraction with meaningful fact focus
                            self.semantic_analyzer.semantics(f"üß† Intelligent semantic extraction (quality-focused SPO): {doc.source_filename}")
                            
                            # CLEAN TEXT SEMANTIC ANALYSIS: Use raw text without entity normalization tags
                            if hasattr(doc, 'clean_markdown_content') and doc.clean_markdown_content:
                                # Use clean markdown directly for semantic extraction (no entity tags)
                                self.semantic_analyzer.semantics(f"üìÑ Using clean text for semantic analysis ({len(doc.clean_markdown_content)} chars)")
                                semantic_facts = self.semantic_extractor.extract_semantic_facts(doc.clean_markdown_content)
                            else:
                                # Fallback to raw text if clean text not available (avoid normalized entity tags)
                                full_document = doc.generate_final_markdown()
                                semantic_facts = self.semantic_extractor.extract_semantic_facts(full_document)
                            
                            if semantic_facts:
                                # Store semantic data in document
                                doc.semantic_json = semantic_facts
                                # Safely update processing stage
                                if 'processing' not in doc.yaml_frontmatter:
                                    doc.yaml_frontmatter['processing'] = {}
                                doc.yaml_frontmatter['processing']['stage'] = 'extracted'
                                
                                # Pages already counted in classification phase
                                self.phase_manager.log('semantic_analyzer', f"üîç Added semantic JSON to document: {doc.source_filename}")
                                
                                # Log intelligent fact extraction results (following Rule #11 format)
                                semantic_fact_data = semantic_facts.get('semantic_facts', {})
                                semantic_summary = semantic_facts.get('semantic_summary', {})
                                
                                # Log meaningful fact categories
                                meaningful_counts = []
                                for category, facts in semantic_fact_data.items():
                                    if isinstance(facts, list) and len(facts) > 0:
                                        meaningful_counts.append(f"{category}:{len(facts)}")
                                
                                if meaningful_counts:
                                    self.semantic_analyzer.semantics(f"üß† Meaningful facts: {', '.join(meaningful_counts)}")
                                
                                # Log quality metrics
                                total_facts = semantic_summary.get('total_facts', 0)
                                actionable_facts = semantic_summary.get('actionable_facts', 0)
                                quality_threshold = semantic_summary.get('quality_threshold', 0.75)
                                
                                if total_facts > 0:
                                    self.semantic_analyzer.semantics(f"üìä Quality extraction: {total_facts} facts (confidence‚â•{quality_threshold})")
                                    if actionable_facts > 0:
                                        self.semantic_analyzer.semantics(f"‚ö° Actionable facts: {actionable_facts}/{total_facts} require specific actions")
                                
                                # Log performance from intelligent extraction
                                if 'intelligent_extraction' in semantic_facts:
                                    extraction_info = semantic_facts['intelligent_extraction']
                                    extraction_time = extraction_info.get('extraction_time_ms', 0)
                                    if extraction_time > 0:
                                        self.semantic_analyzer.semantics(f"üöÄ Intelligent extraction: {extraction_time:.1f}ms (quality-focused)")
                        
                        doc.success = True
                    else:
                        # Fallback to mock processing
                        doc.success = True
                        doc.yaml_frontmatter['processing']['stage'] = 'mock_processed'
                        set_current_phase('document_processing')
                        self.phase_manager.log('document_processor', f"ÔøΩ Ô∏è  Using mock processing for {doc.source_filename}")
                        
                except Exception as e:
                    self.logger.logger.error(f"‚ùå Entity extraction failed for {doc.source_filename}: {e}")
                    doc.success = False
                    doc.mark_failed(f"Entity extraction error: {e}")
                
                processing_time = time.perf_counter() - processing_start
                
                # Return the enriched document
                results.append(doc)
                set_current_phase('document_processing')
                # Document processed (logged in final summary)
                
            except Empty:
                # Normal timeout, continue checking
                continue
            except Exception as e:
                self.logger.logger.error(f"‚ùå CPU worker {worker_id} error: {e}")
        
        self.document_processor.logger.debug(f"üèÅ CPU worker {worker_id} finished: {len(results)} items processed")
        return results
    
    def process_files_service(self, file_paths: List[Path], output_dir: Path = None) -> tuple[List[InMemoryDocument], float]:
        """
        Process files using enhanced ThreadPoolExecutor architecture.
        
        PERFORMANCE ENHANCEMENT: Replaces queue-based I/O + CPU workers with direct ThreadPool processing
        - Eliminates queue blocking issues (142x performance improvement proven)
        - Maintains all existing pipeline functionality
        - Service-ready architecture for concurrent requests
        
        Returns:
            Tuple of (in_memory_documents_list, total_processing_time)
        """
        if not self.active:
            self.start_service()
        
        total_start = time.perf_counter()
        
        self.service_coordinator.staging(f"üöÄ Processing {len(file_paths)} files with enhanced ThreadPool architecture ({self.cpu_workers} workers)")
        
        # A/B TEST MODE: Compare ThreadPool vs Sequential processing
        USE_SEQUENTIAL = False  # Use ThreadPool for parallel processing
        all_results = []
        failed_count = 0
        completed_count = 0
        
        if USE_SEQUENTIAL:
            # SEQUENTIAL PROCESSING TEST
            processing_start = time.perf_counter()
            self.service_coordinator.staging(f"üìä TESTING: Sequential mode (no ThreadPool)")
            
            for file_path in file_paths:
                try:
                    doc = self._process_single_document(file_path)
                    if doc and doc.success:
                        all_results.append(doc)
                        completed_count += 1
                        if completed_count % 100 == 0:
                            self.service_coordinator.staging(f"üìä Processing: {completed_count}/{len(file_paths)} files...")
                    else:
                        failed_count += 1
                        self.logger.logger.warning(f"‚ö†Ô∏è Failed to process {file_path.name}")
                except Exception as e:
                    failed_count += 1
                    self.logger.logger.error(f"‚ùå Pipeline error for {file_path.name}: {e}")
            
            processing_time = (time.perf_counter() - processing_start) * 1000
            self.service_coordinator.staging(f"üìä TIMING: Sequential processing took {processing_time:.2f}ms")
            
        else:
            # THREADPOOL PROCESSING - Optimized for 2 cores
            submission_start = time.perf_counter()
            # Use exactly 2 workers to match our CPU allocation
            actual_workers = self.cpu_workers  # Should be 2 from config
            
            with ThreadPoolExecutor(max_workers=actual_workers) as executor:
                future_to_path = {
                    executor.submit(self._process_single_document, file_path): file_path 
                    for file_path in file_paths
                }
                
                submission_time = (time.perf_counter() - submission_start) * 1000
                self.service_coordinator.staging(f"üìä TIMING: Task submission took {submission_time:.2f}ms")
                
                collection_start = time.perf_counter()
                
                # Process files in batches of 10 as originally requested
                from concurrent.futures import as_completed
                
                # Ensure output directory exists once
                if output_dir is None:
                    output_dir = Path.cwd()
                output_dir.mkdir(parents=True, exist_ok=True)
                
                # Batch configuration
                BATCH_SIZE = self.config.get('performance', {}).get('batch_size', 20)  # Use config performance.batch_size, fallback to 20
                current_batch = []
                
                for future in as_completed(future_to_path):
                    file_path = future_to_path[future]
                    try:
                        doc = future.result()
                        if doc and doc.success:
                            all_results.append(doc)
                            current_batch.append(doc)
                            completed_count += 1
                            
                            # Write batch when we reach BATCH_SIZE
                            if len(current_batch) >= BATCH_SIZE:
                                self._write_results_to_disk(current_batch, output_dir)
                                self.service_coordinator.staging(f"üìä Batch written: {completed_count}/{len(file_paths)} files processed ({BATCH_SIZE} files/batch)")
                                current_batch = []  # Reset batch
                            
                        else:
                            failed_count += 1
                            self.logger.logger.warning(f"‚ö†Ô∏è Failed to process {file_path.name}")
                    except Exception as e:
                        failed_count += 1
                        self.logger.logger.error(f"‚ùå Pipeline error for {file_path.name}: {e}")
                
                # Write any remaining files in the final batch
                if current_batch:
                    self._write_results_to_disk(current_batch, output_dir)
                    self.service_coordinator.staging(f"üìä Final batch written: {len(current_batch)} files")
                
                collection_time = (time.perf_counter() - collection_start) * 1000
                self.service_coordinator.staging(f"üìä TIMING: ThreadPool collection took {collection_time:.2f}ms")
        
        # Files are now written immediately as they complete in ThreadPool mode
        # No need for batch write at the end
        if USE_SEQUENTIAL and all_results:
            # Only do batch write if we're in sequential mode
            io_start = time.perf_counter()
            self._write_results_to_disk(all_results, output_dir or Path.cwd())
            io_time = (time.perf_counter() - io_start) * 1000
            self.service_coordinator.staging(f"üìä TIMING: Batch I/O write took {io_time:.2f}ms")
        
        completed_count = len(all_results)
        failed_count = len(file_paths) - completed_count
        
        total_time = time.perf_counter() - total_start
        
        # Final performance summary
        if completed_count > 0 and total_time > 0:
            files_per_sec = completed_count / total_time
            avg_time_ms = (total_time / completed_count) * 1000
            self.service_coordinator.staging(f"üìä Performance: {files_per_sec:.1f} files/sec, {avg_time_ms:.1f}ms/file")
        
        self.service_coordinator.staging(f"üèÅ Batch pipeline complete: {completed_count}/{len(file_paths)} files in {total_time:.2f}s")
        
        # Return the actual results for proper reporting
        return all_results, total_time
    
    def _process_single_file_streaming(self, file_path: Path, output_dir: Path) -> bool:
        """
        TRUE STREAMING PIPELINE: Process single file through all 7 stages and write to disk immediately.
        Returns success/failure boolean instead of document object to save memory.
        """
        try:
            # Process through full pipeline (same logic as _process_single_document)
            doc = self._process_single_document(file_path)
            if not doc or not doc.success:
                return False
            
            # IMMEDIATE FILE WRITING: Write to disk as soon as processing completes
            self._write_single_result_to_disk(doc, output_dir)
            return True
            
        except Exception as e:
            self.logger.logger.error(f"‚ùå Streaming pipeline failed for {file_path.name}: {e}")
            return False
    
    def _process_single_document(self, file_path: Path) -> Optional[InMemoryDocument]:
        """
        Process a single document through the complete pipeline.
        
        PERFORMANCE ENHANCEMENT: Combines I/O loading and CPU processing into single method
        for ThreadPoolExecutor. Maintains all existing pipeline functionality.
        
        Returns:
            InMemoryDocument: Processed document with all pipeline stages completed
        """
        try:
            # Stage 1: Document Loading and Conversion (I/O phase)
            set_current_phase('pdf_conversion')
            
            doc = InMemoryDocument(source_file_path=str(file_path))
            
            # Use existing PDF/document conversion logic from _io_worker_ingestion
            conversion_start = time.perf_counter()
            
            # Extract markdown content using existing logic
            markdown_content = ""
            content_flags = {}
            
            if file_path.suffix.lower() == '.pdf':
                # PDF processing
                if fitz:
                    try:
                        pdf_doc = fitz.open(str(file_path))
                        page_count = len(pdf_doc)  # Get page count before processing
                        for page_num in range(page_count):
                            page = pdf_doc.load_page(page_num)
                            page_text = page.get_text()
                            markdown_content += f"\n\n# Page {page_num + 1}\n\n{page_text}"
                        pdf_doc.close()
                        doc.page_count = page_count  # Use stored page count, not closed doc
                        content_flags['has_pdf_content'] = True
                    except Exception as e:
                        self.logger.logger.warning(f"PDF processing failed for {file_path.name}: {e}")
                        # Fallback to text reading
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            markdown_content = f.read()
                        doc.page_count = 1
                else:
                    # No PyMuPDF available
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        markdown_content = f.read()
                    doc.page_count = 1
            else:
                # Other file formats
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        markdown_content = f.read()
                    doc.page_count = 1
                    content_flags['has_text_content'] = True
                except Exception as e:
                    self.logger.logger.error(f"Failed to read {file_path.name}: {e}")
                    return None
            
            doc.markdown_content = markdown_content
            conversion_time = (time.perf_counter() - conversion_start) * 1000
            
            # Track pages processed for performance metrics
            add_pages_processed(doc.page_count)
            add_files_processed(1)
            
            # Perform comprehensive content analysis
            comprehensive_analysis = self._quick_content_scan(markdown_content)
            # Merge with basic content flags
            content_flags.update(comprehensive_analysis)
            
            # Set up YAML frontmatter using existing template (FIXED: Deep copy nested dicts)
            import copy
            doc.yaml_frontmatter = copy.deepcopy(self.yaml_template)
            doc.yaml_frontmatter['conversion']['source_file'] = file_path.name
            doc.yaml_frontmatter['conversion']['conversion_time_ms'] = conversion_time
            doc.yaml_frontmatter['conversion']['page_count'] = doc.page_count
            doc.yaml_frontmatter['conversion']['format'] = file_path.suffix.upper().replace('.', '') if file_path.suffix else 'TXT'
            doc.yaml_frontmatter['content_detection'] = content_flags
            doc.yaml_frontmatter['processing']['content_length'] = len(markdown_content)
            
            # Stage 2-6: Use existing CPU processing logic from _cpu_worker_processing
            set_current_phase('document_processing')
            
            # Document classification
            if hasattr(self, '_classify_document'):
                classification = self._classify_document(doc)
                doc.yaml_frontmatter['content_detection'].update(classification)
            
            # Complete pipeline processing - using existing CPU worker logic
            try:
                if self.aho_corasick_engine and self.semantic_extractor:
                    # Real classification with Aho-Corasick
                    set_current_phase('classification')
                    add_pages_processed(doc.page_count)
                    self.document_classifier.classification(f"Classifying document: {doc.source_filename}")
                    
                    # Get domain and document type classifications
                    domain_results = self.aho_corasick_engine.classify_domains(doc.markdown_content)
                    doctype_results = self.aho_corasick_engine.classify_document_types(doc.markdown_content)
                    
                    # Determine primary domain and confidence
                    primary_domain = max(domain_results.keys(), key=lambda k: domain_results[k]) if domain_results else 'general'
                    primary_domain_confidence = domain_results.get(primary_domain, 0)
                    
                    # Determine primary document type and confidence
                    primary_document_type = max(doctype_results.keys(), key=lambda k: doctype_results[k]) if doctype_results else 'document'
                    primary_doctype_confidence = doctype_results.get(primary_document_type, 0)
                    
                    # Domain-based routing decisions
                    routing_decisions = {
                        'skip_entity_extraction': primary_domain_confidence < 5.0,
                        'enable_deep_domain_extraction': primary_domain_confidence >= 60.0,
                        'domain_specialization_route': primary_domain if primary_domain_confidence >= 40.0 else 'general'
                    }
                    
                    # Classification results
                    classification_result = {
                        'domains': domain_results,
                        'document_types': doctype_results,
                        'primary_domain': primary_domain,
                        'primary_domain_confidence': primary_domain_confidence,
                        'primary_document_type': primary_document_type,
                        'primary_doctype_confidence': primary_doctype_confidence,
                        'domain_routing': routing_decisions
                    }
                    
                    # Add classification to YAML
                    doc.yaml_frontmatter['classification'] = classification_result
                    doc.yaml_frontmatter['processing']['stage'] = 'classified'
                    
                    # Extract Core 8 Universal Entities
                    set_current_phase('entity_extraction')
                    add_pages_processed(doc.page_count)
                    self.core8_extractor.enrichment(f"Extracting entities: {doc.source_filename}")
                    entities = self._extract_universal_entities(doc.markdown_content)
                    
                    # NEW: Range Post-Processing (WIP.md approved approach) - CORRECT LOCATION
                    if entities:
                        range_start = time.perf_counter()
                        self.core8_extractor.enrichment(f"üîÑ Range Post-Processing: Detecting ranges from extracted entities")
                        
                        # Detect and consolidate ranges using span analysis
                        try:
                            range_entities = self._detect_ranges_from_text(doc.markdown_content)
                            
                            if range_entities:
                                # Add range entities to the measurement category for normalization
                                # This ensures they get processed and tagged like other measurements
                                if 'measurement' not in entities:
                                    entities['measurement'] = []
                                entities['measurement'].extend(range_entities)
                                self.core8_extractor.enrichment(f"üìä Range consolidation: {len(range_entities)} ranges added to measurements")
                            
                        except Exception as e:
                            self.core8_extractor.enrichment(f"‚ö†Ô∏è Range detection failed: {e}")
                        
                        range_time = (time.perf_counter() - range_start) * 1000
                        self.core8_extractor.enrichment(f"‚úÖ Range post-processing complete: {range_time:.1f}ms")
                    
                    # Add entity extraction to YAML
                    doc.yaml_frontmatter['raw_entities'] = entities
                    doc.yaml_frontmatter['entity_insights'] = {
                        'has_financial_data': len(entities.get('money', [])) > 0,
                        'has_contact_info': len(entities.get('phone', [])) > 0 or len(entities.get('email', [])) > 0,
                        'has_temporal_data': len(entities.get('date', [])) > 0,
                        'has_external_references': len(entities.get('url', [])) > 0,
                        'total_entities_found': sum(len(v) for v in entities.values())
                    }
                    
                    # Log Core 8 entities
                    core8_counts = []
                    for entity_type in ['person', 'org', 'location', 'gpe', 'date', 'time', 'money', 'measurement']:
                        count = len(entities.get(entity_type, []))
                        if count > 0:
                            core8_counts.append(f"{entity_type}:{count}")
                    
                    if core8_counts:
                        self.core8_extractor.semantics(f"üìä Global entities: {', '.join(core8_counts)}")
                        
                    # Entity Normalization Phase
                    if self.entity_normalizer:
                        set_current_phase('normalization')
                        add_pages_processed(doc.page_count)
                        self.entity_normalizer_logger.normalization(f"üîÑ Normalization Phase: Processing entities for canonicalization: {doc.source_filename}")
                        
                        # PRESERVE CLEAN MARKDOWN: Store original text for semantic analysis before normalization
                        doc.clean_markdown_content = doc.markdown_content
                        
                        # Perform entity canonicalization and global text replacement
                        normalization_result = self.entity_normalizer.normalize_entities_phase(
                            entities, doc.markdown_content
                        )
                        
                        # Log normalized entity counts
                        normalized_counts = []
                        original_count = sum(len(entity_list) for entity_list in entities.values())
                        canonical_count = len(normalization_result.normalized_entities)
                        
                        for entity_type in ['person', 'org', 'location', 'gpe', 'date', 'time', 'money', 'measurement']:
                            type_entities = [e for e in normalization_result.normalized_entities if e.type.lower() == entity_type.lower()]
                            if type_entities:
                                normalized_counts.append(f"{entity_type}:{len(type_entities)}")
                        
                        if normalized_counts:
                            reduction_percent = ((original_count - canonical_count) / original_count * 100) if original_count > 0 else 0
                            self.entity_normalizer_logger.normalization(f"üìä Normalized entities: {', '.join(normalized_counts)} ({original_count}‚Üí{canonical_count} entities, {reduction_percent:.0f}% reduction)")
                        
                        # Apply global text replacement
                        doc.markdown_content = normalization_result.normalized_text
                        
                        # Add normalization data to YAML
                        doc.yaml_frontmatter['normalization'] = {
                            'normalization_method': 'mvp-fusion-canonicalization',
                            'processing_time_ms': normalization_result.processing_time_ms,
                            'statistics': normalization_result.statistics,
                            'canonical_entities': [
                                {
                                    'id': entity.id,
                                    'type': entity.type,
                                    'normalized': entity.normalized,
                                    'aliases': entity.aliases,
                                    'count': entity.count,
                                    'mentions': entity.mentions,
                                    'metadata': entity.metadata
                                }
                                for entity in normalization_result.normalized_entities
                            ],
                            'entity_reduction_percent': normalization_result.statistics.get('entity_reduction_percent', 0)
                        }
                    
                    # Semantic fact extraction (OPTIONAL - can be disabled for speed)
                    ENABLE_SEMANTIC = True  # Enable Stage 6: Semantic Analysis
                    if ENABLE_SEMANTIC and hasattr(self, 'semantic_extractor') and self.semantic_extractor:
                        set_current_phase('semantic_analysis')
                        add_pages_processed(doc.page_count)
                        try:
                            semantic_results = self.semantic_extractor.extract_semantic_facts(doc.markdown_content or doc.clean_markdown_content)
                            
                            if semantic_results and semantic_results.get('semantic_facts'):
                                # Convert from StandaloneIntelligentExtractor format to expected format
                                semantic_facts = semantic_results.get('semantic_facts', {})
                                
                                # Extract all facts from different categories
                                all_facts = []
                                all_facts.extend(semantic_facts.get('actionable_requirements', []))
                                all_facts.extend(semantic_facts.get('compliance_facts', []))
                                all_facts.extend(semantic_facts.get('measurement_facts', []))
                                all_facts.extend(semantic_facts.get('organizational_facts', []))
                                all_facts.extend(semantic_facts.get('quantitative_facts', []))
                                
                                # Create expected JSON format
                                doc.semantic_json = {
                                    'facts': all_facts,
                                    'rules': [],  # StandaloneIntelligentExtractor doesn't extract rules separately
                                    'relationships': semantic_facts.get('spo_triplets', []),
                                    'semantic_summary': semantic_results.get('semantic_summary', {})
                                }
                                
                                fact_count = len(all_facts)
                                self.semantic_analyzer.semantics(f"üß† Extracted {fact_count} semantic facts")
                            else:
                                doc.semantic_json = {'facts': [], 'rules': [], 'relationships': []}
                                
                        except Exception as e:
                            self.logger.logger.warning(f"Semantic extraction failed for {file_path.name}: {e}")
                            doc.semantic_json = {'facts': [], 'rules': [], 'relationships': []}
                else:
                    # Fallback if engines not available
                    self.logger.logger.warning(f"Aho-Corasick engine or semantic extractor not available - minimal processing")
                    doc.semantic_json = {'facts': [], 'rules': [], 'relationships': []}
                    
            except Exception as e:
                self.logger.logger.error(f"Complete pipeline processing failed for {file_path.name}: {e}")
                # Ensure doc still has semantic_json for consistency
                if not hasattr(doc, 'semantic_json'):
                    doc.semantic_json = {'facts': [], 'rules': [], 'relationships': []}
            
            # Mark as completed
            doc.yaml_frontmatter['processing']['stage'] = 'completed'
            doc.success = True
            
            return doc
            
        except Exception as e:
            self.logger.logger.error(f"‚ùå Failed to process {file_path.name}: {e}")
            return None
    
    def _write_single_result_to_disk(self, doc: InMemoryDocument, output_dir: Path):
        """STREAMING WRITER: Write single processed document to disk immediately"""
        try:
            # Ensure output directory exists
            output_dir.mkdir(parents=True, exist_ok=True)
            
            if doc.success and doc.markdown_content:
                # Extract single-file writing logic from batch writer
                yaml_size = len(str(doc.yaml_frontmatter)) if doc.yaml_frontmatter else 0
                json_size = len(str(doc.semantic_json)) if doc.semantic_json else 0
                
                # Write markdown file (with YAML frontmatter)
                md_filename = f"{doc.source_stem}.md"
                md_path = output_dir / md_filename
                
                # Construct full markdown with YAML frontmatter
                full_content = doc.markdown_content
                if doc.yaml_frontmatter:
                    import yaml
                    import copy
                    from collections import OrderedDict
                    
                    # Create ordered YAML with proper section ordering (same as batch logic)
                    ordered_yaml = OrderedDict()
                    
                    # 1. Conversion info FIRST
                    if 'conversion' in doc.yaml_frontmatter:
                        ordered_yaml['conversion'] = doc.yaml_frontmatter['conversion']
                    
                    # 2. Content analysis SECOND  
                    if 'content_detection' in doc.yaml_frontmatter:
                        ordered_yaml['content_analysis'] = copy.deepcopy(doc.yaml_frontmatter['content_detection'])
                    
                    # 3. Processing info THIRD
                    if 'processing' in doc.yaml_frontmatter:
                        ordered_yaml['processing'] = doc.yaml_frontmatter['processing']
                    
                    # 4. Add remaining sections
                    for key, value in doc.yaml_frontmatter.items():
                        if key not in ordered_yaml and key not in ['processing_info', 'classification', 'content_detection']:
                            ordered_yaml[key] = value
                    
                    yaml_header = yaml.dump(ordered_yaml, default_flow_style=False, allow_unicode=True)
                    
                    # Clean up YAML formatting
                    import re
                    def force_flow_style_spans(yaml_text):
                        return re.sub(r'span:\s*\n\s*start: (\d+)\s*\n\s*end: (\d+)', 
                                     r'span: {start: \1, end: \2}', yaml_text)
                    
                    yaml_header = force_flow_style_spans(yaml_header)
                    full_content = f"---\n{yaml_header}---\n\n{doc.markdown_content}"
                
                yaml_time = (time.perf_counter() - yaml_start) * 1000
                total_yaml_time += yaml_time
                
                # TIMING: File write
                write_start = time.perf_counter()
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(full_content)
                write_time = (time.perf_counter() - write_start) * 1000
                total_file_write_time += write_time
                
                if i < 3:  # Log first 3 files for analysis
                    self.service_coordinator.staging(f"üìä I/O TIMING File {i+1}: YAML={yaml_time:.2f}ms, Write={write_time:.2f}ms")
                
                # ALWAYS write JSON file alongside Markdown for pipeline consistency
                # Initialize empty if not present
                if not hasattr(doc, 'semantic_json') or doc.semantic_json is None:
                    doc.semantic_json = {'facts': [], 'rules': [], 'relationships': []}
                
                json_filename = f"{doc.source_stem}.json"
                json_path = output_dir / json_filename
                
                # TIMING: JSON write
                json_start = time.perf_counter()
                try:
                    import orjson
                    with open(json_path, 'wb') as f:
                        f.write(orjson.dumps(doc.semantic_json, option=orjson.OPT_INDENT_2))
                except ImportError:
                    import json
                    with open(json_path, 'w', encoding='utf-8') as f:
                        json.dump(doc.semantic_json, f, indent=2)
                
                json_time = (time.perf_counter() - json_start) * 1000
                total_json_write_time += json_time
                
                if i < 3:  # Log first 3 files for analysis  
                    self.service_coordinator.staging(f"üìä I/O TIMING File {i+1}: JSON={json_time:.2f}ms")
                            
        except Exception as e:
            self.logger.logger.error(f"‚ùå Failed to write {doc.source_filename}: {e}")
    
    def _write_batch_async(self, batch: List[InMemoryDocument], output_dir: Path):
        """NON-BLOCKING BATCH WRITER: Fire-and-forget I/O operations"""
        # Create a separate thread to handle the batch writing
        def write_batch_in_background():
            for doc in batch:
                try:
                    self._write_single_result_to_disk(doc, output_dir)
                except Exception as e:
                    self.logger.logger.error(f"Background write failed: {e}")
        
        # Start the background writer thread and don't wait for it
        import threading
        writer_thread = threading.Thread(target=write_batch_in_background, daemon=True)
        writer_thread.start()
        # Return immediately - don't wait for writes to complete
    
    def _write_results_to_disk(self, results: List[InMemoryDocument], output_dir: Path):
        """WRITER-IO phase: Write batch of documents to disk"""
        # Batch writing - no lock needed since we write batches sequentially
        try:
            # Set thread name for I/O worker attribution 
            original_name = threading.current_thread().name
            threading.current_thread().name = "IOWorker-1"
            
            # DETAILED I/O TIMING ANALYSIS
            import time
            total_yaml_time = 0
            total_file_write_time = 0
            total_json_write_time = 0
            
            
            # Ensure output directory exists
            mkdir_start = time.perf_counter()
            output_dir.mkdir(parents=True, exist_ok=True)
            mkdir_time = (time.perf_counter() - mkdir_start) * 1000
            self.service_coordinator.staging(f"üìä I/O TIMING: mkdir took {mkdir_time:.2f}ms")
            
            successful_writes = 0
            
            for i, doc in enumerate(results):
                if doc.success and doc.markdown_content:
                    try:
                        # Debug what we have in the document
                        yaml_size = len(str(doc.yaml_frontmatter)) if doc.yaml_frontmatter else 0
                        json_size = len(str(doc.semantic_json)) if doc.semantic_json else 0
                        
                        self.memory_manager.logger.debug(f"üìÑ Document state: {doc.source_filename} - YAML: {yaml_size}B, JSON: {json_size}B")
                        
                        # Write markdown file (with YAML frontmatter)
                        md_filename = f"{doc.source_stem}.md"
                        md_path = output_dir / md_filename
                        
                        # TIMING: YAML generation
                        yaml_start = time.perf_counter()
                        
                        # Construct full markdown with YAML frontmatter (in correct order)
                        full_content = doc.markdown_content
                        if doc.yaml_frontmatter:
                            import yaml
                            from collections import OrderedDict
                            
                            # Create ordered YAML with proper section ordering
                            ordered_yaml = OrderedDict()
                            
                            # 1. Conversion info FIRST
                            if 'conversion' in doc.yaml_frontmatter:
                                ordered_yaml['conversion'] = doc.yaml_frontmatter['conversion']
                            
                            # 2. Content analysis (pre-screening flags)
                            if 'content_detection' in doc.yaml_frontmatter:
                                import copy
                                ordered_yaml['content_analysis'] = copy.deepcopy(doc.yaml_frontmatter['content_detection'])
                            
                            # 3. Processing info (merge processing and processing_info)
                            processing = {}
                            if 'processing' in doc.yaml_frontmatter:
                                processing.update(doc.yaml_frontmatter['processing'])
                            if 'processing_info' in doc.yaml_frontmatter:
                                processing.update(doc.yaml_frontmatter['processing_info'])
                            if processing:
                                ordered_yaml['processing'] = processing
                            
                            # 4. Domain Classification (enhanced structure)
                            if 'classification' in doc.yaml_frontmatter:
                                import copy
                                classification_data = copy.deepcopy(doc.yaml_frontmatter['classification'])
                                if 'source_file' in classification_data:
                                    del classification_data['source_file']
                                
                                # Build enhanced domain_classification structure
                                domain_classification = {}
                                
                                # Routing decisions FIRST (drives classification)
                                if 'domain_routing' in classification_data:
                                    domain_classification['routing'] = copy.deepcopy(classification_data['domain_routing'])
                                
                                # Top domains and document types (configurable top 5)
                                domains = classification_data.get('domains', {})
                                document_types = classification_data.get('document_types', {})
                                
                                # Get top 5 domains with scores > 0.5
                                top_domains = [(k, v) for k, v in sorted(domains.items(), key=lambda x: x[1], reverse=True) 
                                             if v > 0.5][:5]
                                top_doc_types = [(k, v) for k, v in sorted(document_types.items(), key=lambda x: x[1], reverse=True) 
                                               if v > 0.5][:5]
                                
                                if top_domains:
                                    domain_classification['top_domains'] = [k for k, v in top_domains]
                                if top_doc_types:
                                    domain_classification['top_document_types'] = [k for k, v in top_doc_types]
                                
                                # Full detailed breakdowns (only scores > 0.5)
                                filtered_domains = {k: v for k, v in domains.items() if v > 0.5}
                                filtered_doc_types = {k: v for k, v in document_types.items() if v > 0.5}
                                
                                if filtered_domains:
                                    domain_classification['domains'] = filtered_domains
                                if filtered_doc_types:
                                    domain_classification['document_types'] = filtered_doc_types
                                
                                ordered_yaml['domain_classification'] = domain_classification
                            
                            # 5. Source info
                            if 'source' in doc.yaml_frontmatter:
                                ordered_yaml['source'] = doc.yaml_frontmatter['source']
                            
                            # Add any other sections not explicitly ordered
                            for key, value in doc.yaml_frontmatter.items():
                                if key not in ordered_yaml and key not in ['processing_info', 'classification', 'content_detection']:
                                    ordered_yaml[key] = value
                            
                            yaml_header = yaml.dump(
                                dict(ordered_yaml), 
                                Dumper=NoAliasDumper,
                                default_flow_style=None,  # Let custom representers decide
                                sort_keys=False,
                                width=200,
                                allow_unicode=True
                            )
                            # Apply regex post-processing to guarantee flow style spans
                            yaml_header = force_flow_style_spans(yaml_header)
                            full_content = f"---\n{yaml_header}---\n\n{doc.markdown_content}"
                        
                        yaml_time = (time.perf_counter() - yaml_start) * 1000
                        total_yaml_time += yaml_time
                        
                        # TIMING: File write
                        write_start = time.perf_counter()
                        set_current_phase('file_writing')
                        # Get page count from document metadata
                        doc_page_count = doc.yaml_frontmatter.get('page_count', 0)
                        add_pages_processed(doc_page_count)
                        self.phase_manager.log('file_writer', f"Writing markdown with YAML: {md_filename}")
                        with open(md_path, 'w', encoding='utf-8') as f:
                            f.write(full_content)
                        
                        write_time = (time.perf_counter() - write_start) * 1000
                        total_file_write_time += write_time
                        
                        # TIMING: JSON write
                        json_start = time.perf_counter()
                        
                        # Write JSON knowledge file if we have semantic data (HIGH-PERFORMANCE orjson)
                        if doc.semantic_json:
                            json_filename = f"{doc.source_stem}.json"
                            json_path = output_dir / json_filename
                            
                            set_current_phase('file_writing')
                            self.phase_manager.log('file_writer', f"Writing JSON knowledge (orjson 23.6x faster): {json_filename}")
                            
                            # Use high-performance JSON formatter with horizontal layout
                            try:
                                from utils.high_performance_json import save_semantic_facts_fast
                                save_semantic_facts_fast(doc.semantic_json, str(json_path))
                            except ImportError:
                                # Fallback to standard JSON if orjson not available
                                import json
                                with open(json_path, 'w', encoding='utf-8') as f:
                                    json.dump(doc.semantic_json, f, indent=2)
                        else:
                            set_current_phase('file_writing')
                            self.phase_manager.log('file_writer', f"ÔøΩ Ô∏è  No semantic JSON to write for {doc.source_filename}")
                        
                        json_time = (time.perf_counter() - json_start) * 1000
                        total_json_write_time += json_time
                        
                        successful_writes += 1
                        
                    except Exception as e:
                        self.logger.logger.error(f"‚ùå Failed to write {doc.source_filename}: {e}")
            
            set_current_phase('file_writing')
            self.phase_manager.log('file_writer', f"Saved {successful_writes} files to {output_dir}")
            
            # DETAILED I/O TIMING SUMMARY
            self.service_coordinator.staging(f"üìä I/O BREAKDOWN: YAML gen: {total_yaml_time:.2f}ms, File writes: {total_file_write_time:.2f}ms, JSON writes: {total_json_write_time:.2f}ms")
            self.service_coordinator.staging(f"üìä I/O TOTAL: {total_yaml_time + total_file_write_time + total_json_write_time:.2f}ms for {successful_writes} files ({(total_yaml_time + total_file_write_time + total_json_write_time)/successful_writes:.1f}ms/file)")
            
        except Exception as e:
            self.logger.logger.error(f"‚ùå Batch write failed: {e}")
            raise
        finally:
            # Restore original thread name
            threading.current_thread().name = original_name
    
    def _validate_organization_entity(self, original_text: str, sentence_text: str, start_pos: int) -> bool:
        """
        Confidence Scoring Cascade for Organization Detection
        
        Uses multi-stage detection with evidence-based scoring:
        1. Multi-word organizations (highest confidence) 
        2. Pattern-enhanced single words (medium-high confidence)
        3. Bare single words (lowest confidence, strict validation)
        
        Args:
            original_text: The matched organization candidate from corpus
            sentence_text: The full sentence containing the match
            start_pos: Position of match within sentence
            
        Returns:
            True if confidence score >= threshold, False otherwise
        """
        # Import FLPC engine for high-performance pattern matching
        from knowledge.extractors.fast_regex import FastRegexEngine
        
        # Create context window around the candidate (¬±100 characters for better context)
        context_start = max(0, start_pos - 100)
        context_end = min(len(sentence_text), start_pos + len(original_text) + 100)
        context = sentence_text[context_start:context_end]
        
        # Initialize scoring system
        confidence_score = 0.0
        word_count = len(original_text.split())
        word_length = len(original_text.strip())
        
        # BASE SCORES based on type
        if word_count > 1:
            # Multi-word organizations get high base score
            confidence_score = 0.9
        elif original_text[0].isupper() if original_text else False:
            # Single word, capitalized
            confidence_score = 0.3
        else:
            # Single word, not capitalized
            confidence_score = 0.1
        
        # SHORT WORD PENALTY (words ‚â§4 characters need more evidence)
        if word_count == 1 and word_length <= 4:
            confidence_score *= 0.5  # Halve the base score for short words
        
        # Initialize FLPC engine for pattern matching
        flpc_engine = FastRegexEngine()
        
        # EVIDENCE MODIFIERS - Each pattern adds to confidence
        evidence_patterns = {
            # HIGH VALUE EVIDENCE (+0.5)
            'legal_suffix': (r'\b[A-Za-z0-9\-]*(?:Inc|LLC|Ltd|Corp|Co|SA|SPA|GmbH|PLC|AG|SAS|NV|BV)\b', 0.5),
            'corporate_context': (r'\b[A-Za-z0-9\-]+\s+(?:Inc|LLC|Ltd|Corp|Co|Company|Group|Holdings|Ventures|Partners|Enterprise|Industries|Solutions|Technologies|Services|Consulting)\b', 0.5),
            
            # MEDIUM-HIGH VALUE EVIDENCE (+0.4)
            'numeric_embedded': (r'\b[A-Za-z]*\d+[A-Za-z0-9]*\b', 0.4),
            'numeric_prefix': (r'\b\d{1,5}[A-Za-z][A-Za-z0-9\-]*\b', 0.4),
            'numeric_suffix': (r'\b[A-Za-z][A-Za-z0-9\-]*\d{1,5}\b', 0.4),
            
            # MEDIUM VALUE EVIDENCE (+0.3)
            'executive_context': (r'\b(?:CEO|CTO|CFO|President|Founder|Chairman|Director)\s+(?:of|at)\s+[A-Za-z0-9\-]+\b', 0.3),
            'business_suffix': (r'\b[A-Za-z]+(?:Bank|Air|Labs|Tech|Media|Systems|Motors|Foods|Pharma)\b', 0.3),
            
            # LOWER VALUE EVIDENCE (+0.2)
            'caps_acronym': (r'\b[A-Z]{3,}\b', 0.35),  # Increased for all-caps acronyms
            'camel_case': (r'\b[A-Z][a-z]+[A-Z][A-Za-z0-9]*\b', 0.2),
            'dotted_initials': (r'\b(?:[A-Z]\.){2,}[A-Z]?\b', 0.2),
            'hyphenated': (r'\b[A-Za-z0-9]+\-[A-Za-z0-9\-]+\b', 0.2),
            'ampersand': (r'\b[A-Za-z0-9]+&[A-Za-z0-9]+\b', 0.2)
        }
        
        # Check for evidence patterns and accumulate scores
        candidate_lower = original_text.lower().strip()
        evidence_found = []
        
        try:
            for pattern_name, (pattern, score_modifier) in evidence_patterns.items():
                matches = flpc_engine.findall(pattern, context)
                if matches:
                    # Check if any match contains or is near our candidate
                    for match in matches:
                        match_lower = match.lower()
                        # Direct match or candidate is part of the pattern match
                        if candidate_lower in match_lower or match_lower in candidate_lower:
                            confidence_score += score_modifier
                            evidence_found.append(pattern_name)
                            break
                        # Check proximity (within 20 chars)
                        elif abs(context.lower().find(candidate_lower) - context.lower().find(match_lower)) <= 20:
                            confidence_score += score_modifier * 0.5  # Half credit for proximity
                            evidence_found.append(f"{pattern_name}_proximity")
                            break
                            
        except Exception:
            # FLPC fallback: use basic validation
            if self._basic_organization_validation(original_text, context):
                confidence_score += 0.3
        
        # SPECIAL RULES FOR COMMON WORDS
        common_words_requiring_strong_evidence = {
            'here', 'there', 'place', 'front', 'back', 'side', 'top', 'bottom',
            'left', 'right', 'made', 'built', 'used', 'work', 'home', 'house',
            'time', 'year', 'month', 'week', 'day', 'part', 'area', 'zone'
        }
        
        if candidate_lower in common_words_requiring_strong_evidence:
            # Common words need VERY strong evidence (threshold raised to 0.8)
            return confidence_score >= 0.8
        
        # THRESHOLDS based on word characteristics
        if word_count > 1:
            # Multi-word: accept if score >= 0.5 (already starts at 0.9)
            acceptance_threshold = 0.5
        elif word_length <= 4:
            # Short single words: need strong evidence (threshold 0.6)
            acceptance_threshold = 0.6
        else:
            # Normal single words: standard threshold
            acceptance_threshold = 0.5
        
        return confidence_score >= acceptance_threshold
    
    def _basic_organization_validation(self, original_text: str, context: str) -> bool:
        """Fallback validation when FLPC patterns fail"""
        context_lower = context.lower()
        
        # Basic corporate suffixes
        corporate_indicators = [
            'inc', 'llc', 'corp', 'ltd', 'company', 'co',
            'group', 'holdings', 'ventures', 'enterprise'
        ]
        
        # Check for corporate indicators near the candidate
        candidate_lower = original_text.lower()
        for indicator in corporate_indicators:
            if indicator in context_lower:
                return True
        
        # Check for executive titles
        executive_titles = ['ceo', 'cto', 'cfo', 'president', 'founder', 'chairman', 'director']
        for title in executive_titles:
            if title in context_lower:
                return True
        
        return False
    
    def _flag_range_entities(self, entities: Dict, content: str) -> Dict:
        """AC+FLPC HYBRID: Flag entities with range indicators using detected locations"""
        range_indicators = entities.get('range_indicator', [])
        
        if not range_indicators:
            # No range indicators detected - add False flag to all entities
            for entity_type in ['measurement', 'money', 'date', 'time']:
                if entity_type in entities:
                    for entity in entities[entity_type]:
                        entity['range_indicator'] = {'detected': False}
            return entities
        
        # Flag entities that contain or are adjacent to range indicators
        for entity_type in ['measurement', 'money', 'date', 'time']:
            if entity_type in entities:
                for entity in entities[entity_type]:
                    entity_span = entity.get('span', {})
                    entity_start = entity_span.get('start', 0)
                    entity_end = entity_span.get('end', 0)
                    
                    # Check if any range indicator overlaps or is adjacent to this entity
                    range_detected = False
                    range_info = {'detected': False}
                    
                    for indicator in range_indicators:
                        indicator_span = indicator.get('span', {})
                        indicator_start = indicator_span.get('start', 0)
                        indicator_end = indicator_span.get('end', 0)
                        
                        # Check for overlap or adjacency (within 2 characters)
                        if self._spans_overlap_or_adjacent(entity_start, entity_end, indicator_start, indicator_end):
                            range_context = self._analyze_range_context(entity, indicator, content)
                            
                            range_info = {
                                'detected': True,
                                'type': self._classify_range_indicator(indicator.get('text', '')),
                                'indicator_text': indicator.get('text', ''),
                                'indicator_span': indicator_span,
                                'context': range_context,
                                'pattern_source': 'FLPC'  # FLPC detected the range indicator
                            }
                            range_detected = True
                            break
                    
                    entity['range_indicator'] = range_info
        
        return entities
    
    def _spans_overlap_or_adjacent(self, e_start: int, e_end: int, r_start: int, r_end: int, adjacency_margin: int = 2) -> bool:
        """Check if entity span overlaps with or is adjacent to range indicator span"""
        # Direct overlap
        if (e_start <= r_end and e_end >= r_start):
            return True
        
        # Adjacent (within margin characters)
        if (abs(e_end - r_start) <= adjacency_margin) or (abs(r_end - e_start) <= adjacency_margin):
            return True
        
        return False
    
    def _analyze_range_context(self, entity: Dict, indicator: Dict, content: str) -> str:
        """Determine if range indicator indicates range or negative modifier"""
        indicator_span = indicator.get('span', {})
        indicator_start = indicator_span.get('start', 0)
        indicator_end = indicator_span.get('end', 0)
        
        # Look at surrounding text for context clues
        context_before = content[max(0, indicator_start - 20):indicator_start]
        context_after = content[indicator_end:indicator_end + 20]
        
        # Range pattern: number-number (e.g., "30-37")
        if self._has_number_before(context_before) and self._has_number_after(context_after):
            return 'range'
        
        # Negative pattern: dash at start with no preceding number
        if not self._has_number_before(context_before):
            return 'negative'
        
        # Default to range for ambiguous cases
        return 'ambiguous'
    
    def _has_number_before(self, text: str) -> bool:
        """Check if text ends with a number (for range detection)"""
        import re
        return bool(re.search(r'\d+\s*$', text))
    
    def _has_number_after(self, text: str) -> bool:
        """Check if text starts with a number (for range detection)"""
        import re
        return bool(re.search(r'^\s*\d+', text))
    
    def _classify_range_indicator(self, indicator_text: str) -> str:
        """Classify the type of range indicator"""
        if indicator_text in ['-', '‚Äì', '‚Äî']:
            return 'hyphen_range'
        elif indicator_text.lower() in ['to', 'through']:
            return 'word_range'
        elif indicator_text.lower() in ['between', 'and']:
            return 'between_range'
        else:
            return 'unknown_range'


if __name__ == "__main__":
    # Test the service processor
    from utils.logging_config import setup_logging
    
    setup_logging(verbosity=3, log_file=None, use_colors=False)
    
    config = {'pipeline': {'queue_size': 10, 'memory_limit_mb': 100}}
    processor = ServiceProcessor(config)
    
    # Test with sample files
    test_files = [
        Path('/tmp/test1.txt'),
        Path('/tmp/test2.txt')
    ]
    
    # Create test files
    for f in test_files:
        f.write_text(f"Sample content for {f.name}")
    
    try:
        results, timing = processor.process_files_service(test_files)
        print(f"\n‚úÖ Test completed: {len(results)} results in {timing:.2f}s")
        
        for result in results:
            print(f"  - {result['source_filename']}: worker {result['worker_id']}, "
                  f"processing {result['processing_time']:.3f}s")
    finally:
        processor.stop_service()
        # Clean up test files
        for f in test_files:
            f.unlink(missing_ok=True)