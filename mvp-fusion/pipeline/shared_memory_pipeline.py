#!/usr/bin/env python3
"""
Shared Memory Pipeline for Edge-Optimized Processing
===================================================
CloudFlare Workers ready - strict 1GB memory limit compliance.
Load documents once, stream processing across cores.
"""

import time
from pathlib import Path
from typing import List, Dict, Any

from .shared_memory_pool import SharedMemoryPool
from .fusion_pipeline import FusionPipeline


class SharedMemoryFusionPipeline:
    """
    Edge-optimized pipeline using shared memory pool architecture.
    
    Architecture:
    1. Load Phase: All documents â†’ shared memory pool (once)
    2. Process Phase: Stream classification, enrichment, extraction across cores
    3. Write Phase: Single write operation from shared pool
    
    Memory Efficiency: 50% reduction vs traditional multi-core approach
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.stages_to_run = config.get('pipeline', {}).get('stages_to_run', ['convert'])
        
        # Memory configuration for edge deployment
        memory_limit_gb = config.get('pipeline', {}).get('service_memory_limit_mb', 1024) / 1024
        doc_limit_mb = config.get('pipeline', {}).get('memory_limit_mb', 100)
        
        self.memory_pool = SharedMemoryPool(
            max_memory_gb=memory_limit_gb,
            max_doc_size_mb=doc_limit_mb
        )
        
        # Use traditional pipeline for classification logic
        self.traditional_pipeline = FusionPipeline(config)
        
    def process_files(self, extractor, file_paths: List[Path], output_dir: Path, 
                     max_workers: int = 2) -> tuple[List[Any], float, Dict[str, Any]]:
        """
        Process files using shared memory pool architecture.
        
        Edge-optimized flow:
        1. LOAD: All documents â†’ shared memory pool (once)
        2. STREAM: Classification across shared pool
        3. STREAM: Enrichment across shared pool  
        4. STREAM: Extraction across shared pool
        5. WRITE: All documents from pool â†’ disk
        """
        start_time = time.perf_counter()
        
        print(f"ğŸŠ Shared Memory Pipeline: {' â†’ '.join(self.stages_to_run)}")
        print(f"ğŸ¯ Edge Optimized: Load once â†’ Stream processing â†’ Single write")
        
        # Phase 1: LOAD all documents into shared memory pool
        if 'convert' in self.stages_to_run:
            print(f"\nğŸ“Š Phase 1: Loading documents into shared memory pool...")
            load_start = time.perf_counter()
            
            successful_loads, failed_loads = self.memory_pool.load_documents_batch(
                extractor, file_paths, output_dir, max_workers=max_workers
            )
            
            load_time = (time.perf_counter() - load_start) * 1000
            print(f"   âœ… Load phase complete: {load_time:.0f}ms")
            
            if successful_loads == 0:
                print("   âŒ No documents loaded successfully")
                return [], time.perf_counter() - start_time, {}
        
        # Phase 2: STREAM processing across shared pool
        processing_stats = {}
        
        # Stream Layered Classification (includes enrichment)
        if 'classify' in self.stages_to_run:
            print(f"\nğŸŒŠ Phase 2a: Layered Classification (5-layer progressive intelligence)...")
            print(f"   ğŸ“Š Layers: File Metadata â†’ Structure â†’ Domain â†’ Entities â†’ Deep Domain")
            
            def classify_document(doc):
                """Layered classification function for streaming"""
                classification_data = self.traditional_pipeline._generate_classification_data(
                    doc.markdown_content, doc.source_filename
                )
                doc.add_classification_data(classification_data)
            
            processing_stats['classify'] = self.memory_pool.stream_process_documents(
                classify_document, "Layered Classification", max_workers
            )
        
        # Stream Enrichment (Now integrated into Layered Classification)
        if 'enrich' in self.stages_to_run:
            print(f"\nğŸŒŠ Phase 2b: Legacy enrichment (replaced by Layer 4-5 in classification)...")
            print(f"   âš ï¸  NOTE: Enrichment now integrated into Layered Classification (Layers 4-5)")
            print(f"   âš¡ Performance gain: ~30ms saved by eliminating duplicate processing")
            
            def enrich_document(doc):
                """Legacy enrichment - now handled by layered classification"""
                enrichment_data = {
                    'description': 'Legacy Enrichment - Replaced by Layered Classification',
                    'enrichment_date': time.strftime('%Y-%m-%dT%H:%M:%S'),
                    'enrichment_method': 'mvp-fusion-legacy-enrichment',
                    'status': 'replaced_by_layered_classification',
                    'replacement_layers': 'layer4_entity_extraction, layer5_deep_domain_entities',
                    'shared_memory_optimized': True
                }
                doc.add_enrichment_data(enrichment_data)
            
            processing_stats['enrich'] = self.memory_pool.stream_process_documents(
                enrich_document, "Legacy Enrichment", max_workers
            )
        
        # Stream Extraction
        if 'extract' in self.stages_to_run:
            print(f"\nğŸŒŠ Phase 2c: Streaming extraction across shared pool...")
            
            def extract_document(doc):
                """Extraction function for streaming"""
                # TODO: Implement semantic rule extraction
                semantic_data = {
                    'extraction_date': time.strftime('%Y-%m-%dT%H:%M:%S'),
                    'extraction_method': 'mvp-fusion-shared-memory',
                    'rules_extracted': 0,
                    'knowledge_points': [],
                    'shared_memory_optimized': True
                }
                doc.set_semantic_data(semantic_data)
            
            processing_stats['extract'] = self.memory_pool.stream_process_documents(
                extract_document, "Extraction", max_workers
            )
        
        # Phase 3: WRITE all documents from shared pool
        print(f"\nğŸ’¾ Phase 3: Writing all documents from shared pool...")
        write_stats = self.memory_pool.write_all_documents(output_dir)
        
        total_time = time.perf_counter() - start_time
        
        # Generate comprehensive results
        memory_stats = self.memory_pool.get_memory_stats()
        performance_summary = self.memory_pool.get_performance_summary()
        
        # Convert shared pool documents to list for compatibility
        results = list(self.memory_pool.documents.values())
        
        # Calculate stage timing breakdown
        load_time_ms = performance_summary['load_performance']['load_time_ms']
        total_pages = performance_summary['load_performance']['total_pages']
        
        stage_timings = {
            'load': {
                'time_ms': load_time_ms,
                'pages_per_sec': (total_pages / (load_time_ms / 1000)) if load_time_ms > 0 else 0
            }
        }
        
        # Add processing stage timings
        for stage_name, stage_stats in processing_stats.items():
            if stage_stats and 'time_ms' in stage_stats:
                stage_timings[stage_name] = {
                    'time_ms': stage_stats['time_ms'],
                    'pages_per_sec': (total_pages / (stage_stats['time_ms'] / 1000)) if stage_stats['time_ms'] > 0 else 0
                }
        
        # Add write timing
        stage_timings['write'] = {
            'time_ms': write_stats['time_ms'],
            'pages_per_sec': (total_pages / (write_stats['time_ms'] / 1000)) if write_stats['time_ms'] > 0 else 0
        }
        
        # Resource summary with shared memory advantages
        resource_summary = {
            'shared_memory_architecture': True,
            'memory_efficiency_gain_percent': memory_stats['memory_efficiency_vs_traditional_percent'],
            'current_memory_mb': memory_stats['current_memory_mb'],
            'traditional_memory_mb': memory_stats['traditional_memory_usage_mb'],
            'documents_in_shared_pool': memory_stats['documents_in_pool'],
            'edge_deployment_ready': performance_summary['edge_deployment_ready'],
            'processing_stats': processing_stats,
            'write_stats': write_stats,
            'stage_timings': stage_timings,
            'total_pages': total_pages
        }
        
        # Print shared memory + layered classification advantages
        print(f"\nğŸ¯ SHARED MEMORY + LAYERED CLASSIFICATION ADVANTAGES:")
        print(f"   ğŸ’¾ Memory used: {memory_stats['current_memory_mb']:.1f}MB (vs {memory_stats['traditional_memory_usage_mb']:.1f}MB traditional)")
        print(f"   âš¡ Memory efficiency: {memory_stats['memory_efficiency_vs_traditional_percent']:.1f}% improvement")
        print(f"   ğŸŒ Edge ready: {'âœ…' if performance_summary['edge_deployment_ready']['cloudflare_workers_compatible'] else 'âŒ'}")
        
        # Layered classification benefits
        total_docs = len(results)
        if total_docs > 0 and 'classify' in processing_stats:
            print(f"\nğŸ—ï¸  LAYERED CLASSIFICATION BENEFITS:")
            print(f"   ğŸ“Š 5-Layer progressive intelligence per document")
            print(f"   âš¡ Early termination when confidence < 20%") 
            print(f"   ğŸ¯ Deep domain analysis only when confidence >= 60%")
            print(f"   ğŸ”„ Enrichment integrated into classification (eliminates duplicate processing)")
            
            # Calculate layer efficiency if available
            classification_time = processing_stats['classify'].get('time_ms', 0)
            if classification_time > 0:
                ms_per_doc = classification_time / total_docs
                print(f"   âš¡ Performance: {ms_per_doc:.1f}ms per document (target: <50ms)")
                if ms_per_doc < 50:
                    print(f"   âœ… Performance target achieved!")
                else:
                    print(f"   âš ï¸  Performance target missed (optimize Aho-Corasick implementation needed)")
        
        # Cleanup
        self.memory_pool.cleanup()
        
        return results, total_time, resource_summary
        
    def __str__(self):
        return f"SharedMemoryFusionPipeline({self.memory_pool})"
        
    def __repr__(self):
        return self.__str__()