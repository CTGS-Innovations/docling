#!/usr/bin/env python3
"""
MVP Hyper Enhanced Pipeline
============================
Enhanced progressive pipeline integrating high-performance classification 
and enrichment modules achieving 1,500-2,000+ pages/sec targets.

Based on benchmark findings:
- Pure regex classification: 1,717 pages/sec potential
- pyahocorasick enrichment: 1,816 pages/sec proven
- Combined approach for optimal performance

Steps:
1. CONVERSION: PDF to markdown (mvp-hyper-core) - 700+ pages/sec
2. CLASSIFICATION: Enhanced regex-based classification - 2,000+ pages/sec target  
3. ENRICHMENT: pyahocorasick + regex enrichment - 1,500+ pages/sec target
4. EXTRACTION: Semantic facts with full context - 300+ pages/sec target

Usage:
  python mvp-hyper-pipeline-enhanced.py --step conversion
  python mvp-hyper-pipeline-enhanced.py --step classification
  python mvp-hyper-pipeline-enhanced.py --step enrichment
  python mvp-hyper-pipeline-enhanced.py --step extraction
  python mvp-hyper-pipeline-enhanced.py --full  # All steps
"""

import argparse
import subprocess
import time
import json
import yaml
import os
from pathlib import Path
from typing import Dict, Tuple, List

# Import enhanced modules
try:
    from enhanced_classification import EnhancedClassifier
    from enhanced_enrichment import EnhancedEnrichment
    ENHANCED_MODULES_AVAILABLE = True
except ImportError:
    ENHANCED_MODULES_AVAILABLE = False
    print("‚ö†Ô∏è Enhanced modules not found. Using fallback implementations.")

class MVPHyperEnhancedPipeline:
    """Enhanced pipeline with high-performance classification and enrichment."""
    
    def __init__(self, config_path: str = None):
        # Set default config path
        if config_path is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            config_path = os.path.join(current_dir, ".config", "mvp-hyper-pipeline-progressive-config.yaml")
        
        self.config_path = config_path
        self.performance_stats = {}
        self.config = self.load_config()
        
        # Initialize enhanced modules if available
        if ENHANCED_MODULES_AVAILABLE:
            self.classifier = EnhancedClassifier()
            self.enrichment = EnhancedEnrichment()
            print("‚úÖ Enhanced classification and enrichment modules loaded")
        else:
            self.classifier = None
            self.enrichment = None
            print("‚ö†Ô∏è Using fallback classification and enrichment")
    
    def load_config(self) -> Dict:
        """Load configuration from YAML file."""
        try:
            with open(self.config_path, 'r') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"‚ö†Ô∏è Config file not found: {self.config_path}")
            return {}
        except yaml.YAMLError as e:
            print(f"‚ö†Ô∏è Error parsing config: {e}")
            return {}
    
    def display_enhanced_config_summary(self, input_path: str, output_path: str, steps: list):
        """Display enhanced configuration summary with performance targets."""
        print(f\"\"\"
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë              ENHANCED MVP HYPER PIPELINE                     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üìã CONFIG: {self.config_path}
üìÇ INPUT:  {input_path}
üìÇ OUTPUT: {output_path}

üîÑ PIPELINE STEPS: {' ‚Üí '.join([s.upper() for s in steps])}

‚ö° ENHANCED PERFORMANCE TARGETS:
   Conversion:     700+ pages/sec (mvp-hyper-core baseline)
   Classification: 2000+ pages/sec (enhanced regex patterns)  
   Enrichment:     1500+ pages/sec (pyahocorasick + regex)
   Extraction:     300+ pages/sec (semantic with full context)

üöÄ ENHANCEMENTS:
   ‚úÖ Pre-compiled regex patterns for classification
   ‚úÖ pyahocorasick dictionary lookup for entities
   ‚úÖ Domain-specific pattern libraries
   ‚úÖ Context-aware entity resolution
   ‚úÖ Progressive metadata enhancement
        \"\"\")\
        
        # Show input directories if configured
        if self.config and 'inputs' in self.config and 'directories' in self.config['inputs']:
            dirs = self.config['inputs']['directories']
            if dirs:
                print("üìÅ INPUT DIRECTORIES:")
                for dir_path in dirs:
                    expanded_path = Path(dir_path).expanduser()
                    if expanded_path.exists():
                        file_count = sum(1 for _ in expanded_path.rglob('*') if _.is_file())
                        print(f"   ‚úÖ {dir_path} ({file_count} files)")
                    else:
                        print(f"   ‚ùå {dir_path} (not found)")
        
        print("\\nüöÄ Starting enhanced pipeline execution...\\n")
    
    def run_step(self, step_name: str, command: str, description: str) -> Tuple[bool, float, int]:
        """Run a pipeline step and measure performance."""
        print(f"\\nüî∏ {step_name.upper()}: {description}")
        
        start_time = time.time()
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        elapsed = time.time() - start_time
        
        success = result.returncode == 0
        
        if success:
            print(f"‚úÖ {step_name}: Completed in {elapsed:.2f}s")
        else:
            print(f"‚ùå {step_name}: Failed after {elapsed:.2f}s")
            if result.stderr:
                print(f"Error: {result.stderr}")
        
        # Try to extract file count from output
        files_processed = 0
        if result.stdout:
            # Look for file count patterns in output
            import re
            file_patterns = [
                r'processed (\\d+) files',
                r'(\\d+) files processed',
                r'(\\d+) documents converted'
            ]
            for pattern in file_patterns:
                match = re.search(pattern, result.stdout, re.IGNORECASE)
                if match:
                    files_processed = int(match.group(1))
                    break
        
        return success, elapsed, files_processed
    
    def step_conversion(self, input_path: str, output_path: str) -> bool:
        """Step 1: Convert PDFs to markdown using mvp-hyper-core."""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(current_dir, ".config", "config.yaml")
        core_path = os.path.join(current_dir, "mvp-hyper-core.py")
        
        command = f"python {core_path} {input_path} --output {output_path} --config {config_path} --quiet"
        success, elapsed, files = self.run_step(
            "conversion",
            command,
            "Converting documents to markdown (Target: 700+ pages/sec)"
        )
        
        if success and files > 0:
            pages_per_sec = files / elapsed if elapsed > 0 else 0
            self.performance_stats['conversion'] = {
                'time': elapsed,
                'files': files,
                'pages_per_sec': pages_per_sec
            }
            print(f"üìä Conversion Performance: {pages_per_sec:.1f} pages/sec")
        
        return success
    
    def step_enhanced_classification(self, input_path: str, output_path: str) -> bool:
        """Step 2: Enhanced classification using compiled regex patterns."""
        print(f"\\nüî∏ ENHANCED CLASSIFICATION: High-performance document typing")
        
        start_time = time.time()
        markdown_dir = Path(input_path)
        
        files_processed = 0
        classification_errors = 0
        
        for md_file in markdown_dir.glob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Use enhanced classifier if available
                if self.classifier:
                    result = self.classifier.classify_document(content, md_file.name)
                    insights = self.classifier.get_domain_insights(result)
                    
                    # Format classification metadata
                    classification_metadata = f\"\"\"\\n# Enhanced Classification (Step 2)
document_types: {result['document_types']}
primary_domain: {result['primary_domain']}
classification_confidence: {result['classification_confidence']}
domain_percentages: {result['domain_percentages']}
pattern_matches: {len(result.get('pattern_matches', {}))}
processing_time_ms: {result['processing_time_ms']}
domain_insights: {insights}
\"\"\"
                else:
                    # Fallback to basic classification
                    classification_metadata = self._fallback_classification(content)
                
                # Inject metadata into markdown
                if content.startswith('---'):
                    parts = content.split('---', 2)
                    if len(parts) >= 3:
                        front_matter = parts[1]
                        body = parts[2]
                        new_front_matter = front_matter.rstrip() + classification_metadata
                        content = f"---{new_front_matter}---{body}"
                else:
                    content = f"---{classification_metadata}---\\n\\n{content}"
                
                # Write enhanced file
                with open(md_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                files_processed += 1
                
            except Exception as e:
                classification_errors += 1
                print(f"‚ö†Ô∏è Classification error for {md_file.name}: {e}")
        
        elapsed = time.time() - start_time
        pages_per_sec = files_processed / elapsed if elapsed > 0 else 0
        
        self.performance_stats['classification'] = {
            'time': elapsed,
            'files': files_processed,
            'errors': classification_errors,
            'pages_per_sec': pages_per_sec
        }
        
        target_performance = pages_per_sec / 2000 * 100 if pages_per_sec > 0 else 0
        print(f"‚úÖ Enhanced Classification: {files_processed} files, {pages_per_sec:.1f} pages/sec")
        print(f"üìä Performance vs Target (2000 pages/sec): {target_performance:.1f}%")
        
        return classification_errors == 0
    
    def step_enhanced_enrichment(self, input_path: str, output_path: str) -> bool:
        """Step 3: Enhanced enrichment using pyahocorasick + regex patterns."""
        print(f"\\nüî∏ ENHANCED ENRICHMENT: High-performance entity extraction")
        
        start_time = time.time()
        markdown_dir = Path(input_path)
        
        files_processed = 0
        enrichment_errors = 0
        total_entities = 0
        
        for md_file in markdown_dir.glob("*.md"):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Extract primary domain from existing classification
                primary_domain = self._extract_primary_domain(content)
                
                # Use enhanced enrichment if available
                if self.enrichment:
                    entities = self.enrichment.extract_entities(content, primary_domain, md_file.name)
                    domain_tags = self.enrichment.get_domain_tags(entities, primary_domain)
                    enrichment_metadata = self.enrichment.format_enrichment_metadata(entities, domain_tags)
                    
                    total_entities += entities['metadata']['total_entity_count']
                else:
                    # Fallback to basic enrichment
                    enrichment_metadata = self._fallback_enrichment(content, primary_domain)
                
                # Inject enrichment metadata
                if content.startswith('---'):
                    parts = content.split('---', 2)
                    if len(parts) >= 3:
                        front_matter = parts[1]
                        body = parts[2]
                        new_front_matter = front_matter.rstrip() + enrichment_metadata
                        content = f"---{new_front_matter}---{body}"
                else:
                    content = f"---{enrichment_metadata}---\\n\\n{content}"
                
                # Write enriched file
                with open(md_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                files_processed += 1
                
            except Exception as e:
                enrichment_errors += 1
                print(f"‚ö†Ô∏è Enrichment error for {md_file.name}: {e}")
        
        elapsed = time.time() - start_time
        pages_per_sec = files_processed / elapsed if elapsed > 0 else 0
        entities_per_sec = total_entities / elapsed if elapsed > 0 else 0
        
        self.performance_stats['enrichment'] = {
            'time': elapsed,
            'files': files_processed,
            'errors': enrichment_errors,
            'total_entities': total_entities,
            'pages_per_sec': pages_per_sec,
            'entities_per_sec': entities_per_sec
        }
        
        target_performance = pages_per_sec / 1500 * 100 if pages_per_sec > 0 else 0
        print(f"‚úÖ Enhanced Enrichment: {files_processed} files, {total_entities} entities")
        print(f"üìä Performance: {pages_per_sec:.1f} pages/sec, {entities_per_sec:.1f} entities/sec")
        print(f"üìä Performance vs Target (1500 pages/sec): {target_performance:.1f}%")
        
        return enrichment_errors == 0
    
    def step_extraction(self, input_path: str, output_path: str) -> bool:
        """Step 4: Semantic extraction with full context from classification + enrichment."""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        semantic_path = os.path.join(current_dir, "mvp-hyper-semantic.py")
        
        command = f"python {semantic_path} {input_path} {output_path}"
        success, elapsed, files = self.run_step(
            "extraction",
            command,
            "Extracting semantic facts with full context (Target: 300+ pages/sec)"
        )
        
        if success and files > 0:
            pages_per_sec = files / elapsed if elapsed > 0 else 0
            self.performance_stats['extraction'] = {
                'time': elapsed,
                'files': files,
                'pages_per_sec': pages_per_sec
            }
            print(f"üìä Extraction Performance: {pages_per_sec:.1f} pages/sec")
        
        return success
    
    def _extract_primary_domain(self, content: str) -> str:
        """Extract primary domain from existing front matter."""
        if content.startswith('---'):
            parts = content.split('---', 2)
            if len(parts) >= 3:
                front_matter = parts[1]
                for line in front_matter.split('\\n'):
                    if 'primary_domain:' in line:
                        return line.split(':', 1)[1].strip()
        return 'general'
    
    def _fallback_classification(self, content: str) -> str:
        """Fallback classification using simple keyword counting."""
        content_lower = content.lower()
        
        safety_score = content_lower.count('safety') + content_lower.count('osha')
        tech_score = content_lower.count('algorithm') + content_lower.count('code')
        
        if safety_score > tech_score:
            primary_domain = 'safety'
            confidence = min(safety_score / 20, 1.0)
        elif tech_score > 0:
            primary_domain = 'technical'
            confidence = min(tech_score / 20, 1.0)
        else:
            primary_domain = 'general'
            confidence = 0.5
        
        return f\"\"\"\\n# Fallback Classification (Step 2)
primary_domain: {primary_domain}
classification_confidence: {confidence:.2f}
method: fallback_keyword_counting
\"\"\"
    
    def _fallback_enrichment(self, content: str, primary_domain: str) -> str:
        """Fallback enrichment using basic regex patterns."""
        import re
        
        # Basic entity patterns
        money_pattern = r'\\$[0-9,]+(?:\\.[0-9]{2})?'
        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'
        
        money_matches = re.findall(money_pattern, content)
        email_matches = re.findall(email_pattern, content)
        
        return f\"\"\"\\n# Fallback Enrichment (Step 3)
entities_found: {len(money_matches) + len(email_matches)}
money_mentions: {len(money_matches)}
email_mentions: {len(email_matches)}
method: fallback_basic_regex
\"\"\"
    
    def display_performance_summary(self):
        """Display comprehensive performance summary."""
        print(f\"\"\"
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    PERFORMANCE SUMMARY                       ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        \"\"\")
        
        total_time = 0
        total_files = 0
        
        for step, stats in self.performance_stats.items():
            step_time = stats.get('time', 0)
            step_files = stats.get('files', 0)
            step_speed = stats.get('pages_per_sec', 0)
            
            total_time += step_time
            total_files = max(total_files, step_files)  # Use max since files carry through steps
            
            # Step-specific metrics
            if step == 'enrichment' and 'entities_per_sec' in stats:
                entities = stats.get('total_entities', 0)
                entities_per_sec = stats.get('entities_per_sec', 0)
                print(f"üî∏ {step.upper()}: {step_files} files, {step_speed:.1f} pages/sec")
                print(f"   Entities: {entities} total, {entities_per_sec:.1f} entities/sec")
            else:
                print(f"üî∏ {step.upper()}: {step_files} files, {step_speed:.1f} pages/sec")
        
        overall_speed = total_files / total_time if total_time > 0 else 0
        print(f\"\"\"
üìä OVERALL PIPELINE:
   Total Time: {total_time:.2f}s
   Files Processed: {total_files}
   Overall Speed: {overall_speed:.1f} files/sec
   
‚úÖ Enhanced modules: {'Active' if ENHANCED_MODULES_AVAILABLE else 'Fallback mode'}
        \"\"\")

def main():
    parser = argparse.ArgumentParser(description='MVP Hyper Enhanced Pipeline')
    parser.add_argument('input_path', nargs='?', help='Input directory or file')
    parser.add_argument('--output', required=True, help='Output directory')
    parser.add_argument('--step', choices=['conversion', 'classification', 'enrichment', 'extraction'],
                      help='Run specific step only')
    parser.add_argument('--full', action='store_true', help='Run all steps')
    parser.add_argument('--config', help='Configuration file path')
    
    args = parser.parse_args()
    
    # Initialize pipeline
    pipeline = MVPHyperEnhancedPipeline(args.config)
    
    # Determine steps to run
    if args.full:
        steps = ['conversion', 'classification', 'enrichment', 'extraction']
    elif args.step:
        steps = [args.step]
    else:
        print("‚ùå Must specify either --step or --full")
        return 1
    
    # Use input from config if not provided
    input_path = args.input_path
    if not input_path and pipeline.config.get('inputs', {}).get('directories'):
        # Use first directory from config
        input_path = pipeline.config['inputs']['directories'][0]
        input_path = str(Path(input_path).expanduser())
    
    if not input_path:
        print("‚ùå No input path specified and none found in config")
        return 1
    
    # Display configuration
    pipeline.display_enhanced_config_summary(input_path, args.output, steps)
    
    # Create output directory
    Path(args.output).mkdir(parents=True, exist_ok=True)
    
    # Run pipeline steps
    success = True
    for step in steps:
        if step == 'conversion':
            if not pipeline.step_conversion(input_path, args.output):
                success = False
                break
            # After conversion, work with output directory for subsequent steps
            input_path = args.output
            
        elif step == 'classification':
            if not pipeline.step_enhanced_classification(input_path, args.output):
                success = False
                break
                
        elif step == 'enrichment':
            if not pipeline.step_enhanced_enrichment(input_path, args.output):
                success = False
                break
                
        elif step == 'extraction':
            if not pipeline.step_extraction(input_path, args.output):
                success = False
                break
    
    # Display performance summary
    pipeline.display_performance_summary()
    
    if success:
        print("\\nüéâ Enhanced pipeline completed successfully!")
        return 0
    else:
        print("\\n‚ùå Pipeline failed - check errors above")
        return 1

if __name__ == "__main__":
    exit(main())